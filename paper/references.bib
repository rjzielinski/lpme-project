@book{akhiezerGlazman1963,
  title = {Theory of {{Linear Operators}} in {{Hilbert Space}}},
  author = {Akhiezer, N.I. and Glazman, I.M.},
  year = {1963},
  publisher = {Dover Publications, Inc.},
  address = {New York}
}

@article{akudjedu2018comparative,
  title = {A Comparative Study of Segmentation Techniques for the Quantification of Brain Subcortical Volume},
  author = {Akudjedu, Theophilus N. and Nabulsi, Leila and Makelyte, Migle and Scanlon, Cathy and Hehir, Sarah and Casey, Helen and Ambati, Srinath and Kenney, Joanne and O'Donoghue, Stefani and McDermott, Emma and Kilmartin, Liam and Dockery, Peter and McDonald, Colm and Hallahan, Brian and Cannon, Dara M.},
  year = {2018},
  month = dec,
  journal = {Brain Imaging and Behavior},
  volume = {12},
  number = {6},
  pages = {1678--1695},
  issn = {1931-7565},
  doi = {10.1007/s11682-018-9835-y},
  urldate = {2025-01-07},
  abstract = {Manual tracing of magnetic resonance imaging (MRI) represents the gold standard for segmentation in clinical neuropsychiatric research studies, however automated approaches are increasingly used due to its time limitations. The accuracy of segmentation techniques for subcortical structures has not been systematically investigated in large samples. We compared the accuracy of fully automated [(i) model-based: FSL-FIRST; (ii) patch-based: volBrain], semi--automated (FreeSurfer) and stereological (Measure{\textregistered}) segmentation techniques with manual tracing (ITK-SNAP) for delineating volumes of the caudate (easy-to-segment) and the hippocampus (difficult-to-segment). High resolution 1.5~T T1-weighted MR images were obtained from 177 patients with major psychiatric disorders and 104 healthy participants. The relative consistency (partial correlation), absolute agreement (intraclass correlation coefficient, ICC) and potential technique bias (Bland--Altman plots) of each technique was compared with manual segmentation. Each technique yielded high correlations (0.77--0.87, p\,{$<$}\,0.0001) and moderate ICC's (0.28--0.49) relative to manual segmentation for the caudate. For the hippocampus, stereology yielded good consistency (0.52--0.55, p\,{$<$}\,0.0001) and ICC (0.47--0.49), whereas automated and semi-automated techniques yielded poor ICC (0.07--0.10) and moderate consistency (0.35--0.62, p\,{$<$}\,0.0001). Bias was least using stereology for segmentation of the hippocampus and using FreeSurfer for segmentation of the caudate. In a typical neuropsychiatric MRI dataset, automated segmentation techniques provide good accuracy for an easy-to-segment structure such as the caudate, whereas for the hippocampus, a reasonable correlation with volume but poor absolute agreement was demonstrated. This indicates manual or stereological volume estimation should be considered for studies that require high levels of precision such as those with small sample size.},
  langid = {english},
  keywords = {FreeSurfer,FSL-FIRST,Segmentation techniques,Stereology,Subcortical structures,VolBrain}
}

@article{avants2009Advanced,
  title = {Advanced {{Normalization Tools}} ({{ANTS}})},
  author = {Avants, Brian B and Tustison, Nick and Johnson, Hans},
  year = {2009},
  journal = {Insight j},
  volume = {2},
  number = {365},
  pages = {1--35},
  abstract = {We provide examples and highlights of Advanced Normalization Tools (ANTs), versions 2.x, that address practical problems in real data.},
  file = {/Users/rzielinski/Zotero/storage/67BF96HV/Avants et al. - Advanced Normalization Tools (ANTS).pdf}
}

@article{banfieldIceFloeIdentification1992,
  title = {Ice {{Floe Identification}} in {{Satellite Images Using Mathematical Morphology}} and {{Clustering}} about {{Principal Curves}}},
  author = {Banfield, Jeffrey D. and Raftery, Adrian E.},
  year = {1992},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {87},
  number = {417},
  pages = {7--16},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1992.10475169},
  urldate = {2024-06-18},
  abstract = {Identification of ice floes and their outlines in satellite images is important for understanding physical processes in the polar regions, for transportation in ice-covered seas, and for the design of offshore structures intended to survive in the presence of ice. At present this is done manually, a long and tedious process that precludes full use of the great volume of relevant images now available. We describe an accurate and almost fully automatic method for identifying ice floes and their outlines. Floe outlines are modeled as closed principal curves, a flexible class of smooth nonparametric curves. We propose a robust method of estimating closed principal curves that reduces both bias and variance. Initial estimates of floe outlines come from the erosion-propagation (EP) algorithm, which combines erosion from mathematical morphology with local propagation of information about floe edges. The edge pixels from the EP algorithm are grouped into floe outlines using a new clustering algorithm. This extends existing clustering methods by allowing groups to be centered about principal curves rather than points or lines. This may open the way to efficient feature extraction using cluster analysis in images more generally. The method is implemented in an object-oriented programming environment, for which it is well suited, and is quite computationally efficient.},
  keywords = {Erosion,Feature extraction,Nonparametric curves,Remote sensing,Robustness},
  file = {/Users/rzielinski/Zotero/storage/FTSUT2IP/Banfield_Raftery_1992_Ice Floe Identification in Satellite Images Using Mathematical Morphology and.pdf}
}

@article{bartel2017Regional,
  title = {Regional Analysis of Volumes and Reproducibilities of Automatic and Manual Hippocampal Segmentations},
  author = {Bartel, Fabian and Vrenken, Hugo and Bijma, Fetsje and Barkhof, Frederik and van Herk, Marcel and de Munck, Jan C.},
  year = {2017},
  month = feb,
  journal = {PLOS ONE},
  volume = {12},
  number = {2},
  pages = {e0166785},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0166785},
  urldate = {2025-01-07},
  abstract = {Purpose Precise and reproducible hippocampus outlining is important to quantify hippocampal atrophy caused by neurodegenerative diseases and to spare the hippocampus in whole brain radiation therapy when performing prophylactic cranial irradiation or treating brain metastases. This study aimed to quantify systematic differences between methods by comparing regional volume and outline reproducibility of manual, FSL-FIRST and FreeSurfer hippocampus segmentations. Materials and methods This study used a dataset from ADNI (Alzheimer's Disease Neuroimaging Initiative), including 20 healthy controls, 40 patients with mild cognitive impairment (MCI), and 20 patients with Alzheimer's disease (AD). For each subject back-to-back (BTB) T1-weighted 3D MPRAGE images were acquired at time-point baseline (BL) and 12 months later (M12). Hippocampi segmentations of all methods were converted into triangulated meshes, regional volumes were extracted and regional Jaccard indices were computed between the hippocampi meshes of paired BTB scans to evaluate reproducibility. Regional volumes and Jaccard indices were modelled as a function of group (G), method (M), hemisphere (H), time-point (T), region (R) and interactions. Results For the volume data the model selection procedure yielded the following significant main effects G, M, H, T and R and interaction effects G-R and M-R. The same model was found for the BTB scans. For all methods volumes reduces with the severity of disease. Significant fixed effects for the regional Jaccard index data were M, R and the interaction M-R. For all methods the middle region was most reproducible, independent of diagnostic group. FSL-FIRST was most and FreeSurfer least reproducible. Discussion/Conclusion A novel method to perform detailed analysis of subtle differences in hippocampus segmentation is proposed. The method showed that hippocampal segmentation reproducibility was best for FSL-FIRST and worst for Freesurfer. We also found systematic regional differences in hippocampal segmentation between different methods reinforcing the need of adopting harmonized protocols.},
  langid = {english},
  keywords = {Alzheimer's disease,Atrophy,Cognitive impairment,Hippocampus,Imaging techniques,Interpolation,Magnetic resonance imaging,Reproducibility},
  file = {/Users/rzielinski/Zotero/storage/CF5EX4NS/Bartel et al_2017_Regional analysis of volumes and reproducibilities of automatic and manual.pdf}
}

@article{belkin2003laplacian,
  title = {Laplacian {{Eigenmaps}} for {{Dimensionality Reduction}} and {{Data Representation}}},
  author = {Belkin, Mikhail and Niyogi, Partha},
  year = {2003},
  journal = {Neural Computation},
  volume = {15},
  number = {6},
  pages = {1373--1396},
  issn = {0899-7667},
  doi = {10.1162/089976603321780317},
  abstract = {One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.},
  file = {/Users/rzielinski/Zotero/storage/AZKEL28L/Belkin_Niyogi_2003_Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.pdf;/Users/rzielinski/Zotero/storage/6RS77KNT/Laplacian-Eigenmaps-for-Dimensionality-Reduction.html}
}

@article{boccardiSurveyProtocolsManual2011,
  title = {Survey of {{Protocols}} for the {{Manual Segmentation}} of the {{Hippocampus}}: {{Preparatory Steps Towards}} a {{Joint EADC-ADNI Harmonized Protocol}}},
  shorttitle = {Survey of {{Protocols}} for the {{Manual Segmentation}} of the {{Hippocampus}}},
  author = {Boccardi, Marina and Ganzola, Rossana and Bocchetta, Martina and Pievani, Michela and Redolfi, Alberto and Bartzokis, George and Camicioli, Richard and Csernansky, John G. and {de Leon}, Mony J. and {deToledo-Morrell}, Leyla and Killiany, Ronald J. and Leh{\'e}ricy, St{\'e}phane and Pantel, Johannes and Pruessner, Jens C. and Soininen, H. and Watson, Craig and Duchesne, Simon and Jack Jr, Clifford R. and Frisoni, Giovanni B.},
  year = {2011},
  month = jan,
  journal = {Journal of Alzheimer's Disease},
  volume = {26},
  number = {s3},
  pages = {61--75},
  publisher = {IOS Press},
  issn = {1387-2877},
  doi = {10.3233/JAD-2011-0004},
  urldate = {2023-02-24},
  abstract = {Manual segmentation from magnetic resonance imaging (MR) is the gold standard for evaluating hippocampal atrophy in Alzheimer's disease (AD). Nonetheless, different segmentation protocols provide up to 2.5-fold volume differences. Here we surveyed th},
  langid = {english}
}

@inproceedings{bone2018Learning,
  title = {Learning {{Distributions}} of {{Shape Trajectories From Longitudinal Datasets}}: {{A Hierarchical Model}} on a {{Manifold}} of {{Diffeomorphisms}}},
  shorttitle = {Learning {{Distributions}} of {{Shape Trajectories From Longitudinal Datasets}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {B{\^o}ne, Alexandre and Colliot, Olivier and Durrleman, Stanley},
  year = {2018},
  pages = {9271--9280},
  urldate = {2023-08-02},
  file = {/Users/rzielinski/Zotero/storage/BZMK3YGZ/BÃ´ne et al_2018_Learning Distributions of Shape Trajectories From Longitudinal Datasets.pdf}
}

@article{brown2020Testretest,
  title = {Test-Retest Reliability of {{FreeSurfer}} Automated Hippocampal Subfield Segmentation within and across Scanners},
  author = {Brown, Emma M. and Pierce, Meghan E. and Clark, Dustin C. and Fischl, Bruce R. and Iglesias, Juan E. and Milberg, William P. and McGlinchey, Regina E. and Salat, David H.},
  year = {2020},
  month = apr,
  journal = {NeuroImage},
  volume = {210},
  pages = {116563},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.116563},
  urldate = {2025-01-07},
  abstract = {The human hippocampus is vulnerable to a range of degenerative conditions and as such, accurate in vivo measurement of the hippocampus and hippocampal substructures via neuroimaging is of great interest for understanding mechanisms of disease as well as for use as a biomarker in clinical trials of novel therapeutics. Although total hippocampal volume can be measured relatively reliably, it is critical to understand how this reliability is affected by acquisition on different scanners, as multiple scanning platforms would likely be utilized in large-scale clinical trials. This is particularly true for hippocampal subregional measurements, which have only relatively recently been measurable through common image processing platforms such as FreeSurfer. Accurate segmentation of these subregions is challenging due to their small size, magnetic resonance imaging (MRI) signal loss in medial temporal regions of the brain, and lack of contrast for delineation from standard neuroimaging procedures. Here, we assess the test-retest reliability of the FreeSurfer automated hippocampal subfield segmentation procedure using two Siemens model scanners (a Siemens Trio and Prismafit Trio upgrade). T1-weighted images were acquired for 11 generally healthy younger participants (two scans on the Trio and one scan on the Prismafit). Each scan was processed through the standard cross-sectional stream and the recently released longitudinal pipeline in FreeSurfer v6.0 for hippocampal segmentation. Test-retest reliability of the volumetric measures was examined for individual subfields as well as percent volume difference and Dice overlap among scans and intra-class correlation coefficients (ICC). Reliability was high in the molecular layer, dentate gyrus, and whole hippocampus with the inclusion of three time points with mean volume differences among scans less than 3\%, overlap greater than 80\%, and ICC {$>$}0.95. The parasubiculum and hippocampal fissure showed the least improvement in reliability with mean volume difference greater than 5\%, overlap less than 70\%, and ICC scores ranging from 0.78 to 0.89. Other subregions, including the CA regions, were stable in their mean volume difference and overlap ({$<$}5\% difference and {$>$}75\% respectively) and showed improvement in reliability with the inclusion of three scans (ICC~\hspace{0pt}{$>~$}\hspace{0pt}0.9). Reliability was generally higher within scanner (Trio-Trio), however, Trio-Prismafit reliability was also high and did not exhibit an obvious bias. These results suggest that the FreeSurfer automated segmentation procedure is a reliable method to measure total as well as hippocampal subregional volumes and may be useful in clinical applications including as an endpoint for future clinical trials of conditions affecting the hippocampus.},
  keywords = {FreeSurfer,Hippocampal subfields,Hippocampus,Longitudinal,Magnetic resonance imaging,Test-retest reliability},
  file = {/Users/rzielinski/Zotero/storage/QKDUTZS8/Brown et al_2020_Test-retest reliability of FreeSurfer automated hippocampal subfield.pdf;/Users/rzielinski/Zotero/storage/IBXHSZIJ/S1053811920300501.html}
}

@article{busch2023Multiview,
  title = {Multi-View Manifold Learning of Human Brain-State Trajectories},
  author = {Busch, Erica L. and Huang, Jessie and Benz, Andrew and Wallenstein, Tom and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita and {Turk-Browne}, Nicholas B.},
  year = {2023},
  month = mar,
  journal = {Nature Computational Science},
  volume = {3},
  number = {3},
  pages = {240--253},
  publisher = {Nature Publishing Group},
  issn = {2662-8457},
  doi = {10.1038/s43588-023-00419-0},
  urldate = {2023-08-02},
  abstract = {The complexity of the human brain gives the illusion that brain activity is intrinsically high-dimensional. Nonlinear dimensionality-reduction methods such as uniform manifold approximation and t-distributed stochastic neighbor embedding have been used for high-throughput biomedical data. However, they have not been used extensively for brain activity data such as those from functional magnetic resonance imaging (fMRI), primarily due to their inability to maintain dynamic structure. Here we introduce a nonlinear manifold learning method for time-series data---including those from fMRI---called temporal potential of heat-diffusion for affinity-based transition embedding (T-PHATE). In addition to recovering a low-dimensional intrinsic manifold geometry from time-series data, T-PHATE exploits the data's autocorrelative structure to faithfully denoise and unveil dynamic trajectories. We empirically validate T-PHATE on three fMRI datasets, showing that it greatly improves data visualization, classification, and segmentation of the data relative to several other state-of-the-art dimensionality-reduction benchmarks. These improvements suggest many potential applications of T-PHATE to other high-dimensional datasets of temporally diffuse processes.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cognitive neuroscience,Computational models,Computational neuroscience,Data processing},
  file = {/Users/rzielinski/Zotero/storage/TTBQFZQ7/Busch et al_2023_Multi-view manifold learning of human brain-state trajectories.pdf}
}

@misc{Cannoodt2018princurve,
  title = {Princurve 2.0: {{Fit}} a {{Principal Curve}} in {{Arbitrary Dimension}}},
  author = {Cannoodt, Robrecht},
  year = {2018},
  month = jun,
  doi = {10.5281/zenodo.3351282},
  howpublished = {CRAN}
}

@incollection{crainiceanu2016tutorial,
  title = {A Tutorial for Multisequence Clinical Structural Brain {{MRI}}},
  booktitle = {Handbook of Neuroimaging Data Analysis},
  author = {Crainiceanu, Ciprian and Sweeney, Elizabeth M and Eloyan, Ani and Shinohara, Russell T},
  year = {2016},
  pages = {109--133},
  publisher = {CRC Press/Taylor \& Francis Group},
  address = {Boca Raton}
}

@article{delicado2001Another,
  title = {Another {{Look}} at {{Principal Curves}} and {{Surfaces}}},
  author = {Delicado, Pedro},
  year = {2001},
  month = apr,
  journal = {Journal of Multivariate Analysis},
  volume = {77},
  number = {1},
  pages = {84--116},
  issn = {0047-259X},
  doi = {10.1006/jmva.2000.1917},
  urldate = {2022-11-29},
  abstract = {Principal curves have been defined as smooth curves passing through the ``middle'' of a multidimensional data set. They are nonlinear generalizations of the first principal component, a characterization of which is the basis of the definition of principal curves. We establish a new characterization of the first principal component and base our new definition of a principal curve on this property. We introduce the notion of principal oriented points and we prove the existence of principal curves passing through these points. We extend the definition of principal curves to multivariate data sets and propose an algorithm to find them. The new notions lead us to generalize the definition of total variance. Successive principal curves are recursively defined from this generalization. The new methods are illustrated on simulated and real data sets.},
  langid = {english},
  keywords = {fixed points,generalized total variance,nonlinear multivariate analysis,principal components,smoothing techniques},
  file = {/Users/rzielinski/Zotero/storage/A7W57HKK/Delicado - 2001 - Another Look at Principal Curves and Surfaces.pdf}
}

@article{ding2023learning,
  title = {Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: {{An}} Integral Operator Approach},
  shorttitle = {Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data},
  author = {Ding, Xiucai and Ma, Rong},
  year = {2023},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {51},
  number = {4},
  pages = {1744--1769},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/23-AOS2306},
  urldate = {2024-06-25},
  abstract = {We propose a kernel-spectral embedding algorithm for learning low-dimensional nonlinear structures from noisy and high-dimensional observations, where the data sets are assumed to be sampled from a nonlinear manifold model and corrupted by high-dimensional noise. The algorithm employs an adaptive bandwidth selection procedure which does not rely on prior knowledge of the underlying manifold. The obtained low-dimensional embeddings can be further utilized for downstream purposes such as data visualization, clustering and prediction. Our method is theoretically justified and practically interpretable. Specifically, for a general class of kernel functions, we establish the convergence of the final embeddings to their noiseless counterparts when the dimension grows polynomially with the size, and characterize the effect of the signal-to-noise ratio on the rate of convergence and phase transition. We also prove the convergence of the embeddings to the eigenfunctions of an integral operator defined by the kernel map of some reproducing kernel Hilbert space capturing the underlying nonlinear structures. Our results hold even when the dimension of the manifold grows with the sample size. Numerical simulations and analysis of real data sets show the superior empirical performance of the proposed method, compared to many existing methods, on learning various nonlinear manifolds in diverse applications.},
  keywords = {47G10,62R07,62R30,High-dimensional data,kernel method,manifold learning,nonlinear dimension reduction,Spectral method},
  file = {/Users/rzielinski/Zotero/storage/MQEFPW3Z/Ding_Ma_2023_Learning low-dimensional nonlinear structures from high-dimensional noisy data.pdf}
}

@article{duchamp1996extremal,
  title = {Extremal Properties of Principal Curves in the Plane},
  author = {Duchamp, Tom and Stuetzle, Werner},
  year = {1996},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {24},
  number = {4},
  pages = {1511--1520},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1032298280},
  urldate = {2023-07-06},
  abstract = {Principal curves were introduced to formalize the notion of "a curve passing through the middle of a dataset." Vaguely speaking, a curve is said to pass through the middle of a dataset if every point on the curve is the average of the observations projecting onto it. This idea can be made precise by defining principal curves for probability densities. In this paper we study principal curves in the plane. Like linear principal components, principal curves are critical points of the expected squared distance from the data. However, the largest and smallest principal components are extrema of the distance, whereas all principal curves are saddle points. This explains why cross-validation does not appear to be a viable method for choosing the complexity of principal curve estimates.},
  keywords = {62G07,62H25,62H30,62J02,calculus of variations,curve fitting,least squares,Nonlinear regression,Principal curves}
}

@inproceedings{duchon1977splines,
  title = {Splines Minimizing Rotation-Invariant Semi-Norms in {{Sobolev}} Spaces},
  booktitle = {Constructive {{Theory}} of {{Functions}} of {{Several Variables}}},
  author = {Duchon, Jean},
  editor = {Schempp, Walter and Zeller, Karl},
  year = {1977},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  pages = {85--100},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0086566},
  abstract = {We define a family of semi-norms {\textbardbl}{$\mu$}{\textbardbl}m,s=({$\int\mathbb{R}$}n{$\mid\tau\mid$}2s{$\midF$} Dmu({$\tau$}){$\mid$}2 d{$\tau$})1/2 Minimizing such semi-norms, subject to some interpolating conditions, leads to functions of very simple forms, providing interpolation methods that: 1{$^\circ$}) preserve polynomials of degree{$\leq$}m-1; 2{$^\circ$}) commute with similarities as well as translations and rotations of {$\mathbb{R}$}n; and 3{$^\circ$}) converge in Sobolev spaces Hm+s({\textohm}).},
  isbn = {978-3-540-37496-1},
  langid = {english},
  keywords = {Closed Linear Subspace,Countable Family,Sobolev Space,Surface Spline,Thin Plate}
}

@misc{dunsonInferringManifoldsNoisy2022,
  title = {Inferring {{Manifolds From Noisy Data Using Gaussian Processes}}},
  author = {Dunson, David B. and Wu, Nan},
  year = {2022},
  month = oct,
  number = {arXiv:2110.07478},
  eprint = {2110.07478},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  abstract = {In analyzing complex datasets, it is often of interest to infer lower dimensional structure underlying the higher dimensional observations. As a flexible class of nonlinear structures, it is common to focus on Riemannian manifolds. Most existing manifold learning algorithms replace the original data with lower dimensional coordinates without providing an estimate of the manifold in the observation space or using the manifold to denoise the original data. This article proposes a new methodology for addressing these problems, allowing interpolation of the estimated manifold between fitted data points. The proposed approach is motivated by novel theoretical properties of local covariance matrices constructed from noisy samples on a manifold. Our results enable us to turn a global manifold reconstruction problem into a local regression problem, allowing application of Gaussian processes for probabilistic manifold reconstruction. In addition to theory justifying the algorithm, we provide simulated and real data examples to illustrate the performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/rzielinski/Zotero/storage/LF2AJ9FT/Dunson_Wu_2022_Inferring Manifolds From Noisy Data Using Gaussian Processes.pdf;/Users/rzielinski/Zotero/storage/26C35GHC/2110.html}
}

@article{einbeckLocalPrincipalCurves2005,
  title = {Local Principal Curves},
  author = {Einbeck, Jochen and Tutz, Gerhard and Evers, Ludger},
  year = {2005},
  month = oct,
  journal = {Statistics and Computing},
  volume = {15},
  number = {4},
  pages = {301--313},
  issn = {1573-1375},
  doi = {10.1007/s11222-005-4073-8},
  urldate = {2024-06-18},
  abstract = {Principal components are a well established tool in dimension reduction. The extension to principal curves allows for general smooth curves which pass through the middle of a multidimensional data cloud. In this paper local principal curves are introduced, which are based on the localization of principal component analysis. The proposed algorithm is able to identify closed curves as well as multiple curves which may or may not be connected. For the evaluation of the performance of principal curves as tool for data reduction a measure of coverage is suggested. By use of simulated and real data sets the approach is compared to various alternative concepts of principal curves.},
  langid = {english},
  keywords = {local smoothing,mean shift,principal components,principal curves},
  file = {/Users/rzielinski/Zotero/storage/4AH4JBUR/Einbeck et al_2005_Local principal curves.pdf}
}

@article{fasy2014confidence,
  title = {Confidence Sets for Persistence Diagrams},
  author = {Fasy, Brittany Terese and Lecci, Fabrizio and Rinaldo, Alessandro and Wasserman, Larry and Balakrishnan, Sivaraman and Singh, Aarti},
  year = {2014},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {6},
  pages = {2301--2339},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/14-AOS1252},
  urldate = {2023-07-06},
  abstract = {Persistent homology is a method for probing topological properties of point clouds and functions. The method involves tracking the birth and death of topological features (2000) as one varies a tuning parameter. Features with short lifetimes are informally considered to be ``topological noise,'' and those with a long lifetime are considered to be ``topological signal.'' In this paper, we bring some statistical ideas to persistent homology. In particular, we derive confidence sets that allow us to separate topological signal from topological noise.},
  keywords = {62G05,62G20,62H12,Density estimation,Persistent homology,topology}
}

@article{fefferman2016testing,
  title = {Testing the Manifold Hypothesis},
  author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  year = {2016},
  month = oct,
  journal = {Journal of the American Mathematical Society},
  volume = {29},
  number = {4},
  pages = {983--1049},
  issn = {0894-0347, 1088-6834},
  doi = {10.1090/jams/852},
  urldate = {2024-02-09},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/Users/rzielinski/Zotero/storage/M8F2IY2P/Fefferman et al_2016_Testing the manifold hypothesis.pdf}
}

@article{fellhauer2015Comparison,
  title = {Comparison of Automated Brain Segmentation Using a Brain Phantom and Patients with Early {{Alzheimer}}'s Dementia or Mild Cognitive Impairment},
  author = {Fellhauer, Iven and Z{\"o}llner, Frank G. and Schr{\"o}der, Johannes and Degen, Christina and Kong, Li and Essig, Marco and Thomann, Philipp A. and Schad, Lothar R.},
  year = {2015},
  month = sep,
  journal = {Psychiatry Research: Neuroimaging},
  volume = {233},
  number = {3},
  pages = {299--305},
  issn = {0925-4927},
  doi = {10.1016/j.pscychresns.2015.07.011},
  urldate = {2025-01-07},
  abstract = {Magnetic resonance imaging (MRI) and brain volumetry allow for the quantification of changes in brain volume using automatic algorithms which are widely used in both, clinical and scientific studies. However, studies comparing the reliability of these programmes are scarce and mainly involved MRI derived from younger healthy controls. This study evaluates the reliability of frequently used segmentation programmes (SPM, FreeSurfer, FSL) using a realistic digital brain phantom and MRI brain acquisitions from patients with manifest Alzheimer's disease (AD, n=34), mild cognitive impairment (MCI, n=60), and healthy subjects (n=32) matched for age and sex. Analysis of the brain phantom dataset demonstrated that SPM, FSL and FreeSurfer underestimate grey matter and overestimate white matter volumes with increasing noise. FreeSurfer calculated overall smaller brain volumes with increasing noise. Image inhomogeneity had only minor, non- significant effects on the results obtained with SPM and FreeSurfer 5.1, but had effects on the FSL results (increased white matter volumes with decreased grey matter volumes). The analysis of the patient data yielded decreasing volumes of grey and white matter with progression of brain atrophy independent of the method used. FreeSurfer calculated the largest grey matter and the smallest white matter volumes. FSL calculated the smallest grey matter volumes; SPM the largest white matter volumes. Best results are obtained with good image quality. With poor image quality, especially noise, SPM provides the best segmentation results. An optimised template for segmentation had no significant effect on segmentation results. While our findings underline the applicability of the programmes investigated, SPM may be the programme of choice when MRIs with limited image quality or brain images of elderly should be analysed.},
  keywords = {AD,Brainweb,FreeSurfer,FSL,MCI,MRI,SPM},
  file = {/Users/rzielinski/Zotero/storage/GY2GYTTK/S0925492715300378.html}
}

@article{fjell2014What,
  title = {What Is Normal in Normal Aging? {{Effects}} of Aging, Amyloid and {{Alzheimer}}'s Disease on the Cerebral Cortex and the Hippocampus},
  shorttitle = {What Is Normal in Normal Aging?},
  author = {Fjell, Anders M. and McEvoy, Linda and Holland, Dominic and Dale, Anders M. and Walhovd, Kristine B.},
  year = {2014},
  month = jun,
  journal = {Progress in Neurobiology},
  volume = {117},
  pages = {20--40},
  issn = {0301-0082},
  doi = {10.1016/j.pneurobio.2014.02.004},
  urldate = {2022-11-30},
  abstract = {What can be expected in normal aging, and where does normal aging stop and pathological neurodegeneration begin? With the slow progression of age-related dementias such as Alzheimer's disease (AD), it is difficult to distinguish age-related changes from effects of undetected disease. We review recent research on changes of the cerebral cortex and the hippocampus in aging and the borders between normal aging and AD. We argue that prominent cortical reductions are evident in fronto-temporal regions in elderly even with low probability of AD, including regions overlapping the default mode network. Importantly, these regions show high levels of amyloid deposition in AD, and are both structurally and functionally vulnerable early in the disease. This normalcy-pathology homology is critical to understand, since aging itself is the major risk factor for sporadic AD. Thus, rather than necessarily reflecting early signs of disease, these changes may be part of normal aging, and may inform on why the aging brain is so much more susceptible to AD than is the younger brain. We suggest that regions characterized by a high degree of life-long plasticity are vulnerable to detrimental effects of normal aging, and that this age-vulnerability renders them more susceptible to additional, pathological AD-related changes. We conclude that it will be difficult to understand AD without understanding why it preferably affects older brains, and that we need a model that accounts for age-related changes in AD-vulnerable regions independently of AD-pathology.},
  langid = {english},
  keywords = {Alzheimer's disease (AD),Amyloid,Cerebral cortex,Default mode network (DMN),Hippocampus,Normal aging},
  file = {/Users/rzielinski/Zotero/storage/ALCRFCJJ/S0301008214000288.html}
}

@article{gao2023k,
  title = {K-{{VIL}}: {{Keypoints-Based Visual Imitation Learning}}},
  shorttitle = {K-{{VIL}}},
  author = {Gao, Jianfeng and Tao, Zhi and Jaquier, No{\'e}mie and Asfour, Tamim},
  year = {2023},
  month = oct,
  journal = {IEEE Transactions on Robotics},
  volume = {39},
  number = {5},
  pages = {3888--3908},
  issn = {1941-0468},
  doi = {10.1109/TRO.2023.3286074},
  urldate = {2024-02-09},
  abstract = {Visual imitation learning provides efficient and intuitive solutions for robotic systems to acquire novel manipulation skills. However, simultaneously learning geometric task constraints and control policies from visual inputs alone remains a challenging problem. In this article, we propose the keypoint-based visual imitation learning (K-VIL) approach that automatically extracts sparse, object-centric, and embodiment-independent task representations from a small number of human demonstration videos. The task representation is composed of keypoint-based geometric constraints on principal manifolds, their associated local frames, and the movement primitives that are then needed for the task execution. Our approach is capable of extracting such task representations from a single-demonstration video and of incrementally updating them when new demonstrations are available. To reproduce manipulation skills using the learned set of prioritized geometric constraints in novel scenes, we introduce a novel keypoint-based admittance controller. We evaluate our approach in several real-world applications, showcasing its ability to deal with cluttered scenes, viewpoint mismatch, new instances of categorical objects, and large object pose and shape variations. Our evaluation demonstrates the efficiency and robustness of our approach in both one-shot and few-shot imitation learning settings.},
  keywords = {Learning from demonstration,learning of geometric constraints,Manifolds,manipulation planning,Robots,Shape,Task analysis,Training,Trajectory,visual learning,Visualization},
  file = {/Users/rzielinski/Zotero/storage/44R75KIE/Gao et al_2023_K-VIL.pdf;/Users/rzielinski/Zotero/storage/LU8CNRPC/10189175.html}
}

@misc{gao2024bi,
  title = {Bi-{{KVIL}}: {{Keypoints-based Visual Imitation Learning}} of {{Bimanual Manipulation Tasks}}},
  shorttitle = {Bi-{{KVIL}}},
  author = {Gao, Jianfeng and Jin, Xiaoshu and Krebs, Franziska and Jaquier, No{\'e}mie and Asfour, Tamim},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03270},
  eprint = {2403.03270},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03270},
  urldate = {2024-05-09},
  abstract = {Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning ({\textbackslash}mbox\{K-VIL\}){\textasciitilde}{\textbackslash}cite\{gao\_kvil\_2023\} to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called {\textbackslash}emph\{Hybrid Master-Slave Relationships\} (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/rzielinski/Zotero/storage/SDL6YNAL/Gao et al_2024_Bi-KVIL.pdf;/Users/rzielinski/Zotero/storage/9CMGU44Y/2403.html}
}

@book{greenSilverman1994,
  title = {Nonparametric {{Regression}} and {{Generalized Linear Models}}: {{A Roughness Penalty Approach}}},
  author = {Green, P. J. and Silverman, B.W.},
  year = {1994},
  series = {Monographs on {{Statistics}} and {{Applied Probability}}},
  number = {58},
  publisher = {Springer}
}

@inproceedings{greven2011longitudinal,
  title = {Longitudinal {{Functional Principal Component Analysis}}},
  booktitle = {Recent {{Advances}} in {{Functional Data Analysis}} and {{Related Topics}}},
  author = {Greven, Sonja and Crainiceanu, Ciprian and Caffo, Brian and Reich, Daniel},
  editor = {Ferraty, Fr{\'e}d{\'e}ric},
  year = {2011},
  pages = {149--154},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-7908-2736-1_23},
  abstract = {We introduce models for the analysis of functional data observed at multiple time points. The model can be viewed as the functional analog of the classical mixed effects model where random effects are replaced by random processes. Computational feasibility is assured by using principal component bases. The methodology is motivated by and applied to a diffusion tensor imaging (DTI) study on multiple sclerosis.},
  isbn = {978-3-7908-2736-1},
  langid = {english},
  keywords = {Multiple Sclerosis,Pattern Recognition,Principal Component Analysis,Random Effect,Stochastic Process},
  file = {/Users/rzielinski/Zotero/storage/XWVRYIX4/Greven et al_2011_Longitudinal Functional Principal Component Analysis.pdf}
}

@article{grimm2015Amygdalar,
  title = {Amygdalar and Hippocampal Volume: {{A}} Comparison between Manual Segmentation, {{Freesurfer}} and {{VBM}}},
  shorttitle = {Amygdalar and Hippocampal Volume},
  author = {Grimm, Oliver and Pohlack, Sebastian and Cacciaglia, Raffaele and Winkelmann, Tobias and Plichta, Michael M. and Demirakca, Traute and Flor, Herta},
  year = {2015},
  month = sep,
  journal = {Journal of Neuroscience Methods},
  volume = {253},
  pages = {254--261},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2015.05.024},
  urldate = {2025-01-07},
  abstract = {Automated segmentation of the amygdala and the hippocampus is of interest for research looking at large datasets where manual segmentation of T1-weighted magnetic resonance tomography images is less feasible for morphometric analysis. Manual segmentation still remains the gold standard for subcortical structures like the hippocampus and the amygdala. A direct comparison of VBM8 and Freesurfer is rarely done, because VBM8 results are most often used for voxel-based analysis. We used the same region-of-interest (ROI) for Freesurfer and VBM8 to relate automated and manually derived volumes of the amygdala and the hippocampus. We processed a large manually segmented dataset of n=92 independent samples with an automated segmentation strategy (VBM8 vs. Freesurfer Version 5.0). For statistical analysis, we only calculated Pearsons's correlation coefficients, but used methods developed for comparison such as Lin's concordance coefficient. The correlation between automatic and manual segmentation was high for the hippocampus [0.58--0.76] and lower for the amygdala [0.45--0.59]. However, concordance coefficients point to higher concordance for the amygdala [0.46--0.62] instead of the hippocampus [0.06--0.12]. VBM8 and Freesurfer segmentation performed on a comparable level in comparison to manual segmentation. We conclude (1) that correlation alone does not capture systematic differences (e.g. of hippocampal volumes), (2) calculation of ROI volumes with VBM8 gives measurements comparable to Freesurfer V5.0 when using the same ROI and (3) systematic and proportional differences are caused mainly by different definitions of anatomic boundaries and only to a lesser part by different segmentation strategies. This work underscores the importance of using method comparison techniques and demonstrates that even with high correlation coefficients, there can be still large differences in absolute volume.},
  keywords = {Amygdala,Freesurfer,Hippocampus,Method comparison,VBM8,Volumetry},
  file = {/Users/rzielinski/Zotero/storage/Z7M5RJMT/S0165027015002150.html}
}

@article{hastie1989Principal,
  title = {Principal {{Curves}}},
  author = {Hastie, Trevor and Stuetzle, Werner},
  year = {1989},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {84},
  number = {406},
  pages = {502--516},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1989.10478797},
  urldate = {2022-11-29},
  abstract = {Principal curves are smooth one-dimensional curves that pass through the middle of a p-dimensional data set, providing a nonlinear summary of the data. They are nonparametric, and their shape is suggested by the data. The algorithm for constructing principal curves starts with some prior summary, such as the usual principal-component line. The curve in each successive iteration is a smooth or local average of the p-dimensional points, where the definition of local is based on the distance in arc length of the projections of the points onto the curve found in the previous iteration. In this article principal curves are defined, an algorithm for their construction is given, some theoretical results are presented, and the procedure is compared to other generalizations of principal components. Two applications illustrate the use of principal curves. The first describes how the principal-curve procedure was used to align the magnets of the Stanford linear collider. The collider uses about 950 magnets in a roughly circular arrangement to bend electron and positron beams and bring them to collision. After construction, it was found that some of the magnets had ended up significantly out of place. As a result, the beams had to be bent too sharply and could not be focused. The engineers realized that the magnets did not have to be moved to their originally planned locations, but rather to a sufficiently smooth arc through the middle of the existing positions. This arc was found using the principal-curve procedure. In the second application, two different assays for gold content in several samples of computer-chip waste appear to show some systematic differences that are blurred by measurement error. The classical approach using linear errors in variables regression can detect systematic linear differences but is not able to account for nonlinearities. When the first linear principal component is replaced with a principal curve, a local ``bump'' is revealed, and bootstrapping is used to verify its presence.},
  langid = {english},
  file = {/Users/rzielinski/Zotero/storage/TMMWWYQ6/Hastie and Stuetzle - 1989 - Principal Curves.pdf}
}

@article{hastie1993VaryingCoefficient,
  title = {Varying-{{Coefficient Models}}},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1993},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {55},
  number = {4},
  pages = {757--779},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1993.tb01939.x},
  urldate = {2023-08-10},
  abstract = {We explore a class of regression and generalized regression models in which the coefficients are allowed to vary as smooth functions of other variables. General algorithms are presented for estimating the models flexibly and some examples are given. This class of models ties together generalized additive models and dynamic generalized linear models into one common framework. When applied to the proportional hazards model for survival data, this approach provides a new way of modelling departures from the proportional hazards assumption.},
  copyright = {{\copyright} 1993 Royal Statistical Society},
  langid = {english},
  keywords = {dynamic generalized linear models,generalized additive models,generalized linear models,regression,smoothing,splines,survival analysis},
  file = {/Users/rzielinski/Zotero/storage/Z962T4LD/j.2517-6161.1993.tb01939.html}
}

@book{hatcher2002algebraic,
  title = {Algebraic {{Topology}}},
  author = {Hatcher, Allen},
  year = {2002},
  publisher = {Cambridge University Press}
}

@article{haubergPrincipalCurvesRiemannian2016,
  title = {Principal {{Curves}} on {{Riemannian Manifolds}}},
  author = {Hauberg, S{\o}ren},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {9},
  pages = {1915--1921},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2015.2496166},
  urldate = {2024-06-18},
  abstract = {Euclidean statistics are often generalized to Riemannian manifolds by replacing straight-line interpolations with geodesic ones. While these Riemannian models are familiar-looking, they are restricted by the inflexibility of geodesics, and they rely on constructions which are optimal only in Euclidean domains. We consider extensions of Principal Component Analysis (PCA) to Riemannian manifolds. Classic Riemannian approaches seek a geodesic curve passing through the mean that optimizes a criteria of interest. The requirements that the solution both is geodesic and must pass through the mean tend to imply that the methods only work well when the manifold is mostly flat within the support of the generating distribution. We argue that instead of generalizing linear Euclidean models, it is more fruitful to generalize non-linear Euclidean models. Specifically, we extend the classic Principal Curves from Hastie \& Stuetzle to data residing on a complete Riemannian manifold. We show that for elliptical distributions in the tangent of spaces of constant curvature, the standard principal geodesic is a principal curve. The proposed model is simple to compute and avoids many of the pitfalls of traditional geodesic approaches. We empirically demonstrate the effectiveness of the Riemannian principal curves on several manifolds and datasets.},
  keywords = {Computational modeling,Data models,differential geometry,Electronics packaging,Manifolds,Market research,Measurement,Principal component analysis,principal curves,Riemannian manifolds},
  file = {/Users/rzielinski/Zotero/storage/P7HR2GFU/Hauberg_2016_Principal Curves on Riemannian Manifolds.pdf;/Users/rzielinski/Zotero/storage/RMDUHUL9/7312494.html}
}

@article{jack2008adni,
  title = {The {{Alzheimer}}'s Disease Neuroimaging Initiative ({{ADNI}}): {{MRI}} Methods},
  shorttitle = {The {{Alzheimer}}'s Disease Neuroimaging Initiative ({{ADNI}})},
  author = {Jack Jr., Clifford R. and Bernstein, Matt A. and Fox, Nick C. and Thompson, Paul and Alexander, Gene and Harvey, Danielle and Borowski, Bret and Britson, Paula J. and L. Whitwell, Jennifer and Ward, Chadwick and Dale, Anders M. and Felmlee, Joel P. and Gunter, Jeffrey L. and Hill, Derek L.G. and Killiany, Ron and Schuff, Norbert and {Fox-Bosetti}, Sabrina and Lin, Chen and Studholme, Colin and DeCarli, Charles S. and Krueger, Gunnar and Ward, Heidi A. and Metzger, Gregory J. and Scott, Katherine T. and Mallozzi, Richard and Blezek, Daniel and Levy, Joshua and Debbins, Josef P. and Fleisher, Adam S. and Albert, Marilyn and Green, Robert and Bartzokis, George and Glover, Gary and Mugler, John and Weiner, Michael W.},
  year = {2008},
  journal = {Journal of Magnetic Resonance Imaging},
  volume = {27},
  number = {4},
  pages = {685--691},
  issn = {1522-2586},
  doi = {10.1002/jmri.21049},
  urldate = {2023-07-07},
  abstract = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) is a longitudinal multisite observational study of healthy elders, mild cognitive impairment (MCI), and Alzheimer's disease. Magnetic resonance imaging (MRI), (18F)-fluorodeoxyglucose positron emission tomography (FDG PET), urine serum, and cerebrospinal fluid (CSF) biomarkers, as well as clinical/psychometric assessments are acquiredat multiple time points. All data will be cross-linked and made available to the general scientific community. The purpose of this report is to describe the MRI methods employed in ADNI. The ADNI MRI core established specifications thatguided protocol development. A major effort was devoted toevaluating 3D T1-weighted sequences for morphometric analyses. Several options for this sequence were optimized for the relevant manufacturer platforms and then compared in a reduced-scale clinical trial. The protocol selected for the ADNI study includes: back-to-back 3D magnetization prepared rapid gradient echo (MP-RAGE) scans; B1-calibration scans when applicable; and an axial proton density-T2 dual contrast (i.e., echo) fast spin echo/turbo spin echo (FSE/TSE) for pathology detection. ADNI MRI methods seek to maximize scientific utility while minimizing the burden placed on participants. The approach taken in ADNI to standardization across sites and platforms of the MRI protocol, postacquisition corrections, and phantom-based monitoring of all scanners could be used as a model for other multisite trials. J. Magn. Reson. Imaging 2008. {\copyright} 2008 Wiley-Liss, Inc.},
  copyright = {Copyright {\copyright} 2008 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {Alzheimer's disease,clinical trials,imaging methods,imaging standardization,MRI},
  file = {/Users/rzielinski/Zotero/storage/57W567QB/jmri.html}
}

@article{jenkinsonFSL2012,
  title = {{{FSL}}},
  author = {Jenkinson, Mark and Beckmann, Christian F. and Behrens, Timothy E. J. and Woolrich, Mark W. and Smith, Stephen M.},
  year = {2012},
  month = aug,
  journal = {NeuroImage},
  series = {20 {{YEARS OF fMRI}}},
  volume = {62},
  number = {2},
  pages = {782--790},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2011.09.015},
  urldate = {2024-06-24},
  abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on ``20 years of fMRI'' we have been asked to write about the history, developments and current status of FSL. We also include some descriptions of parts of FSL that are not well covered in the existing literature. We hope that some of this content might be of interest to users of FSL, and also maybe to new research groups considering creating, releasing and supporting new software packages for brain image analysis.},
  keywords = {FSL,Software},
  file = {/Users/rzielinski/Zotero/storage/3DBESR3U/Jenkinson et al_2012_FSL.pdf;/Users/rzielinski/Zotero/storage/SIK92HI7/S1053811911010603.html}
}

@article{kahhale2023Quantifying,
  title = {Quantifying Numerical and Spatial Reliability of Hippocampal and Amygdala Subdivisions in {{FreeSurfer}}},
  author = {Kahhale, Isabella and Buser, Nicholas J. and Madan, Christopher R. and Hanson, Jamie L.},
  year = {2023},
  month = apr,
  journal = {Brain Informatics},
  volume = {10},
  number = {1},
  pages = {9},
  issn = {2198-4026},
  doi = {10.1186/s40708-023-00189-5},
  urldate = {2025-01-07},
  abstract = {On-going, large-scale neuroimaging initiatives can aid in uncovering neurobiological causes and correlates of poor mental health, disease pathology, and many other important conditions. As projects grow in scale with hundreds, even thousands, of individual participants and scans collected, quantification of brain structures by automated algorithms is becoming the only truly tractable approach. Here, we assessed the spatial and numerical reliability for newly deployed automated segmentation of hippocampal subfields and amygdala nuclei in FreeSurfer 7. In a sample of participants with repeated structural imaging scans (N\,=\,928), we found numerical reliability (as assessed by intraclass correlations, ICCs) was reasonable. Approximately 95\% of hippocampal subfields had ``excellent'' numerical reliability (ICCs\,{$\geq$}\,0.90), while only 67\% of amygdala subnuclei met this same threshold. In terms of spatial reliability, 58\% of hippocampal subfields and 44\% of amygdala subnuclei had Dice coefficients\,{$\geq$}\,0.70. Notably, multiple regions had poor numerical and/or spatial reliability. We also examined correlations between spatial reliability and person-level factors (e.g., participant age; T1 image quality). Both sex and image scan quality were related to variations in spatial reliability metrics. Examined collectively, our work suggests caution should be exercised for a few hippocampal subfields and amygdala nuclei with more variable reliability.},
  langid = {english},
  keywords = {Amygdala,Artificial Intelligence,Automated segmentation,FreeSurfer,FreeSurfer 7.1,Hippocampus},
  file = {/Users/rzielinski/Zotero/storage/SLMYSRUA/Kahhale et al_2023_Quantifying numerical and spatial reliability of hippocampal and amygdala.pdf}
}

@article{katabathulaPredictAlzheimerDisease2021,
  title = {Predict {{Alzheimer}}'s Disease Using Hippocampus {{MRI}} Data: A Lightweight {{3D}} Deep Convolutional Network Model with Visual and Global Shape Representations},
  shorttitle = {Predict {{Alzheimer}}'s Disease Using Hippocampus {{MRI}} Data},
  author = {Katabathula, Sreevani and Wang, Qinyong and Xu, Rong},
  year = {2021},
  month = dec,
  journal = {Alzheimer's Research \& Therapy},
  volume = {13},
  number = {1},
  pages = {1--9},
  publisher = {BioMed Central},
  issn = {1758-9193},
  doi = {10.1186/s13195-021-00837-0},
  urldate = {2023-04-19},
  abstract = {Alzheimer's disease (AD) is a progressive and irreversible brain disorder. Hippocampus is one of the involved regions and its atrophy is a widely used biomarker for AD diagnosis. We have recently developed DenseCNN, a lightweight 3D deep convolutional network model, for AD classification based on hippocampus magnetic resonance imaging (MRI) segments. In addition to the visual features of the hippocampus segments, the global shape representations of the hippocampus are also important for AD diagnosis. In this study, we propose DenseCNN2, a deep convolutional network model for AD classification by incorporating global shape representations along with hippocampus segmentations. The data was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and was T1-weighted structural MRI from initial screening or baseline, including ADNI 1,2/GO and 3. DenseCNN2 was trained and evaluated with 326 AD subjects and 607 CN hippocampus MRI using 5-fold cross-validation strategy. DenseCNN2 was compared with other state-of-the-art machine learning approaches for the task of AD classification. We showed that DenseCNN2 with combined visual and global shape features performed better than deep learning models with visual or global shape features alone. DenseCNN2 achieved an average accuracy of 0.925, sensitivity of 0.882, specificity of 0.949, and area under curve (AUC) of 0.978, which are better than or comparable to the state-of-the-art methods in AD classification. Data visualization analysis through 2D embedding of UMAP confirmed that global shape features improved class discrimination between AD and normal. DenseCNN2, a lightweight 3D deep convolutional network model based on combined hippocampus segmentations and global shape features, achieved high performance and has potential as an efficient diagnostic tool for AD classification.},
  copyright = {2021 The Author(s)},
  langid = {english}
}

@article{kegl2000learning,
  title = {Learning and Design of Principal Curves},
  author = {Kegl, B. and Krzyzak, A. and Linder, T. and Zeger, K.},
  year = {2000},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {3},
  pages = {281--297},
  issn = {1939-3539},
  doi = {10.1109/34.841759},
  abstract = {Principal curves have been defined as "self-consistent" smooth curves which pass through the "middle" of a d-dimensional probability distribution or data cloud. They give a summary of the data and also serve as an efficient feature extraction tool. We take a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution. The new definition makes it possible to theoretically analyze principal curve learning from training data and it also leads to a new practical construction. Our theoretical learning scheme chooses a curve from a class of polygonal lines with k segments and with a given total length to minimize the average squared distance over n training points drawn independently. Convergence properties of this learning scheme are analyzed and a practical version of this theoretical algorithm is implemented. In each iteration of the algorithm, a new vertex is added to the polygonal line and the positions of the vertices are updated so that they minimize a penalized squared distance criterion. Simulation results demonstrate that the new algorithm compares favorably with previous methods, both in terms of performance and computational complexity, and is more robust to varying data models.},
  keywords = {Algorithm design and analysis,Clouds,Computational complexity,Computational modeling,Convergence,Data models,Feature extraction,Probability distribution,Robustness,Training data},
  file = {/Users/rzielinski/Zotero/storage/AAD8REPU/841759.html}
}

@article{kinson2020Longitudinal,
  title = {Longitudinal {{Principal Component Analysis With}} an {{Application}} to {{Marketing Data}}},
  author = {Kinson, Christopher and Tang, Xiwei and Zuo, Zhen and Qu, Annie},
  year = {2020},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {2},
  pages = {335--350},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2019.1677244},
  urldate = {2023-01-17},
  abstract = {We propose a longitudinal principal component analysis method for multivariate longitudinal data using a random-effects eigen-decomposition, where the eigen-decomposition uses longitudinal information through nonparametric splines and the multivariate random effects incorporate significant store-wise heterogeneity. Our method can effectively analyze large marketing data containing sales information for products from hundreds of stores over an 11-year time period. The proposed method leads to more accurate estimation and interpretation compared to existing approaches. We illustrate our method through simulation studies and an application to marketing data from IRI. Supplementary materials for this article are available online.},
  keywords = {Marketing data,Multivariate longitudinal data,Nonparametric spline,Principal component analysis,Random-effects model,Time-varying covariance matrix},
  file = {/Users/rzielinski/Zotero/storage/4QK5QB4D/Kinson et al_2020_Longitudinal Principal Component Analysis With an Application to Marketing Data.pdf}
}

@article{knopman2021Alzheimer,
  title = {Alzheimer Disease},
  author = {Knopman, David S. and Amieva, Helene and Petersen, Ronald C. and Ch{\'e}telat, G{\"a}el and Holtzman, David M. and Hyman, Bradley T. and Nixon, Ralph A. and Jones, David T.},
  year = {2021},
  month = may,
  journal = {Nature Reviews Disease Primers},
  volume = {7},
  number = {1},
  pages = {1--21},
  publisher = {Nature Publishing Group},
  issn = {2056-676X},
  doi = {10.1038/s41572-021-00269-y},
  urldate = {2023-01-23},
  abstract = {Alzheimer disease (AD) is biologically defined by the presence of {$\beta$}-amyloid-containing plaques and tau-containing neurofibrillary tangles. AD is a genetic and sporadic neurodegenerative disease that causes an amnestic cognitive impairment in its prototypical presentation and non-amnestic cognitive impairment in its less common variants. AD is a common cause of cognitive impairment acquired in midlife and late-life but its clinical impact is modified by other neurodegenerative and cerebrovascular conditions. This Primer conceives of AD biology as the brain disorder that results from a complex interplay of loss of synaptic homeostasis and dysfunction in the highly interrelated endosomal/lysosomal clearance pathways in which the precursors, aggregated species and post-translationally modified products of A{$\beta$} and tau play important roles. Therapeutic endeavours are still struggling to find targets within this framework that substantially change the clinical course in persons with AD.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Alzheimer's disease,Diagnostic markers,Translational research},
  file = {/Users/rzielinski/Zotero/storage/XDXN3F8U/Knopman et al_2021_Alzheimer disease.pdf}
}

@article{konrad2009Defining,
  title = {Defining the Human Hippocampus in Cerebral Magnetic Resonance Images---{{An}} Overview of Current Segmentation Protocols},
  author = {Konrad, C. and Ukas, T. and Nebel, C. and Arolt, V. and Toga, A.W. and Narr, K.L.},
  year = {2009},
  month = oct,
  journal = {NeuroImage},
  volume = {47},
  number = {4},
  pages = {1185--1195},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.05.019},
  urldate = {2023-01-25},
  abstract = {Due to its crucial role for memory processes and its relevance in neurological and psychiatric disorders, the hippocampus has been the focus of neuroimaging research for several decades. In vivo measurement of human hippocampal volume and shape with magnetic resonance imaging has become an important element of neuroimaging research. Nevertheless, volumetric findings are still inconsistent and controversial for many psychiatric conditions including affective disorders. Here we review the wealth of anatomical protocols for the delineation of the hippocampus in MR images, taking into consideration 71 different published protocols from the neuroimaging literature, with an emphasis on studies of affective disorders. We identified large variations between protocols in five major areas. 1) The inclusion/exclusion of hippocampal white matter (alveus and fimbria), 2) the definition of the anterior hippocampal--amygdala border, 3) the definition of the posterior border and the extent to which the hippocampal tail is included, 4) the definition of the inferior medial border of the hippocampus, and 5) the use of varying arbitrary lines. These are major sources of variance between different protocols. In contrast, the definitions of the lateral, superior, and inferior borders are less disputed. Directing resources to replication studies that incorporate characteristics of the segmentation protocols presented herein may help resolve seemingly contradictory volumetric results between prior neuroimaging studies and facilitate the appropriate selection of protocols for manual or automated delineation of the hippocampus for future research purposes.},
  langid = {english}
}

@article{li2023inference,
  title = {Inference for {{Gaussian Processes}} with {{Matern Covariogram}} on {{Compact Riemannian Manifolds}}},
  author = {Li, Didong and Tang, Wenpin and Banerjee, Sudipto},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {101},
  pages = {1--26},
  issn = {1533-7928},
  urldate = {2024-06-25},
  abstract = {Gaussian processes are widely employed as versatile modelling and predictive tools in spatial statistics, functional data analysis, computer modelling and diverse applications of machine learning. They have been widely studied over Euclidean spaces, where they are specified using covariance functions or covariograms for modelling complex dependencies. There is a growing literature on Gaussian processes over Riemannian manifolds in order to develop richer and more flexible inferential frameworks for non-Euclidean data. While numerical approximations through graph representations have been well studied for the Matern covariogram and heat kernel, the behaviour of asymptotic inference on the parameters of the covariogram has received relatively scant attention. We focus on asymptotic behaviour for Gaussian processes constructed over compact Riemannian manifolds. Building upon a recently introduced Matern covariogram on a compact Riemannian manifold, we employ formal notions and conditions for the equivalence of two Matern Gaussian random measures on compact manifolds to derive the parameter that is identifiable, also known as the microergodic parameter, and formally establish the consistency of the maximum likelihood estimate and the asymptotic optimality of the best linear unbiased predictor. The circle is studied as a specific example of compact Riemannian manifolds with numerical experiments to illustrate and corroborate the theory.},
  file = {/Users/rzielinski/Zotero/storage/6X27W2AV/Li et al_2023_Inference for Gaussian Processes with Matern Covariogram on Compact Riemannian.pdf}
}

@article{lidauer2022Subcortical,
  title = {Subcortical and Hippocampal Brain Segmentation in 5-Year-Old Children: {{Validation}} of {{FSL-FIRST}} and {{FreeSurfer}} against Manual Segmentation},
  shorttitle = {Subcortical and Hippocampal Brain Segmentation in 5-Year-Old Children},
  author = {Lidauer, Kristian and Pulli, Elmo P. and Copeland, Anni and Silver, Eero and Kumpulainen, Venla and Hashempour, Niloofar and Merisaari, Harri and Saunavaara, Jani and Parkkola, Riitta and L{\"a}hdesm{\"a}ki, Tuire and Saukko, Ekaterina and Nolvi, Saara and Kataja, Eeva-Leena and Karlsson, Linnea and Karlsson, Hasse and Tuulari, Jetro J.},
  year = {2022},
  journal = {European Journal of Neuroscience},
  volume = {56},
  number = {5},
  pages = {4619--4641},
  issn = {1460-9568},
  doi = {10.1111/ejn.15761},
  urldate = {2025-01-07},
  abstract = {Developing accurate subcortical volumetric quantification tools is crucial for neurodevelopmental studies, as they could reduce the need for challenging and time-consuming manual segmentation. In this study, the accuracy of two automated segmentation tools, FSL-FIRST (with three different boundary correction settings) and FreeSurfer, were compared against manual segmentation of the hippocampus and subcortical nuclei, including the amygdala, thalamus, putamen, globus pallidus, caudate and nucleus accumbens, using volumetric and correlation analyses in 80 5-year-olds. Both FSL-FIRST and FreeSurfer overestimated the volume on all structures except the caudate, and the accuracy varied depending on the structure. Small structures such as the amygdala and nucleus accumbens, which are visually difficult to distinguish, produced significant overestimations and weaker correlations with all automated methods. Larger and more readily distinguishable structures such as the caudate and putamen produced notably lower overestimations and stronger correlations. Overall, the segmentations performed by FSL-FIRST's default pipeline were the most accurate, whereas FreeSurfer's results were weaker across the structures. In line with prior studies, the accuracy of automated segmentation tools was imperfect with respect to manually defined structures. However, apart from amygdala and nucleus accumbens, FSL-FIRST's agreement could be considered satisfactory (Pearson correlation {$>$} 0.74, intraclass correlation coefficient (ICC) {$>$} 0.68 and Dice score coefficient (DSC) {$>$} 0.87) with highest values for the striatal structures (putamen, globus pallidus, caudate) (Pearson correlation {$>$} 0.77, ICC {$>$} 0.87 and DSC {$>$} 0.88, respectively). Overall, automated segmentation tools do not always provide satisfactory results, and careful visual inspection of the automated segmentations is strongly advised.},
  copyright = {{\copyright} 2022 The Authors. European Journal of Neuroscience published by Federation of European Neuroscience Societies and John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {brain,brain (growth and development),child,neuroimaging},
  file = {/Users/rzielinski/Zotero/storage/JTZA8IWN/Lidauer et al_2022_Subcortical and hippocampal brain segmentation in 5-year-old children.pdf;/Users/rzielinski/Zotero/storage/ZNJXTRKU/ejn.html}
}

@inproceedings{louis2019Riemannian,
  title = {Riemannian {{Geometry Learning}} for {{Disease Progression Modelling}}},
  booktitle = {Information {{Processing}} in {{Medical Imaging}}},
  author = {Louis, Maxime and Couronn{\'e}, Rapha{\"e}l and Koval, Igor and Charlier, Benjamin and Durrleman, Stanley},
  editor = {Chung, Albert C. S. and Gee, James C. and Yushkevich, Paul A. and Bao, Siqi},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {542--553},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-20351-1_42},
  abstract = {The analysis of longitudinal trajectories is a longstanding problem in medical imaging which is often tackled in the context of Riemannian geometry: the set of observations is assumed to lie on an a priori known Riemannian manifold. When dealing with high-dimensional or complex data, it is in general not possible to design a Riemannian geometry of relevance. In this paper, we perform Riemannian manifold learning in association with the statistical task of longitudinal trajectory analysis. After inference, we obtain both a submanifold of observations and a Riemannian metric so that the observed progressions are geodesics. This is achieved using a deep generative network, which maps trajectories in a low-dimensional Euclidean space to the observation space.},
  isbn = {978-3-030-20351-1},
  langid = {english},
  keywords = {Longitudinal progression,Medical imaging,Riemannian geometry},
  file = {/Users/rzielinski/Zotero/storage/G76FQVLD/Louis et al_2019_Riemannian Geometry Learning for Disease Progression Modelling.pdf}
}

@article{meng2021Principal,
  title = {Principal Manifold Estimation via Model Complexity Selection},
  author = {Meng, Kun and Eloyan, Ani},
  year = {2021},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {83},
  number = {2},
  pages = {369--394},
  issn = {1467-9868},
  doi = {10.1111/rssb.12416},
  urldate = {2022-11-29},
  abstract = {We propose a framework of principal manifolds to model high-dimensional data. This framework is based on Sobolev spaces and designed to model data of any intrinsic dimension. It includes principal component analysis and principal curve algorithm as special cases. We propose a novel method for model complexity selection to avoid overfitting, eliminate the effects of outliers and improve the computation speed. Additionally, we propose a method for identifying the interiors of circle-like curves and cylinder/ball-like surfaces. The proposed approach is compared to existing methods by simulations and applied to estimate tumour surfaces and interiors in a lung cancer study.},
  langid = {english},
  keywords = {lung cancer,splines,total squared curvature,tumour interior},
  file = {/Users/rzielinski/Zotero/storage/6SFTW3GD/Meng and Eloyan - 2021 - Principal manifold estimation via model complexity.pdf}
}

@article{mulder2014Hippocampal,
  title = {Hippocampal Volume Change Measurement: {{Quantitative}} Assessment of the Reproducibility of Expert Manual Outlining and the Automated Methods {{FreeSurfer}} and {{FIRST}}},
  shorttitle = {Hippocampal Volume Change Measurement},
  author = {Mulder, Emma R. and {de Jong}, Remko A. and Knol, Dirk L. and {van Schijndel}, Ronald A. and Cover, Keith S. and Visser, Pieter J. and Barkhof, Frederik and Vrenken, Hugo},
  year = {2014},
  month = may,
  journal = {NeuroImage},
  volume = {92},
  pages = {169--181},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.01.058},
  urldate = {2023-01-30},
  abstract = {Background To measure hippocampal volume change in Alzheimer's disease (AD) or mild cognitive impairment (MCI), expert manual delineation is often used because of its supposed accuracy. It has been suggested that expert outlining yields poorer reproducibility as compared to automated methods, but this has not been investigated. Aim To determine the reproducibilities of expert manual outlining and two common automated methods for measuring hippocampal atrophy rates in healthy aging, MCI and AD. Methods From the Alzheimer's Disease Neuroimaging Initiative (ADNI), 80 subjects were selected: 20 patients with AD, 40 patients with mild cognitive impairment (MCI) and 20 healthy controls (HCs). Left and right hippocampal volume change between baseline and month-12 visit was assessed by using expert manual delineation, and by the automated software packages FreeSurfer (longitudinal processing stream) and FIRST. To assess reproducibility of the measured hippocampal volume change, both back-to-back (BTB) MPRAGE scans available for each visit were analyzed. Hippocampal volume change was expressed in {$\mu$}L, and as a percentage of baseline volume. Reproducibility of the 1-year hippocampal volume change was estimated from the BTB measurements by using linear mixed model to calculate the limits of agreement (LoA) of each method, reflecting its measurement uncertainty. Using the delta method, approximate p-values were calculated for the pairwise comparisons between methods. Statistical analyses were performed both with inclusion and exclusion of visibly incorrect segmentations. Results Visibly incorrect automated segmentation in either one or both scans of a longitudinal scan pair occurred in 7.5\% of the hippocampi for FreeSurfer and in 6.9\% of the hippocampi for FIRST. After excluding these failed cases, reproducibility analysis for 1-year percentage volume change yielded LoA of {\textpm}7.2\% for FreeSurfer, {\textpm}9.7\% for expert manual delineation, and {\textpm}10.0\% for FIRST. Methods ranked the same for reproducibility of 1-year{$\mu$}L volume change, with LoA of {\textpm}218{$\mu$}L for FreeSurfer, {\textpm}319{$\mu$}L for expert manual delineation, and {\textpm}333{$\mu$}L for FIRST. Approximate p-values indicated that reproducibility was better for FreeSurfer than for manual or FIRST, and that manual and FIRST did not differ. Inclusion of failed automated segmentations led to worsening of reproducibility of both automated methods for 1-year raw and percentage volume change. Conclusion Quantitative reproducibility values of 1-year microliter and percentage hippocampal volume change were roughly similar between expert manual outlining, FIRST and FreeSurfer, but FreeSurfer reproducibility was statistically significantly superior to both manual outlining and FIRST after exclusion of failed segmentations.},
  langid = {english},
  keywords = {Alzheimer's disease,Automatic segmentation,Hippocampus,Magnetic resonance imaging,Manual segmentation,Mild cognitive impairment},
  file = {/Users/rzielinski/Zotero/storage/J75M4PMX/Mulder et al_2014_Hippocampal volume change measurement.pdf;/Users/rzielinski/Zotero/storage/PYQVQL3I/S1053811914000895.html}
}

@article{muschelliFslrConnectingFSL2015,
  title = {Fslr: {{Connecting}} the {{FSL Software}} with {{R}}},
  shorttitle = {Fslr},
  author = {Muschelli, John and Sweeney, Elizabeth and Lindquist, Martin and Crainiceanu, Ciprian},
  year = {2015},
  month = jun,
  journal = {The R journal},
  volume = {7},
  number = {1},
  pages = {163--175},
  issn = {2073-4859},
  urldate = {2024-06-24},
  abstract = {We present the package fslr, a set of R functions that interface with FSL (FMRIB Software Library), a commonly-used open-source software package for processing and analyzing neuroimaging data. The fslr package performs operations on `nifti' image objects in R using command-line functions from FSL, and returns R objects back to the user. fslr allows users to develop image processing and analysis pipelines based on FSL functionality while interfacing with the functionality provided by R. We present an example of the analysis of structural magnetic resonance images, which demonstrates how R users can leverage the functionality of FSL without switching to shell commands.},
  pmcid = {PMC4911193},
  pmid = {27330830},
  file = {/Users/rzielinski/Zotero/storage/97YT8IUV/Muschelli et al_2015_fslr.pdf}
}

@article{patenaude2011Bayesian,
  title = {A {{Bayesian}} Model of Shape and Appearance for Subcortical Brain Segmentation},
  author = {Patenaude, Brian and Smith, Stephen M. and Kennedy, David N. and Jenkinson, Mark},
  year = {2011},
  month = jun,
  journal = {NeuroImage},
  volume = {56},
  number = {3},
  pages = {907--922},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2011.02.046},
  urldate = {2023-01-25},
  abstract = {Automatic segmentation of subcortical structures in human brain MR images is an important but difficult task due to poor and variable intensity contrast. Clear, well-defined intensity features are absent in many places along typical structure boundaries and so extra information is required to achieve successful segmentation. A method is proposed here that uses manually labelled image data to provide anatomical training information. It utilises the principles of the Active Shape and Appearance Models but places them within a Bayesian framework, allowing probabilistic relationships between shape and intensity to be fully exploited. The model is trained for 15 different subcortical structures using 336 manually-labelled T1-weighted MR images. Using the Bayesian approach, conditional probabilities can be calculated easily and efficiently, avoiding technical problems of ill-conditioned covariance matrices, even with weak priors, and eliminating the need for fitting extra empirical scaling parameters, as is required in standard Active Appearance Models. Furthermore, differences in boundary vertex locations provide a direct, purely local measure of geometric change in structure between groups that, unlike voxel-based morphometry, is not dependent on tissue classification methods or arbitrary smoothing. In this paper the fully-automated segmentation method is presented and assessed both quantitatively, using Leave-One-Out testing on the 336 training images, and qualitatively, using an independent clinical dataset involving Alzheimer's disease. Median Dice overlaps between 0.7 and 0.9 are obtained with this method, which is comparable or better than other automated methods. An implementation of this method, called FIRST, is currently distributed with the freely-available FSL package.},
  langid = {english},
  keywords = {Bayesian,Classification,Segmentation,Shape model,Subcortical structures},
  file = {/Users/rzielinski/Zotero/storage/GYGE9XUP/Patenaude et al_2011_A Bayesian model of shape and appearance for subcortical brain segmentation.pdf;/Users/rzielinski/Zotero/storage/FNF2MZK3/S1053811911002023.html}
}

@article{paveseImagingNeurodegenerationParkinson2009,
  title = {Imaging Neurodegeneration in {{Parkinson}}'s Disease},
  author = {Pavese, Nicola and Brooks, David J.},
  year = {2009},
  month = jul,
  journal = {Biochimica et Biophysica Acta (BBA) - Molecular Basis of Disease},
  series = {Parkinson's {{Disease}}},
  volume = {1792},
  number = {7},
  pages = {722--729},
  issn = {0925-4439},
  doi = {10.1016/j.bbadis.2008.10.003},
  urldate = {2023-04-19},
  abstract = {Neuroimaging techniques have evolved over the past several years giving us unprecedented information about the degenerative process in Parkinson's disease (PD) and other movement disorders. Functional imaging approaches such as positron emission tomography (PET) and single photon emission computerised tomography (SPECT) have been successfully employed to detect dopaminergic dysfunction in PD, even while at a preclinical stage, and to demonstrate the effects of therapies on function of intact dopaminergic neurons within the affected striatum. PET and SPECT can also monitor PD progression as reflected by changes in brain levodopa and glucose metabolism and dopamine transporter binding. Structural imaging approaches include magnetic resonance imaging (MRI) and transcranial sonography (TCS). Recent advances in voxel-based morphometry and diffusion-weighted MRI have provided exciting potential applications for the differential diagnosis of parkinsonian syndromes. Substantia nigra hyperechogenicity, detected with TCS, may provide a marker of susceptibility to PD, probably reflecting disturbances of iron metabolism, but does not appear to correlate well with disease severity or change with disease progression. In the future novel radiotracers may help us assess the involvement of non-dopaminergic brain pathways in the pathology of both motor and non-motor complications in PD.},
  langid = {english},
  keywords = {Imaging,MRI,Neurodegeneration,Parkinson's disease,PET},
  file = {/Users/rzielinski/Zotero/storage/C6DEVE3J/S0925443908001956.html}
}

@article{pearson1901on,
  title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, Karl},
  year = {1901},
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  publisher = {Taylor \& Francis Group},
  doi = {10.1080/14786440109462720},
  urldate = {2024-06-25},
  langid = {english},
  file = {/Users/rzielinski/Zotero/storage/H2IYEVVJ/F.R.S_1901_LIII.pdf;/Users/rzielinski/Zotero/storage/98ISITTD/14786440109462720.html}
}

@article{pengDetectionLungContour2018,
  title = {Detection of {{Lung Contour}} with {{Closed Principal Curve}} and {{Machine Learning}}},
  author = {Peng, Tao and Wang, Yihuai and Xu, Thomas Canhao and Shi, Lianmin and Jiang, Jianwu and Zhu, Shilang},
  year = {2018},
  month = aug,
  journal = {Journal of Digital Imaging},
  volume = {31},
  number = {4},
  pages = {520--533},
  issn = {1618-727X},
  doi = {10.1007/s10278-018-0058-y},
  urldate = {2024-06-18},
  abstract = {Radiation therapy plays an essential role in the treatment of cancer. In radiation therapy, the ideal radiation doses are delivered to the observed tumor while not affecting neighboring normal tissues. In three-dimensional computed tomography (3D-CT) scans, the contours of tumors and organs-at-risk (OARs) are often manually delineated by radiologists. The task is complicated and time-consuming, and the manually delineated results will be variable from different radiologists. We propose a semi-supervised contour detection algorithm, which firstly uses a few points of region of interest (ROI) as an approximate initialization. Then the data sequences are achieved by the closed polygonal line (CPL) algorithm, where the data sequences consist of the ordered projection indexes and the corresponding initial points. Finally, the smooth lung contour can be obtained, when the data sequences are trained by the backpropagation neural network model (BNNM). We use the private clinical dataset and the public Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset to measure the accuracy of the presented method, respectively. To the private dataset, experimental results on the initial points which are as low as 15\% of the manually delineated points show that the Dice coefficient reaches up to 0.95 and the global error is as low as 1.47\,{\texttimes}\,10-2. The performance of the proposed algorithm is also better than the cubic spline interpolation (CSI) algorithm. While on the public LIDC-IDRI dataset, our method achieves superior segmentation performance with average Dice of 0.83.},
  langid = {english},
  keywords = {Closed polygonal line algorithm,Lung contour,Machine learning,Principal curve},
  file = {/Users/rzielinski/Zotero/storage/HR7UKWWT/Peng et al_2018_Detection of Lung Contour with Closed Principal Curve and Machine Learning.pdf}
}

@article{perlakiComparisonAccuracyFSL2017,
  title = {Comparison of Accuracy between {{FSL}}'s {{FIRST}} and {{Freesurfer}} for Caudate Nucleus and Putamen Segmentation},
  author = {Perlaki, Gabor and Horvath, Reka and Nagy, Szilvia Anett and Bogner, Peter and Doczi, Tamas and Janszky, Jozsef and Orsi, Gergely},
  year = {2017},
  month = may,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {2418},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-02584-5},
  urldate = {2023-01-30},
  abstract = {Although several methods have been developed to automatically delineate subcortical gray matter structures from MR images, the accuracy of these algorithms has not been comprehensively examined. Most of earlier studies focused primarily on the hippocampus. Here, we assessed the accuracy of two widely used non-commercial programs (FSL-FIRST and Freesurfer) for segmenting the caudate and putamen. T1-weighted 1\,mm3 isotropic resolution MR images were acquired for thirty healthy subjects (15 females). Caudate nucleus and putamen were segmented manually by two independent observers and automatically by FIRST and Freesurfer (v4.5 and v5.3). Utilizing manual labels as reference standard the following measures were studied: Dice coefficient (D), percentage volume difference (PVD), absolute volume difference as well as intraclass correlation coefficient (ICC) for consistency and absolute agreement. For putamen segmentation, FIRST achieved higher D, lower PVD and higher ICC for absolute agreement with manual tracing than either version of Freesurfer. Freesurfer overestimated the putamen, while FIRST was not statistically different from manual tracing. The ICC for consistency with manual tracing was similar between the two methods. For caudate segmentation, FIRST and Freesurfer performed more similarly. In conclusion, Freesurfer and FIRST are not equivalent when comparing to manual tracing. FIRST was superior for putaminal segmentation.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Anatomy,Medical research,Neurology},
  file = {/Users/rzielinski/Zotero/storage/JUNFK4LU/Perlaki et al. - 2017 - Comparison of accuracy between FSLâs FIRST and Fre.pdf}
}

@article{poewe2017Parkinson,
  title = {Parkinson Disease},
  author = {Poewe, Werner and Seppi, Klaus and Tanner, Caroline M. and Halliday, Glenda M. and Brundin, Patrik and Volkmann, Jens and Schrag, Anette-Eleonore and Lang, Anthony E.},
  year = {2017},
  month = mar,
  journal = {Nature Reviews Disease Primers},
  volume = {3},
  number = {1},
  pages = {1--21},
  publisher = {Nature Publishing Group},
  issn = {2056-676X},
  doi = {10.1038/nrdp.2017.13},
  urldate = {2023-01-24},
  abstract = {Parkinson disease is the second-most common neurodegenerative disorder that affects 2--3\% of the population {$\geq$}65 years of age. Neuronal loss in the substantia nigra, which causes striatal dopamine deficiency, and intracellular inclusions containing aggregates of {$\alpha$}-synuclein are the neuropathological hallmarks of Parkinson disease. Multiple other cell types throughout the central and peripheral autonomic nervous system are also involved, probably from early disease onwards. Although clinical diagnosis relies on the presence of bradykinesia and other cardinal motor features, Parkinson disease is associated with many non-motor symptoms that add to overall disability. The underlying molecular pathogenesis involves multiple pathways and mechanisms: {$\alpha$}-synuclein proteostasis, mitochondrial function, oxidative stress, calcium homeostasis, axonal transport and neuroinflammation. Recent research into diagnostic biomarkers has taken advantage of neuroimaging in which several modalities, including PET, single-photon emission CT (SPECT) and novel MRI techniques, have been shown to aid early and differential diagnosis. Treatment of Parkinson disease is anchored on pharmacological substitution of striatal dopamine, in addition to non-dopaminergic approaches to address both motor and non-motor symptoms and deep brain stimulation for those developing intractable L-DOPA-related motor complications. Experimental therapies have tried to restore striatal dopamine by gene-based and cell-based approaches, and most recently, aggregation and cellular transport of {$\alpha$}-synuclein have become therapeutic targets. One of the greatest current challenges is to identify markers for prodromal disease stages, which would allow novel disease-modifying therapies to be started earlier.},
  copyright = {2017 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Movement disorders,Neurodegenerative diseases,Parkinson's disease}
}

@misc{quilis-sancho2020comparative,
  title = {A Comparative Analysis of Automated {{MRI}} Brain Segmentation in a Large Longitudinal Dataset: {{Freesurfer}} vs. {{FSL}}},
  shorttitle = {A Comparative Analysis of Automated {{MRI}} Brain Segmentation in a Large Longitudinal Dataset},
  author = {{Quilis-Sancho}, Javier and {Fernandez-Blazquez}, Miguel A. and {Gomez-Ramirez}, J.},
  year = {2020},
  month = aug,
  primaryclass = {New Results},
  pages = {2020.08.13.249474},
  publisher = {bioRxiv},
  doi = {10.1101/2020.08.13.249474},
  urldate = {2025-01-07},
  abstract = {The study of brain volumetry and morphology of the different brain structures can determine the diagnosis of an existing disease, quantify its prognosis or even help to identify an early detection of dementia. Manual segmentation is an extremely time consuming task and automated methods are thus, gaining importance as clinical tool for diagnosis. In the last few years, AI-based segmentation has delivered, in some cases, superior results than manual segmentation, in both time and accuracy. In this study we aim at performing a comparative analysis of automated brain segmentation. In order to test the performance of automated segmentation methods, the two most commonly used software libraries for brain segmentation Freesurfer and FSL, were put to work in each of the 4028 MRIs available in the study. We find a lack of linear correlation between the segmentation results obtained from Freesurfer and FSL. On the other hand. Freesurfer volume estimates of subcortical brain structures tends to be larger than FSL estimates of same areas. The study builds on an uniquely large, longitudinal dataset of over 4,000 MRIs, all performed with identical equipment to help researchers understand what to expect from fully automated segmentation procedures.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/rzielinski/Zotero/storage/TULEYQ54/Quilis-Sancho et al_2020_A comparative analysis of automated MRI brain segmentation in a large.pdf}
}

@article{reuterWithinsubjectTemplateEstimation2012,
  title = {Within-Subject Template Estimation for Unbiased Longitudinal Image Analysis},
  author = {Reuter, Martin and Schmansky, Nicholas J. and Rosas, H. Diana and Fischl, Bruce},
  year = {2012},
  month = jul,
  journal = {NeuroImage},
  volume = {61},
  number = {4},
  pages = {1402--1418},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.02.084},
  urldate = {2023-02-24},
  abstract = {Longitudinal image analysis has become increasingly important in clinical studies of normal aging and neurodegenerative disorders. Furthermore, there is a growing appreciation of the potential utility of longitudinally acquired structural images and reliable image processing to evaluate disease modifying therapies. Challenges have been related to the variability that is inherent in the available cross-sectional processing tools, to the introduction of bias in longitudinal processing and to potential over-regularization. In this paper we introduce a novel longitudinal image processing framework, based on unbiased, robust, within-subject template creation, for automatic surface reconstruction and segmentation of brain MRI of arbitrarily many time points. We demonstrate that it is essential to treat all input images exactly the same as removing only interpolation asymmetries is not sufficient to remove processing bias. We successfully reduce variability and avoid over-regularization by initializing the processing in each time point with common information from the subject template. The presented results show a significant increase in precision and discrimination power while preserving the ability to detect large anatomical deviations; as such they hold great potential in clinical applications, e.g. allowing for smaller sample sizes or shorter trials to establish disease specific biomarkers or to quantify drug effects.},
  langid = {english},
  keywords = {FreeSurfer,MRI biomarkers,Reliability and power,Unbiased longitudinal image processing,Within-subject template},
  file = {/Users/rzielinski/Zotero/storage/KSPN7UCC/S1053811912002765.html}
}

@article{roweis2000Nonlinear,
  title = {Nonlinear {{Dimensionality Reduction}} by {{Locally Linear Embedding}}},
  author = {Roweis, Sam T. and Saul, Lawrence K.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2323--2326},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.290.5500.2323},
  urldate = {2023-02-24},
  abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.}
}

@misc{rSoftware2023,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2023},
  address = {Vienna, Austria},
  howpublished = {R Foundation for Statistical Computing}
}

@book{rudin1991functional,
  title = {Functional Analysis},
  author = {Rudin, Walter},
  year = {1991},
  edition = {2nd},
  publisher = {McGraw-Hill},
  address = {New York}
}

@article{sadil2024Comparing,
  title = {Comparing Automated Subcortical Volume Estimation Methods; Amygdala Volumes Estimated by {{FSL}} and {{FreeSurfer}} Have Poor Consistency},
  author = {Sadil, Patrick and Lindquist, Martin A.},
  year = {2024},
  journal = {Human Brain Mapping},
  volume = {45},
  number = {17},
  pages = {e70027},
  issn = {1097-0193},
  doi = {10.1002/hbm.70027},
  urldate = {2024-12-06},
  abstract = {Subcortical volumes are a promising source of biomarkers and features in biosignatures, and automated methods facilitate extracting them in large, phenotypically rich datasets. However, while extensive research has verified that the automated methods produce volumes that are similar to those generated by expert annotation; the consistency of methods with each other is understudied. Using data from the UK Biobank, we compare the estimates of subcortical volumes produced by two popular software suites: FSL and FreeSurfer. Although most subcortical volumes exhibit good to excellent consistency across the methods, the tools produce diverging estimates of amygdalar volume. Through simulation, we show that this poor consistency can lead to conflicting results, where one but not the other tool suggests statistical significance, or where both tools suggest a significant relationship but in opposite directions. Considering these issues, we discuss several ways in which care should be taken when reporting on relationships involving amygdalar volume.},
  copyright = {{\copyright} 2024 The Author(s). Human Brain Mapping published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {amygdala,MRI},
  file = {/Users/rzielinski/Zotero/storage/G3YB9J5Q/Sadil_Lindquist_2024_Comparing automated subcortical volume estimation methods\; amygdala volumes.pdf;/Users/rzielinski/Zotero/storage/J7ECVA92/hbm.html}
}

@article{samann2022FreeSurferbased,
  title = {{{FreeSurfer-based}} Segmentation of Hippocampal Subfields: {{A}} Review of Methods and Applications, with a Novel Quality Control Procedure for {{ENIGMA}} Studies and Other Collaborative Efforts},
  shorttitle = {{{FreeSurfer-based}} Segmentation of Hippocampal Subfields},
  author = {S{\"a}mann, Philipp G. and Iglesias, Juan Eugenio and Gutman, Boris and Grotegerd, Dominik and Leenings, Ramona and Flint, Claas and Dannlowski, Udo and {Clarke-Rubright}, Emily K. and Morey, Rajendra A. and {van Erp}, Theo G.M. and Whelan, Christopher D. and Han, Laura K. M. and {van Velzen}, Laura S. and Cao, Bo and Augustinack, Jean C. and Thompson, Paul M. and Jahanshad, Neda and Schmaal, Lianne},
  year = {2022},
  journal = {Human Brain Mapping},
  volume = {43},
  number = {1},
  pages = {207--233},
  issn = {1097-0193},
  doi = {10.1002/hbm.25326},
  urldate = {2025-01-07},
  abstract = {Structural hippocampal abnormalities are common in many neurological and psychiatric disorders, and variation in hippocampal measures is related to cognitive performance and other complex phenotypes such as stress sensitivity. Hippocampal subregions are increasingly studied, as automated algorithms have become available for mapping and volume quantification. In the context of the Enhancing Neuro Imaging Genetics through Meta Analysis Consortium, several Disease Working Groups are using the FreeSurfer software to analyze hippocampal subregion (subfield) volumes in patients with neurological and psychiatric conditions along with data from matched controls. In this overview, we explain the algorithm's principles, summarize measurement reliability studies, and demonstrate two additional aspects (subfield autocorrelation and volume/reliability correlation) with illustrative data. We then explain the rationale for a standardized hippocampal subfield segmentation quality control (QC) procedure for improved pipeline harmonization. To guide researchers to make optimal use of the algorithm, we discuss how global size and age effects can be modeled, how QC steps can be incorporated and how subfields may be aggregated into composite volumes. This discussion is based on a synopsis of 162 published neuroimaging studies (01/2013--12/2019) that applied the FreeSurfer hippocampal subfield segmentation in a broad range of domains including cognition and healthy aging, brain development and neurodegeneration, affective disorders, psychosis, stress regulation, neurotoxicity, epilepsy, inflammatory disease, childhood adversity and posttraumatic stress disorder, and candidate and whole genome (epi-)genetics. Finally, we highlight points where FreeSurfer-based hippocampal subfield studies may be optimized.},
  copyright = {{\copyright} 2020 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {ENIGMA,FreeSurfer,hippocampal subfields,hippocampal subregions,hippocampus,MRI,quality control,segmentation},
  file = {/Users/rzielinski/Zotero/storage/36QGHW2J/SÃ¤mann et al_2022_FreeSurfer-based segmentation of hippocampal subfields.pdf;/Users/rzielinski/Zotero/storage/MRU5GDJB/hbm.html}
}

@article{scahill2003longitudinal,
  title = {A {{Longitudinal Study}} of {{Brain Volume Changes}} in {{Normal Aging Using Serial Registered Magnetic Resonance Imaging}}},
  author = {Scahill, Rachael I. and Frost, Chris and Jenkins, Rhian and Whitwell, Jennifer L. and Rossor, Martin N. and Fox, Nick C.},
  year = {2003},
  month = jul,
  journal = {Archives of Neurology},
  volume = {60},
  number = {7},
  pages = {989--994},
  issn = {0003-9942},
  doi = {10.1001/archneur.60.7.989},
  urldate = {2024-02-09},
  abstract = {To investigate the effect of age on global and regional brain volumes and rates of atrophy, and to compare directly results based on cross-sectional and longitudinal data.Thirty-nine healthy control subjects (age range, 31-84 years) underwent serial magnetic resonance imaging assessments. Measurements included the whole-brain, temporal lobe, hippocampal, and ventricular volumes at baseline and for repeat scans.We found significant decreases in cross-sectional whole-brain (P\&lt;.001), temporal lobe (P\&lt;.001), and hippocampal (P = .003) volumes and a significant increase in ventricular volume (P\&lt;.001) with increasing age. Cross-sectional and longitudinal estimates of atrophy rates were similar. We also found directional evidence of acceleration in atrophy rates with increasing age in all analyses, with the most marked changes occurring after 70 years of age. This increase in rates after 70 years of age was particularly marked in the ventricles (P\&lt;.001) and the hippocampi (P = .01).We found a significant age-associated decrease in global and regional brain volumes. Some evidence indicates that this decline in brain volumes may be due to a nonlinear acceleration in rates of atrophy with increasing age. A better understanding of this process may help to discriminate normal age-related changes from neurodegenerative diseases.Arch Neurol. 2003;60:989-994--{$>$}},
  file = {/Users/rzielinski/Zotero/storage/8V4SGRLW/Scahill et al_2003_A Longitudinal Study of Brain Volume Changes in Normal Aging Using Serial.pdf;/Users/rzielinski/Zotero/storage/UKE2XIKA/784396.html}
}

@article{shen2022robust,
  title = {Robust and Scalable Manifold Learning via Landmark Diffusion for Long-Term Medical Signal Processing},
  author = {Shen, Chao and Lin, Yu-Ting and Wu, Hau-Tieng},
  year = {2022},
  month = jan,
  journal = {The Journal of Machine Learning Research},
  volume = {23},
  number = {1},
  pages = {86:3742--86:3771},
  issn = {1532-4435},
  abstract = {Motivated by analyzing long-termphysiological time series, we design a robust and scalable spectral embedding algorithm that we refer to as RObust and Scalable Embedding via LANdmark Diffusion ( Roseland). The key is designing a diffusion process on the dataset where the diffusion is done via a small subset called the landmark set. Roseland is theoretically justified under the manifold model, and its computational complexity is comparable with commonly applied subsampling scheme such as the Nystr{\"o}m extension. Specifically, when there are n data points in {$\mathbb{R}$}q and n{$\beta$} points in the landmark set, where {$\beta$} {$\in$} (0, 1), the computational complexity of Roseland is O(n1+2{$\beta$} + qn1+{$\beta$}), while that of Nystrom is O(n2.81{$\beta$} + qn1+2{$\beta$}). To demonstrate the potential of Roseland, we apply it to three datasets and compare it with several other existing algorithms. First, we apply Roseland to the task of spectral clustering using the MNIST dataset (70,000 images), achieving 85\% accuracy when the dataset is clean and 78\% accuracy when the dataset is noisy. Compared with other subsampling schemes, overall Roseland achieves a better performance. Second, we apply Roseland to the task of image segmentation using images from COCO. Finally, we demonstrate how to apply Roseland to explore long-term arterial blood pressure waveform dynamics during a liver transplant operation lasting for 12 hours. In conclusion, Roseland is scalable and robust, and it has a potential for analyzing large datasets.},
  file = {/Users/rzielinski/Zotero/storage/2WTTBMDD/Shen et al_2022_Robust and scalable manifold learning via landmark diffusion for long-term.pdf}
}

@article{smola2001Regularized,
  title = {Regularized {{Principal Manifolds}}},
  author = {Smola, Alexander J. and Mika, Sebastian and Sch{\"o}lkopf, Bernhard and Williamson, Robert C.},
  year = {2001},
  journal = {Journal of Machine Learning Research},
  volume = {1},
  number = {Jun},
  pages = {179--209},
  issn = {ISSN 1533-7928},
  urldate = {2023-02-24},
  abstract = {Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding. We explore this connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators. Experimental results demonstrate the feasibility of the approach.},
  file = {/Users/rzielinski/Zotero/storage/6EDX4H3M/Smola et al_2001_Regularized Principal Manifolds.pdf}
}

@article{suppa2016Performance,
  title = {Performance of {{Hippocampus Volumetry}} with {{FSL-FIRST}} for {{Prediction}} of {{Alzheimer}}'s {{Disease Dementia}} in at {{Risk Subjects}} with {{Amnestic Mild Cognitive Impairment}}},
  author = {Suppa, Per and Hampel, Harald and Kepp, Timo and Lange, Catharina and Spies, Lothar and Fiebach, Jochen B. and Dubois, Bruno and Buchert, Ralph and Initiative, for the Alzheimer's Disease Neuroimaging},
  year = {2016},
  month = jan,
  journal = {Journal of Alzheimer's Disease},
  volume = {51},
  number = {3},
  pages = {867--873},
  publisher = {IOS Press},
  issn = {1387-2877},
  doi = {10.3233/JAD-150804},
  urldate = {2025-01-07},
  abstract = {MRI-based hippocampus volume, a core feasible biomarker of Alzheimer's disease (AD), is not yet widely used in clinical patient care, partly due to lack of validation of software tools for hippocampal volumetry that are compatible with routine workfl},
  langid = {english}
}

@article{tenenbaum2000Global,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and de Silva, Vin and Langford, John C.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.290.5500.2319},
  urldate = {2023-02-24},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 106 optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}
}

@article{thurfjell2000,
  title = {Registration of {{Neuroimaging Data}}: {{Implementation}} and {{Clinical Applications}}},
  shorttitle = {Registration of {{Neuroimaging Data}}},
  author = {Thurfjell, Lennart and Pagani, Marco and Andersson, Jesper L.R. and Jonsson, Cathrine and Lundqvist, Roger and Wagner, Anna},
  year = {2000},
  journal = {Journal of Neuroimaging},
  volume = {10},
  number = {1},
  pages = {39--46},
  issn = {1552-6569},
  doi = {10.1111/jon200010139},
  urldate = {2023-01-15},
  abstract = {Image registration brings images into a form in which each voxel corresponds to a predetermined anatomic entity and is necessary for comparisons of data across scans. lntrasubject registration is a matter of translating and rotating one image volume into correspondence with another. lntersubject registration is more difficult because it requires the removal of individual anatomy dependence from the data. This article describes, with the help of clinical examples, automated methods for intrasubject registration of scans within and between modalities, and intersubject registration used for registering a three-dimensional brain atlas with a patient's brain scan.},
  langid = {english},
  file = {/Users/rzielinski/Zotero/storage/IBY6SSPW/jon200010139.html}
}

@article{velasco-annis2018Reproducibility,
  title = {Reproducibility of {{Brain MRI Segmentation Algorithms}}: {{Empirical Comparison}} of {{Local MAP PSTAPLE}}, {{FreeSurfer}}, and {{FSL-FIRST}}},
  shorttitle = {Reproducibility of {{Brain MRI Segmentation Algorithms}}},
  author = {{Velasco-Annis}, Clemente and {Akhondi-Asl}, Alireza and Stamm, Aymeric and Warfield, Simon K.},
  year = {2018},
  journal = {Journal of Neuroimaging},
  volume = {28},
  number = {2},
  pages = {162--172},
  issn = {1552-6569},
  doi = {10.1111/jon.12483},
  urldate = {2025-01-07},
  abstract = {BACKGROUND AND PURPOSE Segmentation of human brain structures is crucial for the volumetric quantification of brain disease. Advances in algorithmic approaches have led to automated techniques that save time compared to interactive methods. Recently, the utility and accuracy of template library fusion algorithms, such as Local MAP PSTAPLE (PSTAPLE), have been demonstrated but there is little guidance regarding its reproducibility compared to single template-based algorithms such as FreeSurfer and FSL-FIRST. METHODS Eight repeated magnetic resonance imagings of 20 subjects were segmented using FreeSurfer, FSL-FIRST, and PSTAPLE. We reported the reproducibility of segmentation-derived volume measurements for brain structures and calculated sample size estimates for detecting hypothetical rates of tissue atrophy given the observed variances. RESULTS PSTAPLE had the most reproducible volume measurements for hippocampus, putamen, thalamus, caudate, pallidum, amygdala, Accumbens area, and cortical regions. FreeSurfer was most reproducible for brainstem. PSTAPLE was the most accurate algorithm in terms of several metrics include Dice's coefficient. The sample size estimates showed that a study utilizing PSTAPLE would require tens to hundreds less subjects than the other algorithms for detecting atrophy rates typically observed in brain disease. CONCLUSIONS PSTAPLE is a useful tool for automatic human brain segmentation due to its precision and accuracy, which enable the detection of the size of the effect typically reported for neurological disorders with a substantially reduced sample size, in comparison to the other tools we assessed. This enables randomized controlled trials to be executed with reduced cost and duration, in turn, facilitating the assessment of new therapeutic interventions.},
  copyright = {Copyright {\copyright} 2017 by the American Society of Neuroimaging},
  langid = {english},
  keywords = {brain,FreeSurfer,FSL,MRI,segmentation,STAPLE},
  file = {/Users/rzielinski/Zotero/storage/3J4CIDKL/jon.html}
}

@book{wahba1990,
  title = {Spline {{Models}} for {{Observational Data}}},
  author = {Wahba, Grace},
  year = {1990},
  series = {{{CBMS-NSF Regional Conference Series}} in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, Pennsylvania}
}

@article{west1994Differences,
  title = {Differences in the Pattern of Hippocampal Neuronal Loss in Normal Ageing and {{Alzheimer}}'s Disease},
  author = {West, M. J and Coleman, P. D and Flood, D. G and Troncoso, J. C},
  year = {1994},
  month = sep,
  journal = {The Lancet},
  series = {Originally Published as {{Volume}} 2, {{Issue}} 8925},
  volume = {344},
  number = {8925},
  pages = {769--772},
  issn = {0140-6736},
  doi = {10.1016/S0140-6736(94)92338-8},
  urldate = {2022-11-30},
  abstract = {The distinction between the neurodegenerative changes that accompany normal ageing and those that characterise Alzheimer's disease is not clear. The resolution of this issue has important implications for the design of therapeutic and investigative strategies. To this end we have used modern stereological techniques to compare the regional pattern of neuronal cell loss in the hippocampus related to normal ageing to that associated with Alzheimer's disease. The loss related to normal ageing was evaluated from estimates of the total number of neurons in each of the major hippocampal subdivisions of 45 normal ageing subjects who ranged in age from 13 to 101 years. The Alzheimer's disease related losses were evaluated from similar data obtained from 7 cases of Alzheimer's disease and 14 age matched controls. Qualitative differences were observed in the regional patterns of neuronal loss related to normal ageing and Alzheimer's disease. The most distinctive Alzheimer's disease related neuron loss was seen in the CA1 region of the hippocampus. In the normal ageing group there was almost no neuron loss in this region (final neuron count in the CA1 region: 4{$\cdot$}40 X 106 neurons for the Alzheimer's disease group vs 14{$\cdot$}08 {\texttimes} 106 neurons in the normal ageing group). It is concluded that the neurodegenerative processes associated with normal ageing and with Alzheimer's disease are qualitatively different and that Alzheimer's disease is not accelerated by ageing but is a distinct pathological process.},
  langid = {english},
  file = {/Users/rzielinski/Zotero/storage/PVVSWKEQ/S0140673694923388.html}
}

@inproceedings{wolz2010Manifold,
  title = {Manifold {{Learning}} for {{Biomarker Discovery}} in {{MR Imaging}}},
  booktitle = {Machine {{Learning}} in {{Medical Imaging}}},
  author = {Wolz, Robin and Aljabar, Paul and Hajnal, Joseph V. and Rueckert, Daniel},
  editor = {Wang, Fei and Yan, Pingkun and Suzuki, Kenji and Shen, Dinggang},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {116--123},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-15948-0_15},
  abstract = {We propose a framework for the extraction of biomarkers from low-dimensional manifolds representing inter- and intra-subject brain variation in MR image data. The coordinates of each image in such a low-dimensional space captures information about structural shape and appearance and, when a phenotype exists, about the subject's clinical state. A key contribution is that we propose a method for incorporating longitudinal image information in the learned manifold. In particular, we compare simultaneously embedding baseline and follow-up scans into a single manifold with the combination of separate manifold representations for inter-subject and intra-subject variation. We apply the proposed methods to 362 subjects enrolled in the Alzheimer's Disease Neuroimaging Initiative (ADNI) and classify healthy controls, subjects with Alzheimer's disease (AD) and subjects with mild cognitive impairment (MCI). Learning manifolds based on both the appearance and temporal change of the hippocampus, leads to correct classification rates comparable with those provided by state-of-the-art automatic segmentation estimates of hippocampal volume and atrophy. The biomarkers identified with the proposed method are data-driven and represent a potential alternative to a-priori defined biomarkers derived from manual or automated segmentations.},
  isbn = {978-3-642-15948-0},
  langid = {english},
  keywords = {Embed Space,Laplacian Eigenmaps,Mild Cognitive Impairment,Mild Cognitive Impairment Group,Unlabelled Image},
  file = {/Users/rzielinski/Zotero/storage/XQN5CLVJ/Wolz et al_2010_Manifold Learning for Biomarker Discovery in MR Imaging.pdf}
}

@article{wu2018think,
  title = {Think Globally, Fit Locally under the Manifold Setup: {{Asymptotic}} Analysis of Locally Linear Embedding},
  shorttitle = {Think Globally, Fit Locally under the Manifold Setup},
  author = {Wu, Hau-Tieng and Wu, Nan},
  year = {2018},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {46},
  number = {6B},
  pages = {3805--3837},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/17-AOS1676},
  urldate = {2024-02-09},
  abstract = {Since its introduction in 2000, Locally Linear Embedding (LLE) has been widely applied in data science. We provide an asymptotical analysis of LLE under the manifold setup. We show that for a general manifold, asymptotically we may not obtain the Laplace--Beltrami operator, and the result may depend on nonuniform sampling unless a correct regularization is chosen. We also derive the corresponding kernel function, which indicates that LLE is not a Markov process. A comparison with other commonly applied nonlinear algorithms, particularly a diffusion map, is provided and its relationship with locally linear regression is also discussed.},
  keywords = {60K35,diffusion maps,Dimension reduction,Locally linear embedding,locally linear regression,measurement error},
  file = {/Users/rzielinski/Zotero/storage/XM9W7LCF/Wu_Wu_2018_Think globally, fit locally under the manifold setup.pdf}
}

@article{yue2016Parameterization,
  title = {Parameterization of {{White Matter Manifold-Like Structures Using Principal Surfaces}}},
  author = {Yue, Chen and Zipunnikov, Vadim and Bazin, Pierre-Louis and Pham, Dzung and Reich, Daniel and Crainiceanu, Ciprian and Caffo, Brian},
  year = {2016},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {515},
  pages = {1050--1060},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1164050},
  urldate = {2023-02-24},
  abstract = {In this article, we are concerned with data generated from a diffusion tensor imaging (DTI) experiment. The goal is to parameterize manifold-like white matter tracts, such as the corpus callosum, using principal surfaces. The problem is approached by finding a geometrically motivated surface-based representation of the corpus callosum and visualized fractional anisotropy (FA) values projected onto the surface. The method also applies to any other diffusion summary. An algorithm is proposed that (a) constructs the principal surface of a corpus callosum; (b) flattens the surface into a parametric two-dimensional (2D) map; and (c) projects associated FA values on the map. The algorithm is applied to a longitudinal study containing 466 diffusion tensor images of 176 multiple sclerosis (MS) patients observed at multiple visits. For each subject and visit, the study contains a registered DTI scan of the corpus callosum at roughly 20,000 voxels. Extensive simulation studies demonstrate fast convergence and robust performance of the algorithm under a variety of challenging scenarios.},
  pmid = {28090127},
  keywords = {Corpus callosum,Principal curves and surfaces,Thin plate splines},
  file = {/Users/rzielinski/Zotero/storage/A74IGZ37/Yue et al_2016_Parameterization of White Matter Manifold-Like Structures Using Principal.pdf}
}
