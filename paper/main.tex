%\documentclass[12pt]{amsart}
\documentclass[11pt,reqno]{article}
%\usepackage{cases}


%\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
\RequirePackage{graphicx}%% uncomment this for including figures

\usepackage[
backend=biber,
style=authoryear
]{biblatex}

\addbibresource{references.bib}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage[perpage,symbol*]{footmisc}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}  
\usepackage{tikz}

\usepackage{algorithm2e}

%\usepackage{natbib}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={(},close={)}}
\usepackage{authblk}

\usepackage{csquotes}
\usepackage[english]{babel}

\renewcommand{\baselinestretch}{1.0}
\setlength{\oddsidemargin}{-0.5cm}
\renewcommand{\topmargin}{-2cm}
\renewcommand{\oddsidemargin}{0mm}
\renewcommand{\evensidemargin}{0mm}
\renewcommand{\textwidth}{180mm}
\renewcommand{\textheight}{240mm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\title{Longitudinal Manifold Estimation}
\author[1]{Robert Zielinski and Ani Eloyan}


\maketitle

\doublespacing


\section{Introduction}

Neuroimaging plays a critical role in the diagnosis and monitoring of a number of common neurodegenerative conditions, such as Alzheimer's disease and Parkinson's disease (\cite{knopmanAlzheimerDisease2021}, \cite{poeweParkinsonDisease2017}). Frequently, interest centers around longitudinal changes in one or more neurological substructures. For example, it is common to observe atrophy the hippocampus of those with Alzheimer's disease or cognitive impairment related to other causes. Image segmentation allows for the extraction of subcortical structures from images of the complete brain for further analysis.

Traditionally, manual segmentation of images by a trained radiologist has been considered the most accurate approach and is the gold standard. However, this approach is highly time and resource intensive. In studies analyzing a large number of images, this approach may impose prohibitive costs. Additionally, manual segmentation suffers from meaningful inter-rater and intra-rater variability (\cite{boccardiSurveyProtocolsManual2011}). Automated segmentation approaches, such as FSL-FIRST and FreeSurfer, have been introduced to address these concerns (\cite{patenaudeBayesianModelShape2011}, \cite{reuterWithinsubjectTemplateEstimation2012}). While automating the segmentation process drastically reduces costs, it potentially introduces additional inaccuracies.

To anecdotally demonstrate the potential extent of the inaccuracies introduced by segmentation, using FIRST to segment the hippocampus in images of one healthy individual over the course of four years in the ADNI dataset, the volume of the left and right hippocampus decreased by 3.6\% and 5.4\%, respectively, but each hippocampus was estimated to have increased in volume between subsequent study visits twice. In a larger comparison of the accuracy of FIRST and FreeSurfer, \cite{mulderHippocampalVolumeChange2014} found that 6.9\% of segmentations by FIRST failed visual inspection for accuracy, as did 7.5\% of segmentations by FreeSurfer. After removing these failed segmentations, FIRST and FreeSurfer produced segmentations with variability similar to and slightly lower than manual segmentation, respectively. If failed segmentations were not removed from analysis, reflecting a more realistic situation when working with a large number of images, variability as measured by the Limits of Agreement were much higher for FIRST and FreeSurfer than for manual segmentation. \cite{mulderHippocampalVolumeChange2014} also found slightly higher rates of segmentation failure among individuals with Alzheimer's disease for both automated segmentation approaches, suggesting that variability increases as images are taken from individuals with greater deviations from the healthy brains on which the segmentation models were trained [See original articles to confirm, see if there are any confirming studies].

Ultimately, high levels of variability are present at the subcortical level, both between study visits for the same individual and between individuals, regardless of the segmentation approach used, and may be particularly influential when using automated segmentation methods. Mitigating the extent of this variability is a priority from a statistical perspective. To this end, in this article, we propose a manifold learning-based method to develop smooth estimates of the surfaces of subcortical structures over time. Manifold learning describes a set of statistical approaches to modeling high-dimensional data that are assumed to lie along a low-dimensional manifold. In medical imaging applications, interacting with a low-dimensional interpretation of a structure may be more intuitive than working with the structure in its original high-dimensional space. For example, \cite{yueParameterizationWhiteMatter2016} seeks to compute a parametric representation of the corpus callosum. To obtain the low-dimensional parameterization of the substructures of interest, the proposed approach extends the principal manifold estimation approach introduced by \cite{mengPrincipalManifoldEstimation2021}.

Previously introduced methods for manifold learning, such as Isomap (\cite{tenenbaumGlobalGeometricFramework2000}), locally linear embedding (\cite{roweisNonlinearDimensionalityReduction2000}), and Laplacian eigenmaps (\cite{belkinLaplacianEigenmapsDimensionality2003a}), are ill-equipped to reach meaningful estimates across multiple time points. This is because these methods consider time as an additional dimension in the data, meaning they include time in the dimension reduction process. This treatment of the time dimension causes it to be incorporated into the low-dimensional parameterization, preventing meaningful interpretation of this dimension. Here, we propose an alternative approach to estimating manifolds over time.

Rather than include time as another dimension in a manifold learning method, we use a process that maintains the interpretability of the time dimension. Specifically, we use the method proposed in \cite{mengPrincipalManifoldEstimation2021} to estimate the appropriate principal manifold at each given time point. We then smooth over these approximated manifolds in the time dimension, yielding an estimate showing longitudinal changes in the underlying manifold. The regularization involved in this process mitigates the effects of variability between study visits.

Some previous work has been conducted on longitudinal dimension reduction methods, but this research has primarily focused on linear methods, such as the longitudinal principal component analysis approach proposed by \cite{kinsonLongitudinalPrincipalComponent2020}. To our knowledge, the approach to longitudinal smoothing in the manifold learning setting taken in this article is novel.

The remainder of this article is laid out as follows: In Section 2, we introduce the principal manifold framework proposed in \cite{mengPrincipalManifoldEstimation2021}. Section 3 discusses how this approach can be adapted for longitudinal settings, and shows the algorithm being used to accomplish this. Section 4 demonstrates the performance of this approach on simulated data. In Section 5, we apply this method to longitudinal data of the hippocampus in individuals with Alzheimer's disease. The article concludes with a discussion of the method's contributions in Section 6.

\section{Principal Manifolds and Estimation Algorithm}

The framework for principal manifolds originated with the concept of principal curves (\cite{hastiePrincipalCurves1989}), which are essentially curves that pass through the middle of the data. \cite{hastiePrincipalCurves1989} defines principal curves as follows:

\begin{definition}
  \label{def:principal_curves}

  (Part I) Let $I \in \mathbb{R}^{1}$ be a closed and possibly infinite interval. Suppose a map $f:I \to \mathbb{R}^{D}$ satisfies the conditions:
  \begin{enumerate}
    \label{enum:hs_conditions}
  
    \item $f \in \mathcal{C}^{\infty}(I \to \mathbb{R}^{D})$
    \item $\|f'(r)\|_{\mathbb{R}^{D}} = 1$ for all $r \in I$
    \item $f$ does not self intersect, meaning $r_1 \neq r_2$ implies that $f(r_1) \neq f(r_2)$
    \item $\int_{\left\{r: f(r) \in B\right\}}dt < \infty$ for any finite ball $B$ in $\mathbb{R}^{D}$
  \end{enumerate}
  Then the projection index with respect to $f$ is defined as \[%
    \pi_f(x) = \sup\left\{r \in I: \|x - f(r)\|_{\mathbb{R}^{D}} = \inf_{r' \in I}\|x - f(r')\|_{\mathbb{R}^{D}} \right\}, \ \text{for all} \ x \in \mathbb{R}^{D}
  .\]%

  (Part II) Suppose $X$ is a continuous random $D$-vector with finite second moments. Principal curves of $X$ are all maps $f: I \to \mathbb{R}^{D}$ satisfying the conditions above and the Self-consistency defined as \[%
    E(X | \pi_f(X) = t) = f(r)
  .\]%
\end{definition}

Principal manifolds seek to extend this concept to higher dimensions. \cite{smolaRegularizedPrincipalManifolds2001} proposed a regularized principal manifold framework, where principal manifolds minimize \[%
  \argmin_{f \in \mathcal{F}}\left\{E\|X - f\left(\pi_f(X)\right)\|_{\mathbb{R}^D}^2 + \lambda\|Pf\|_{\mathcal{H}}^2\right\}
\]%
with $\mathcal{F}$ being a collection of functions and $P$ being an operator that maps $f$ into inner product space $\mathcal{H}$. If $P = \frac{d}{dr}$, this framework minimizes the mean squared distance (MSD), $\mathcal{D}_X(f) = E\|X - f(\pi_f(X))\|_{\mathbb{R}^{D}}^2$, while penalizing the non-smoothness of the curve $f$. \cite{mengPrincipalManifoldEstimation2021} build on this framework to provide a more complete definition of principal manifolds, and proposes an algorithm to estimate these principal manifolds.

To ensure that the upcoming definitions are well behaved, we introduce the following function spaces:
\begin{itemize}
  \item $\mathcal{C}_{\infty}(\mathbb{R}^{d} \to \mathbb{R}^{D}) = \left\{f \in \mathcal{C}(\mathbb{R}^{d} \to \mathbb{R}^{D}): \lim_{\|r\|_{\mathbb{R}^{d}} \to \infty}\|f(r)\|_{\mathbb{R}^{D}} = \infty\right\}$
  \item $\nabla^{-\otimes 2}L^2(\mathbb{R}^{d}) = \left\{f \in \mathcal{D}'(\mathbb{R}^{d}): \|\nabla^{\otimes 2}f\|_{\mathbb{R}^{d \times d}} \in L^2(\mathbb{R}^{d})\right\}$
  \item $H^2(\mathbb{R}^{d}) = \left\{f \in L^2(\mathbb{R}^{d}): \|\nabla f\|_{\mathbb{R}^{d}}, \|\nabla^{\otimes 2}f\|_{\mathbb{R}^{d \times d}} \in L^2(\mathbb{R}^{d})\right\}$
  \item $\nabla^{-\otimes 2}L^2(\Omega) = \left\{f|_{\Omega}: f \in \nabla^{-\otimes 2}L^2(\mathbb{R}^{d})\right\}$
  \item $H^2(\Omega) = \left\{f|_{\Omega}: f \in H^2(\mathbb{R}^{d})\right\}$
\end{itemize}

In these function spaces, we have the notation:
\begin{itemize}
  \item $\Omega$ is any open subset of $\mathbb{R}^{d}$ with a smooth boundary
  \item $f|_{\Omega}$ denotes the restriction of $f$ to $\Omega$
  \item $\nabla f$ denotes the gradient vector of $f$
  \item $\nabla^{\otimes 2}f$ denotes the Hessian matrix of $f$
  \item $\|\nabla^{\otimes 2}f\|_{\mathbb{R}^{d \times d}}$ is the function $r \to \|\nabla^{\otimes 2}f(r)\|_{\mathbb{R}^{d \times d}} = \left(\sum_{i, j = 1}^{d}\left|\frac{\partial^2f}{\partial t_i \partial t_j}(r)\right|^2\right)^{\frac{1}{2}}$
  \item $\|\nabla f\|_{L^2(\mathbb{R}^{d})}^2 = \int_{\mathbb{R}^{d}}\sum_{i=1}^{d}\left|\frac{\partial f}{\partial r_i}(r)\right|^2dr $
  \item $\|\nabla^{\otimes 2}f\|_{L^2(\mathbb{R}^{d})}^2 = \int_{\mathbb{R}^{d}}\sum_{i, j = 1}^{d}\left|\frac{\partial^2f}{\partial r_i \partial r_j}(r)\right|^2dr $
\end{itemize}

To completely define principal manifolds, \cite{mengPrincipalManifoldEstimation2021} let $\|Pf\|_{\mathcal{H}} = \|f''(r)\|_{\mathbb{R}^{D}}^2$, which provides a greater penalization of roughness than using the first derivative. To allow for manifolds with intrinsic dimension $d \geq 1$ and map $f(t) = \left(f_1(r), f_2(r), \dots, f_D(r)\right)^{T}$, they generalize this term as
\[%
  \|\nabla^{\otimes 2}f\|_{L^2(\mathbb{R}^{d})}^2 = \sum_{l=1}^{D} \int_{\mathbb{R}^{d}}\sum_{i, j = 1}^{d}\left|\frac{\partial^2f_l}{\partial r_i \partial r_j}(r)\right|^2dr
,\]%
referred to as the total squared curvature of a manifold. With this penalty term set, principal manifolds are then defined as follows:
\begin{definition}
  \label{def:principal_manifolds}
  Let $X$ be a random $D$-vector associated with the probability measure or density function $\mathbb{P}$ such that $X$ has compact support $\text{supp}(\mathbb{P})$ and finite second moments. Let $f, g \in \mathcal{C}_{\infty}\cap \nabla^{-\otimes 2}L^2(\mathbb{R}^{d} \to \mathbb{R}^{D})$ and $\lambda \in [0, \infty]$, and define the following functionals:
  \[%
    \mathcal{K}_{\lambda, \mathbb{P}}(f, g) = E\|X - f(\pi_g(X))\|_{\mathbb{R}^{D}}^2 + \lambda\|\nabla^{\otimes 2}f\|_{L^2(\mathbb{R}^{d})}^2, \quad \mathcal{K}_{\lambda, \mathbb{P}}(f) = \mathcal{K}_{\lambda, \mathbb{P}}(f, f)
  .\]%

  Then a manifold $M_{f^{*}}^{d}$ determined by $f^{*}$ is called a principal manifold for $X$ with the tuning parameter $\lambda$ if 
  \[%
    f_{\lambda}^{*} = \argmin_{f \in \mathcal{F}(\mathbb{P})}\mathcal{K}_{\lambda, \mathbb{P}}(f), \ \text{where} \ \mathcal{F}(\mathbb{P}) = \left\{f \in \mathcal{C}_{\infty} \cap \nabla^{-\otimes 2}L^2(\mathbb{R}^{d} \to \mathbb{R}^{D}): \sup_{x \in \text{supp}(\mathbb{P})}\|\pi_f(x)\|_{\mathbb{R}^{d}} = 1\right\}
  .\]%
\end{definition}

In this setting, we can see that $\pi_{f_{\lambda}^{*}}(X)$ maps the $D$-vector $X$ to a $d$-dimensional parameterization, while $f_{\lambda}^{*}$ embeds the $d$-dimensional parameterization in the original $D$-dimensional space.

To estimate principal manifolds under this definition, \cite{mengPrincipalManifoldEstimation2021} propose the Principal Manifold Estimation (PME) algorithm. This algorithm has three main steps: data reduction, fitting, and tuning. In the reduction step, the HDMDE algorithm uses $k$-means clustering to summarize the data $\left\{x_i\right\}_{i=1}^{I}$ by cluster centers $\left\{\mu_j\right\}_{j=1}^{N}$ with $N \leq I$, and each $\mu_j$ associated with weight $\theta_j$. The number of clusters is chosen using an iterative hypothesis testing approach. For each given value of $\lambda$, the fitting stage of the algorithm uses this reduced dataset to find estimated function 
\[%
  \hat{f}_\lambda = \arg\min_f \mathcal{K}_{\lambda, \hat{Q}_N}(f)
,\]%
where 
\[%
  \mathcal{K}_{\lambda, \hat{Q}_N}(f) = \sum_{j = 1}^{N}\theta_j \|\mu_j - f(\pi_f(\mu_j))\|_{\mathbb{R}^{D}}^2 + \lambda\|\nabla^{\otimes 2}f\|_{L^2(\mathbb{R}^{d})}^2
\]% 
approximates $\mathcal{K}_{\lambda, \mathbb{P}}(f)$. The tuning stage chooses the optimal value of $\lambda$, $\lambda^{*} = \arg\min_{\lambda > 0}(\mathcal{D}(\hat{f}_\lambda))$, where 
\[%
  \mathcal{D}(\hat{f}_\lambda) = \frac{1}{I}\sum_{i=1}^{I}\|x_i - \hat{f}_\lambda(\pi_{\hat{f}_\lambda}(x_i))\|_{\mathbb{R}^{D}}^2
\]%
is an empirical estimate of the MSD using the full dataset.

\cite{mengPrincipalManifoldEstimation2021} show that the optimal function will be of spline form, meaning that when $d = 1$, the function will be a cubic smoothing spline, while the function is a thin plate spline when $d = 2$. Specific details of the HDMDE algorithm (reduction step) and the PME algorithm (fitting and tuning steps) are given in \cite{mengPrincipalManifoldEstimation2021}.

\section{Longitudinal Principal Manifold Estimation Algorithm}

We now introduce the proposed longitudinal principal manifold estimation (LPME) algorithm, following a similar setting as \cite{mengPrincipalManifoldEstimation2021}. Let 

Outline:
\begin{enumerate}
    \item Show updated loss function for LPME algorithm
    \item LPME algorithm
\end{enumerate}

\RestyleAlgo{ruled}
\LinesNumbered

\SetKwComment{Comment}{/* }{ */}

	\begin{equation}\label{PMSEF}
	\mathcal{K}_{\lambda,\mathbb{P}}(f,g)=\mathbb{E}\left\Vert X_t - f\left(\pi_g(X_t)\right)\right\Vert^2_{\mathbb{R}^D} + \lambda \sum_{t = 1}^T \left\Vert\nabla^{\otimes 2} f(t)\right\Vert_{L^2(\mathbb{R}^d)}^2 + \gamma \sum_{X} \left\Vert\nabla^{\otimes 2} f(X_t)\right\Vert_{L^2(\mathbb{R})}^2, 
	\end{equation}
	\begin{equation}%\nonumber
	\mathcal{K}_{\lambda,\mathbb{P}}(f)=\mathcal{K}_{\lambda,\mathbb{P}}(f,f),
	\end{equation}
	where $\Vert\nabla^{\otimes 2}f \Vert_{L^2(\mathbb{R}^d)}^2 = \sum_{l=1}^D\int_{\mathbb{R}^d} \sum_{i,j=1}^d\vert\frac{\partial^2 f_l}{\partial t_i \partial t_j}(t)\vert^2dt$. A manifold $M_{f^*}^d$ determined by $f^*$ is called a principal manifold for $X$ (or $\mathbb{P}$) with the tuning parameter $\lambda$ if 
	\begin{align}\label{def: PM}
	f^*=\argmin_{f\in\mathscr{F}(\mathbb{P})}\mathcal{K}_{\lambda,\mathbb{P}}(f),\ \ \mbox{ where }\mathscr{F}(\mathbb{P}):=\left\{f\in C_\infty\cap \nabla^{-\otimes 2}L^2( R^d \rightarrow R^D):\sup_{x\in supp(\mathbb{P})}\left\Vert\pi_f(x)\right\Vert_{R^d} = 1 \right\}.
	\end{align}

\RestyleAlgo{ruled}
\LinesNumbered

\begin{algorithm}
\caption{Longitudinal Principal Manifold Estimation}\label{alg:lpme}
\KwData{Data points $\left\{y_{i, t}\right\}_{i, t}^{I_t, T}$, positive integer $d$, positive integer $N_0 < I - 1$, $\alpha$, $\epsilon$, $\epsilon^* \in (0, 1)$, candidate tuning parameters $\left\{\lambda_k\right\}_{k=1}^K$, $\left\{\gamma_l\right\}_{l=1}^L$, $itr \geq 1$, which is the maximum number of iterations allowed.}
\KwResult{Analytic formula of $\hat{g}^*: \mathbb{R}^{d + 1} \to \mathbb{R}^{D}$, optimal tuning parameter $\gamma^*$.}
\For{$t = 1, 2, \dots, T$} {
  Apply HDMDE algorithm with input $\left(\left\{y_{i, t}\right\}_{i = 1}^{I_t}, N_0, \epsilon, \alpha\right)$ and obtain $N_t$, $\left\{\mu_{j, N_t, t}\right\}_{j = 1}^{N_t}$, and $\left\{\theta_{j, N_t, t}\right\}_{j = 1}^{N_t}$\;
}

Parameterization: Apply ISOMAP to parameterize $\left\{\mu_{j, N_t, t}\right\}_{j = 1, t = 1}^{N_t, T}$ by the $d$-dimensional parameters $\left\{r_{j, t}\right\}_{j = 1, t = 1}^{N_t, T}$. Formally set $\pi_{f_t(0)}(\mu_{j, N_t, t}) \gets r_j$ for $j = 1, 2, \dots, N_t$, $t = 1, 2, \dots, T$\;

\For{t = 1, 2, \dots, T} {
  Apply modified PME algorithm with $\pi_{f_t(0)}(\mu_{j, N_t, t}) \gets \left\{r_{j, t}\right\}_{j = 1}^{N_t}$ and obtain $f_t^*$ and $\lambda_t^*$\;
  $\left\{r_{j, t}\right\} \gets \pi_{f_t^*}(\mu_{j, N_t, t})$ for $j = 1, \dots, N_t$ and $x_{j, t} \gets f_t^*(r_{j, t})$ for $j = 1, \dots, N_t$\;
  Add column vector $\mathbf{t}$ as first column of $\mathbf{r}$\;
}

Let $N = \sum_{t=1}^{T}N_t$, $\mathbf{x}$ be a $N \times D$ matrix of stacked values of $\left\{x_{j, t}\right\}_{t = 1}^T$, and $\mathbf{r}$ be a $N \times d + 1$ matrix of stacked values of $\left\{r_{j, t}\right\}_{t = 1}^T$ \;

Set $\pi_{g(0)}(x_i) \gets r_i$ for $i = 1, 2, \dots, N$\;

\For{l = 1, 2, \dots, L}{
  $\gamma \gets \gamma_l$\;
  Obtain $g_{(1)}$ by solving $\left(\begin{array}{ccc}
      2\mathbf{E}\mathbf{W}\mathbf{E} + 2\gamma\mathbf{E} & 2\mathbf{E}\mathbf{W}\mathbf{T} & \mathbf{T} \\
      2\mathbf{T}^{T}\mathbf{W}\mathbf{E} & 2\mathbf{T}^{T}\mathbf{W}\mathbf{T} & \mathbf{0} \\
      \mathbf{T}^{T} & \mathbf{0} & \mathbf{0}
    \end{array}\right)\left(\begin{array}{c}
    s_l \\
    \alpha_l \\
    m_l
    \end{array}\right) = \left(\begin{array}{c}
    2\mathbf{E}\mathbf{W}\mu_l \\
    2\mathbf{T}^{T}\mathbf{W}\mu_l \\
    \mathbf{0}
  \end{array}\right)$\;
  $\mathcal{\epsilon} \gets 2 \times \epsilon^*$, $n \gets 1$, $\mathcal{D}_{\hat{Q}_N}(f_{(n + 1)}) \gets \frac{1}{N}\sum_{j = 1}^{N}\|x_{j} - f_{(1)}(\pi_{f_{(1)}}(x_j))\|_{\mathbb{R}^{D}}^2$ \;
  \While{$\epsilon > \epsilon^*$ and $n \leq itr$} {
    Compute $f_{(n + 1)})$ from $f_{(n)}$ by solving (15) \;
    $\mathcal{D}_{\hat{Q}_N}(f_{(n + 1)}) \gets \frac{1}{N}\sum_{j=1}^{N}\|x_j - f_{(n + 1)}(\pi_{f_{(n + 1)}}(x_j))\|_{\mathbb{R}^D}^2$ \;
    $\epsilon \gets |\mathcal{D}_{\hat{Q}_N}(f_{(n + 1)}) - \mathcal{D}_{\hat{Q}_N}(f_{(n)})| / \mathcal{D}_{\hat{Q}_N}(f_{(n)})$ and $n \gets n + 1$\;
  }
  $\hat{f}_l \gets f_{(n)}$\;
}
$l^* \gets \argmin_l\left\{\frac{1}{T}\sum_{t = 1}^{T}\frac{1}{I_t}\sum_{i = 1}^{I_t}\|y_{i, t} - \hat{f}_l(\pi_{f_l}(y_{i, t}))\|_{\mathbb{R}^D}^2: \ l = 1, 2, \dots, L \right\}$ \;
$\kappa \gets \max\left\{\|\pi_{\hat{f}_{g^*}}(y_i, t)\|_{\mathbb{R}^{d}}: \ i = 1, 2, \dots, I_t; j = 1, 2, \dots, T \right\}$ \;
$f^*(r) = \hat{f}_{l^*}(r)$, where the analytic formula of $f^*$ is from (14), and $\gamma^* \gets \gamma_l^*$.
\end{algorithm}

\section{Simulations}

\section{Application}

\section{Discussion}

Contributions:

Limitations:
\begin{enumerate}
  \item Self-consistency condition means that the model cannot fit to self-intersecting manifolds. This is why partitioning is necessary.
  \item Partitioning approach is somewhat ad hoc and may not work in every situation. In testing it struggled when the structure shape changed drastically, or when orientation was not equivalent. There is not a clear approach to automatic partitioning.
  \item The process of joining the partitions may introduce error.
  \item Algorithm initialization is sensitive to magnitude of change between time points.
  \item Computation time.
\end{enumerate}

Future Work:

Questions:
\begin{enumerate}
  \item What is the best way to evaluate the performance of the estimates? We can use SSD for the embedded values, but how to assess the parameterized values?
  \item Should there be updates to what is defined as the "optimal" model in the PME algorithm? Currently it is possible for a model with non-minimal SSD to be chosen within the fitting of a single tuning value.
  \item Why would we see the SSD increase substantially in late optimization steps?
  \item Generally, what are the limitations on the estimated manifolds intersecting? How close can it come to intersecting without showing strange behavior?
  \item Is thin-plate spline computational strategy from Wahba the best approach, or could updated computational approach help (Wood, 2003)?
  \item How much error does the registration process introduce? Can the LPME algorithm successfully handle a certain number of bad registrations or extractions?
\end{enumerate}

\nocite{*}
%\bibliographystyle{plain}
%\bibliography{references}
\printbibliography

\end{document}
