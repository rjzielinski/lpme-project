\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url}

%\documentclass[12pt]{amsart}
%\documentclass[11pt,reqno]{article}
%\usepackage{cases}

%\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
%\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
%\RequirePackage{graphicx}%% uncomment this for including figures

%\usepackage[
%backend=biber,
%style=authoryear,
%url=false,
%isbn=false,
%date=year,
%doi=false
%]{biblatex}

%\addbibresource{references.bib}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage[perpage,symbol*]{footmisc}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}

\usepackage{algorithm2e}

%\usepackage{natbib}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={(},close={)}}
\usepackage{authblk}

\usepackage{csquotes}
\usepackage[english]{babel}

\renewcommand{\baselinestretch}{1.0}
\setlength{\oddsidemargin}{-0.5cm}
\setlength{\evensidemargin}{-0.5cm}
\renewcommand{\topmargin}{-2cm}
\renewcommand{\oddsidemargin}{0mm}
\renewcommand{\evensidemargin}{0mm}
\renewcommand{\textwidth}{180mm}
\renewcommand{\textheight}{240mm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\T}{\intercal}
\newcommand{\commentout}[1]{}

\newcommand{\kmedit}[1]{{\color{purple}  #1}}
\newcommand{\meng}[1]{{\color{purple} \sf $\clubsuit\clubsuit\clubsuit$ Kun Meng: [#1]}}
\newcommand{\Meng}[1]{\margMa{(Kun Meng) #1}}

\newcommand{\zielinski}[1]{{\color{blue} \sf $\spadesuit\spadesuit\spadesuit$ Rob Zielinski: [#1]}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\title{Longitudinal Principal Manifold Estimation}
\author[1]{Robert Zielinski}
\author[2]{Kun Meng}
\author[1]{Ani Eloyan}
\author[*]{for the Alzheimer’s Disease Neuroimaging Initiative}
\affil[1]{Department of Biostatistics, Brown University}
\affil[2]{Division of Applied Mathematics, Brown University}



\maketitle

\doublespacing

\section*{Abstract}

Longitudinal magnetic resonance imaging data is used to model trajectories of change in brain regions of interest to identify areas susceptible to atrophy in those with neurodegenerative conditions like Alzheimer’s disease. Most methods for extracting brain regions are applied to scans from study participants independently, resulting in wide variability in shape and volume estimates of these regions over time in longitudinal studies. To address this problem, we propose a longitudinal principal manifold estimation method, which seeks to recover smooth, longitudinally meaningful manifold estimates of shapes over time. The proposed approach uses a smoothing spline to smooth over the coefficients of principal manifold embedding functions estimated at each time point. This mitigates the effects of random disturbances to the manifold between time points. Additionally, we propose a novel data augmentation approach to enable principal manifold estimation on self-intersecting manifolds. Simulation studies demonstrate performance improvements over naïve applications of principal manifold estimation and principal curve/surface methods. The proposed method improves the estimation of surfaces of hippocampuses and thalamuses using data from participants of the Alzheimer’s Disease Neuroimaging Initiative. An analysis of magnetic resonance imaging data from 236 individuals shows the advantages of our proposed methods that leverage regional longitudinal trends for segmentation. \footnote{\textbf{Abbreviations:} AD, Alzheimer's disease; ADNI, Alzheimer's Disease Neuroimaging Initiative; HDMDE, high-dimensional mixture density estimation; LPME, longitudinal principal manifold estimation; MRI, magnetic resonance imaging; PCA, principal component analysis; PME, principal manifold estimation;}

% Alzheimer's disease is a neurogenerative disorder affecting, among others, the structure of the brain. Longitudinal magnetic resonance imaging data is used to model trajectories of change in brain regions of interest to identify areas more susceptible to atrophy. Most methods for extracting surfaces of brain regions are applied to individual scans from study participants independently. As a result, there is wide variability in shape and volume estimates of brain regions of interest over time in longitudinal studies with major implications for biomarker estimation and modeling, especially when used in therapeutic clinical trials. To address this problem, we propose a longitudinal principal manifold estimation method, with the goal of recovering smooth, longitudinally meaningful manifold estimates of shapes over time. The proposed approach uses a smoothing spline to smooth over the coefficients of the principal manifold embedding function estimated at each time point. This approach mitigates the effects of random disturbances to the manifold between time points. In addition, we propose a novel data augmentation approach to allow the use of principal manifold estimation on self-intersecting manifolds. We use simulation studies with several classes of manifolds to demonstrate performance improvements over naïve applications of principal manifold estimation and principal curve/surface methods. These improvements persist when considering varying between-time-point noise levels and the types and magnitudes of systematic change between time points. We show that the proposed method improves the estimation of surfaces of hippocampuses and thalamuses of healthy individuals using data from the Alzheimer’s Disease Neuroimaging Initiative dataset. We present a comprehensive analysis of magnetic resonance imaging data from 263 participants demonstrating high levels of variability of regional volumes when using existing methods for segmentation of hippocampuses and thalamnuses and the advantages gained by using our proposed methods that leverage longitudinal regional trends for segmentation.  \footnote{\textbf{Abbreviations:} AD, Alzheimer's disease; ADNI, Alzheimer's Disease Neuroimaging Initiative; HDMDE, high-dimensional mixture density estimation; LPME, longitudinal principal manifold estimation; MRI, magnetic resonance imaging; PCA, principal component analysis; PME, principal manifold estimation;}

\section{Introduction}

Neuroimaging plays a critical role in the diagnosis and monitoring of a number of common neurodegenerative conditions, such as Alzheimer's disease (AD) and Parkinson's disease (\cite{knopmanAlzheimerDisease2021}, \cite{poeweParkinsonDisease2017}). Frequently, interest centers around longitudinal changes in one or more neurological substructures. These structural changes can be observed in magnetic resonance imaging (MRI) data (\cite{crainiceanu2016tutorial}). For example, it is common to observe atrophy in the hippocampus, a structure in the temporal lobe of the brain, of those with  cognitive impairment either due to AD or other causes. Image segmentation enables the extraction of subcortical structures from MRI images of the brain for identifying differences between these structures in disease populations, modeling trajectories of change in the structures, and evaluating treatment effects in terms of reduction of atrophy over time. Traditionally, manual segmentation of images by a trained radiologist has been considered the most accurate approach for segmentation of regions of interest and is the gold standard. However, this approach is highly time and resource intensive. When analyzing data from studies with a large number of images, this approach may impose prohibitive costs. Additionally, there are a large number of manual segmentation protocols, each producing distinct results and incurring varying levels of inter-rater and intra-rater variability (\cite{boccardiSurveyProtocolsManual2011}). Automated segmentation approaches, such as FSL-FIRST and FreeSurfer, have been introduced to address these concerns (\cite{patenaudeBayesianModelShape2011}, \cite{reuterWithinsubjectTemplateEstimation2012}). While automating the segmentation process drastically reduces costs, it potentially introduces additional inaccuracies.


\subsection{A Motivating Question from Medical Imaging}

Failures of automatic segmentation algorithms may have dramatic effects when analyzing large datasets of images where manual quality control may not be feasible, such as data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. In a study comparing the accuracy of FIRST and FreeSurfer using a large sample, \cite{mulderHippocampalVolumeChange2014} found that 6.9\% of segmentations by FIRST failed visual inspection for accuracy, as did 7.5\% of segmentations by FreeSurfer. After removing the scans with failed segmentations, FIRST and FreeSurfer produced segmentations with variability similar to and slightly lower than manual segmentation, respectively. If failed segmentations were not removed from analysis, reflecting a more realistic situation when working with a large number of images, variability was much higher for FIRST and FreeSurfer than for manual segmentation. \cite{mulderHippocampalVolumeChange2014} also found slightly higher rates of segmentation failure among individuals with AD for both automated segmentation approaches, suggesting that variability increases as images are taken from individuals with greater deviations from the brain images of cognitively normal people which compose the majority of the data used for training the segmentation models. In studying longitudinal trajectories of atrophy, segmentation problems may result in seemingly random increases and reductions in brain region volumes, although it has been shown that brain volumes decrease as a result of normal aging (\cite{scahill2003longitudinal}). To anecdotally demonstrate the potential extent of the inaccuracies introduced by segmentation, using FIRST to segment the hippocampus in images of one healthy individual over the course of 6 visits during four years in the ADNI dataset, the volume of the left and right hippocampus decreased by 3.6\% and 5.4\% over the study duration, respectively, but each hippocampus was estimated to have increased in volume between subsequent study visits twice. A thorough description of the data and computation of hippocampus volumes and further examples are given in Section \ref{s:application} and Figure \ref{fig:lhipp_volume_comparison}.

Ultimately, high levels of variability are present at the subcortical level, both between study visits for the same individual and between individuals, regardless of the segmentation approach, and may be particularly influential when using automated segmentation methods. Mitigating the extent of this variability is a priority from a statistical perspective to improve the inferences that can be made from segmented image data. To this end, in this article, we propose a manifold learning-based method to develop smooth estimates of the surfaces of subcortical structures over time.


\subsection{A Brief Overview of Existing Approaches and Our Contributions}

Manifold learning refers to a set of approaches to modeling high-dimensional data that satisfy the ``manifold hypothesis"---\textit{``high-dimensional data tend to lie in the vicinity of a low-dimensional manifold''} (\cite{fefferman2016testing}). Manifold learning approaches have been applied in various scientific fields, e.g., medical signal processing (\cite{shen2022robust}), single-cell biology (\cite{ding2023learning}), and robotics (\cite{gao2023k}; \cite{gao2024bi}). In medical imaging applications, interacting with a low-dimensional interpretation of a structure may be more intuitive and efficient than working with the structure in its original high-dimensional space. For example, \cite{yueParameterizationWhiteMatter2016} seeks to compute a parametric representation of the corpus callosum that is used for investigating the features of this structure in people with multiple sclerosis. Estimating time-dependent low-dimensional representations of these shapes is essential for investigating changes of these low-dimensional structures. 

Nonlinear manifold learning methods have been extensively studied in the literature. However, many of them are not applicable in longitudinal studies, such as Isomap (\cite{tenenbaumGlobalGeometricFramework2000}), locally linear embedding (\cite{roweisNonlinearDimensionalityReduction2000}; \cite{wu2018think}), and Laplacian eigenmaps (\cite{belkin2003laplacian}). That is, these methods include the time dimension in the dimension reduction process, causing the time dimension to be incorporated into the low-dimensional parameterization, preventing meaningful interpretation of the time dimension. We use the oldest linear manifold learning method, principal component analysis (PCA, \cite{Pearson1901on}), as a simple example to illustrate this point: A principal component may be a linear combination of both time and spatial coordinates.

Previous work on longitudinal dimension reduction methods has primarily focused on linear approaches, such as the longitudinal PCA method proposed by \cite{kinsonLongitudinalPrincipalComponent2020} and longitudinal functional PCA proposed by \cite{greven2011longitudinal}. As for nonlinear methods, \cite{wolzManifoldLearningBiomarker2010} used Laplacian eigenmaps to incorporate longitudinal information when reducing neuroimaging data to lower dimensional space. However, parameterizations for baseline images and longitudinal image differences were estimated separately while we seek to parameterize them simultaneously. In addition, \cite{louisRiemannianGeometryLearning2019} used a deep learning-based approach to learn Riemannian manifolds in the context of disease progression modeling. However, this approach relies on modeling assumptions specific to the application, and the deep learning methodology used imposes costs in terms of computational requirements and interpretability. 

In this article, we propose a new longitudinal approach to estimating nonlinear manifolds over time. Rather than treating the time dimension the same as the space dimensions, we introduce a novel process that maintains the interpretability of the time dimension. Specifically, we use the method proposed by \cite{mengPrincipalManifoldEstimation2021} to model the appropriate principal manifold at each given time point. Unlike most commonly used manifold learning methods, this approach results in an analytic and computationally efficient functional representation of the manifold estimate (see equation~\eqref{eq: spline representation of the PME}, in Section \ref{s:PME}). Importantly, our approach allows us to iteratively update the smoothing and fitting steps, which could lead to an overall better fit. We then build on this model by imposing smoothness over these approximated manifolds in the time dimension, yielding an estimate showing longitudinal changes in the underlying manifold. We demonstrate that the regularization involved in this process mitigates the effects of variability between time points. Our work includes two major contributions to the existing literature: 1) we propose a novel approach for longitudinal smoothing in the general-purpose manifold learning setting, and 2) we show the potential of leveraging longitudinal manifold smoothing for reducing variability and increasing signal-to-noise in segmentation of brain structures over time. 

%Previously introduced methods for manifold learning, such as Isomap (\cite{tenenbaumGlobalGeometricFramework2000}), locally linear embedding (\cite{roweisNonlinearDimensionalityReduction2000}; \cite{wu2018think}), and Laplacian eigenmaps (\cite{belkin2003laplacian}), are ill-equipped to reach meaningful estimates across multiple time points. These methods consider time as an additional dimension in the data, including time in the dimension reduction process. Because of this treatment of the time dimension, it is incorporated into the low-dimensional parameterization, preventing meaningful interpretation of this dimension. Here, we propose an alternative approach to estimating manifolds over time. Rather than include time as another dimension in a manifold learning method, we introduce a novel process that maintains the interpretability of the time dimension. Specifically, we use the method proposed in \cite{mengPrincipalManifoldEstimation2021} to model the appropriate principal manifold at each given time point. Unlike most commonly used manifold learning methods, this approach results in an explicit functional representation of the manifold estimate. We then build on this model by imposing smoothness over these approximated manifolds in the time dimension, yielding an estimate showing longitudinal changes in the underlying manifold. We demonstrate that the regularization involved in this process mitigates the effects of variability between study visits.

%Previous work on longitudinal dimension reduction methods has primarily focused on linear approaches, such as the longitudinal principal component analysis (PCA) method proposed by \cite{kinsonLongitudinalPrincipalComponent2020} and longitudinal functional PCA proposed by \cite{greven2011longitudinal}. \cite{wolzManifoldLearningBiomarker2010a} used Laplacian eigenmaps to incorporate longitudinal information when reducing neuroimaging data to lower dimensional space, however, parameterizations for baseline images and longitudinal image differences were estimated separately while we seek to parameterize them simultaneously. \cite{louisRiemannianGeometryLearning2019} used a deep learning-based approach to learn Riemannian manifolds in the context of disease progression modeling. However, this approach relies on modeling assumptions specific to the application, and the deep learning methodology used imposes costs in terms of computational requirements and interpretability. 

%Our work includes two major contributions to the existing literature: 1) we propose a novel approach for longitudinal smoothing in the general-purpose manifold learning setting, and 2) we show the potential of leveraging longitudinal manifold smoothing for reducing variability and increasing signal-to-noise in segmentation of brain structures over time. 

This work was motivated by longitudinal imaging studies, where data of participants are collected during several study visits to observe changes in brain structure (among other variables of interest) over time. One such study is the ADNI, where we obtained data that we analyzed to obtain the results presented in this article. The data are publicly available and are hosted at \href{https://adni.loni.usc.edu/}{\texttt{https://adni.loni.usc.edu/}}. The study has over 20 years of history with the goal of obtaining biomarkers of progression of early AD from various data sources, including brain imaging (such as magnetic resonance imaging and positron emission tomography), cognitive and biological markers. We implement the proposed methods to estimate the surfaces of two regions of interest in the brain - the hippocampus and the thalamus. The thalamus is selected as a ``easy-to-fit" region of interest to evaluate the performance of our proposed methods when the underlying structure is somewhat close to a spherical shape. The hippocampus is more of a pear-shaped region and is selected as a region that may exhibit atrophy for people with AD. 

The remainder of this article is laid out as follows. In Section \ref{s:PME}, we review the principal manifold framework proposed by \cite{mengPrincipalManifoldEstimation2021}. In Section \ref{s:LPME}, we discuss our approach to adapting the principal manifold framework for longitudinal settings, and show our proposed novel  algorithm for dimension reduction using the longitudinal principal manifold framework. Section \ref{s:simulations} demonstrates the performance of this approach on simulated data. In Section \ref{s:application}, we apply our proposed method to estimate smooth time dependent surfaces of the hippocampus and thalamus in individuals with AD and cognitively normal adults. The article concludes with a discussion of the method's contributions in Section \ref{s:discussion}.








\section{Principal Manifold Estimation Algorithm}\label{s:PME}

The framework for principal manifolds originated with the concept of principal curves (\cite{hastiePrincipalCurves1989}), which are essentially curves that pass through the middle of a data cloud. However, as pointed out by \cite{duchamp1996extremal}, the principal curve framework in \cite{hastiePrincipalCurves1989} has limitations such as the model complexity selection and problems with applications in high-dimensional settings. Motivated by the penalization approaches presented by \cite{kegl2000learning} and \cite{smolaRegularizedPrincipalManifolds2001}, \cite{mengPrincipalManifoldEstimation2021} proposed the principal manifold estimation (PME) framework. In order to present the longitudinal PME approach, we start by introducing notation used throughout this paper. We use $d$ and $D$ to denote the dimensions of the low-dimensional manifold and high-dimensional spaces, respectively. We assume that a data cloud is observed in the $D$ dimensional space. In the context of segmentation, we consider a collection of vertices on the surface of the brain region of interest as the 3-dimensional data cloud ($D=3$), while the 2-dimensional smooth surface of the region is the underlying manifold of interest ($d=2$; for example, see Figure \ref{fig:adni_result}). For any positive integer $q$ and point $\xi=(\xi_1,\ldots,\xi_q)\in\mathbb{R}^q$, we use the following norm $\Vert \xi\Vert_{\mathbb{R}^q}=\sqrt{\sum_{l=1}^q \xi_l^2}$. Since our interest in this work centers on continuous mappings, we denote the collection of continuous vector-valued functions from $\mathbb{R}^d$ to $\mathbb{R}^D$ as $\mathcal{C}(\mathbb{R}^{d} \to \mathbb{R}^{D})$. Given a $d$-variable function $u$, $\nabla^{\otimes 2} u$ denotes the Hessian matrix of $u$, defined as $r \to \left( \frac{\partial ^2 u}{\partial r_i \partial r_j}(r) \right)_{1 \leq i, j \leq d}$. The $d$-dimensional manifold corresponding to a function $f \in C(\mathbb{R}^{d} \to \mathbb{R}^{D})$ is formally defined as $M_f^d=\{f(r):\,r\in\mathbb{R}^d\}$. Many manifold estimation methods are based on defining a projection index, denoted by $\pi_f(x)$, defined as the point in the $d$-dimensional space, where $f(\pi_f(x))$ is the projection of $x$ on the manifold $M_f^d$. The definitions of $\pi_f(x)$ given by \cite{hastiePrincipalCurves1989} and \cite{mengPrincipalManifoldEstimation2021} for one- and higher-dimensional intrinsic spaces, respectively, guarantee the uniqueness of the projection. The principal manifolds are then defined as follows.
\begin{definition}
  \label{def:principal_manifolds}
  Let $\mathbf{X}$ denote a random $D$-vector associated with the probability distribution $\mathbb{P}$ such that $\mathbf{X}$ has compact support, $\text{supp}(\mathbb{P})$, and finite second moments. Under some assumptions on limiting behavior and smoothness on function $f$, given a $\lambda \in [0, \infty)$, define the following functional:
  \begin{align}\label{eq:pme_kappa}
\mathcal{K}_{\lambda, \mathbb{P}}(f) = \mathbb{E}\|\mathbf{X} - f(\pi_f(\mathbf{X}))\|_{\mathbb{R}^{D}}^2 + \lambda\|\nabla^{\otimes 2}f\|_{L^2(\mathbb{R}^{d})}^2. 
  \end{align}
  The principal manifold of $\mathbf{X}$ with the tuning parameter $\lambda$ is the manifold $M_{f^{*}}^{d}$ determined by $f^{*}$ if
  \begin{equation}\nonumber
    \begin{aligned}
        & f_{\lambda}^{*} = \argmin_{f \in \mathcal{F}(\mathbb{P})}\mathcal{K}_{\lambda, \mathbb{P}}(f),
    \end{aligned}
  \end{equation}
where $\mathcal{F}(\mathbb{P})$ is the collection of functions $f \in \mathcal{C}(\mathbb{R}^{d} \to \mathbb{R}^{D})$ such that $\lim_{\|r\|_{\mathbb{R}^{d}} \to \infty}\|f(r)\|_{\mathbb{R}^{D}} = \infty$, the components of the function $f$ are in $\nabla^{-\otimes 2}L^2(\mathbb{R}^{d}) = \left\{u : \|\nabla^{\otimes 2} u\|_{\mathbb{R}^{d \times d}} \in L^2(\mathbb{R}^{d})\right\}$, and the projection function of $f$ satisfies $\sup_{x \in \text{supp}(\mathbb{P})}\|\pi_f(x)\|_{\mathbb{R}^{d}} = 1$.
\end{definition}

It is important to note that, unlike manifold learning methods that use an eigendecomposition to directly estimate coordinates in the $d$-dimensional manifold space (e.g. Isomap, locally linear embedding, and others) with only an implicit representation of the embedding function $f$, principal manifolds may be estimated via an explicit approximation of the embedding function $f$, from which the projection index $\pi_f(x)$ and the desired coordinates on the manifold can be found. In this way, regression approaches become relevant to the manifold estimation problem.

The functional $\mathcal{K}_{\lambda, \mathbb{P}}(f)$ in equation (\ref{eq:pme_kappa}) consists of two elements. The first element is the expected squared distance from data points $\mathbf{X}$ in the cloud to their projections $f(\pi_f(\mathbf{X}))$ on the manifold $M_f^d$. The second element $\|\nabla^{\otimes 2}f\|_{L^2(\mathbb{R}^{d})}^2 = \sum_{l=1}^{D} \int_{\mathbb{R}^{d}}\sum_{i, j = 1}^{d}\left|\frac{\partial^2f_l}{\partial r_i \partial r_j}(r)\right|^2dr$ imposes the smoothness/curvature penalty on the estimated manifold. The coefficient $\lambda$ is a tuning parameter. In this setting,  $\pi_{f_{\lambda}^{*}}(\mathbf{X})$ maps the $D$-vector $\mathbf{X}$ to a $d$-dimensional parameterization, while $f_{\lambda}^{*}$ embeds the $d$-dimensional parameterization in the original $D$-dimensional space. Thus, even though manifold learning is commonly considered an unsupervised learning approach, \cite{mengPrincipalManifoldEstimation2021} show that finding the function that minimizes $\mathcal{K}_{\lambda, \mathbb{P}}(f)$ in fact takes the form of a penalized regression problem making this a supervised learning problem. \cite{mengPrincipalManifoldEstimation2021} also show that this function takes spline form
\begin{align}\label{eq: spline representation of the PME}
    f_{(n+1), l}(r) = \sum_{j=1}^N s_{j, l} \times \eta_{4-d}\left(r - \pi_{f_{(n)}}(\mu_{j, N})\right) + \sum_{k=1}^{d + 1}\alpha_{k, l} \times p_k(r), \ \ \text{ for } \ l = 1, 2, \dots, D,
\end{align}
under the constraint $\sum_{j=1}^{N}s_{j, l} \times p_{k}\left(\pi_{f_{(n)}}(\mu_{j, N})\right) = 0$ for all $k = 1, 2, \dots, d + 1$ and $l = 1, 2, \dots, D$, where $\mu_{j, N}, j = 1, \dots, N$ denote points that summarize the data cloud. Importantly, $N$ tends to be much smaller than the number of points in the data cloud (see Figure 3 of \cite{mengPrincipalManifoldEstimation2021}), which indicates that the representation in equation~\eqref{eq: spline representation of the PME} is computationally efficient. This spline function is specified by coefficients $s_{j, l}$ and $\alpha_{k, l}$ for $j = 1, \dots, N$, $k = 1, \dots, d+1$, and $l = 1, \dots, D$. These coefficients will be used to smooth over principal manifold estimates obtained at several time points in Section \ref{s:LPME}.






\section{Longitudinal Principal Manifold Estimation Algorithm}\label{s:LPME}

In this section, we introduce an approach to extending the PME framework described in Section \ref{s:PME} to longitudinal point clouds. We first heuristically explain the goal of our proposed model and its importance in the segmentation of brain regions for longitudinal imaging studies. Our goal is to estimate a smooth surface for the brain region of interest at each time point such that the changes of the surface over time are smooth. The estimated surfaces across different time points enable further inference, e.g., modeling trajectories of change in region volumes over time, identifying regions that are most affected in terms of atrophy, etc. We consider that 3-dimensional data clouds are observed longitudinally during several visits by a study participant. Hence, the observed data $(x_1, x_2, x_3,t_j)$ are 4-dimensional, where $t_j$ denotes the time of the $j$th study visit, and $(x_1, x_2, x_3)$ denotes a spatial point on the surface of the segmented brain region of interest. 

Figure \ref{fig:lpme_step1} displays a simplified example of this setting using simulated data, in which 2-dimensional point clouds are observed longitudinally at several time points. The goal in this simplified example is to fit a surface in 3-dimensional spacetime---comprising two spatial dimensions and one time dimension---that fits the longitudinal data $(x_1, x_2, t_j)$ in spacetime and is not affected by the random noise added at each time point and in space (see Figure \ref{fig:lpme_step4}). The PME framework does not directly apply due to the following:
\begin{enumerate}
    \item Suppose we apply the PME approach to the data $(x_1, x_2, t_j)$ in spacetime and obtain an estimated surface $(r_1, r_2) \mapsto F(r_1, r_2)\in\mathbb{R}^2\times\mathbb{R}$ in spacetime. It is likely that neither coordinate $r_1$ nor $r_2$ coincides with the time dimension---they are (potentially nonlinear) combinations of both time and spatial coordinates, which prevents meaningful interpretation of the time dimension.

    \item If the manifold is estimated separately at each time point, the results may not be smooth over time.
\end{enumerate}
We expect that our proposed dimension reduction method will yield a smooth surface $(t,r)\mapsto F(t,r)$ in spacetime, $\mathbb{R}^2\times\mathbb{R}$, while ensuring that one coordinate, $t$, coincides with the time dimension. This can be achieved by the longitudinal principal manifold estimation (LPME) framework proposed in this section.

%The key reasons why the existing principal manifold estimation methods cannot be implemented in this setting are 1) if we implement the data reduction method in Section \ref{s:PME} then the resulting principal manifold will not be a function of time $t_j$, which is necessary for the resulting segmentations to be useful in practice, 2) if the manifold is estimated separately at each time point, the results may not be smooth over time. In other words, the dimension reduction should be implemented to obtain the 2-dimensional coordinates $(r_1,r_2,t_j)$ while ensuring that the resulting manifold is also a function of $t_j$. In order to address the limitations of existing approaches for the setting described above, we propose the longitudinal principal manifold estimation (LPME) framework to estimate $d$-dimensional representations of $D$-dimensional objects observed over time.

%Hereafter, we utilize the following notations to describe the LPME framework for generic low and high dimensions, $d$ and $D$. Let $\left\{x_{it}\right\}_{i=1, t=1}^{I_t, T}$ represent the $I = \sum_{t=1}^{T}I_t$ observations in $\mathbb{R}^D$ (i.e., each $x_{it}\in\mathbb{R}^D$) for each individual $i$, where $I_t$ denotes the number of observations available at each time point $t$, with $T$ total time points. As in the setting described above, at each time point $t$, these observations lie in the vicinity of a $d$-dimensional manifold and are corrupted by $D$-dimensional noise. In this setting, the manifold to estimate is represented by $(t,\mathbf{r})\mapsto F(t,\mathbf{r})\in\mathbb{R}^D$, where $\mathbf{r}\in\mathbb{R}^d$, and $t$ coincides with the time dimension.

\subsection{The Longitudinal Principal Manifold Framework}\label{section: The Longitudinal Principal Manifold Framework}

Hereafter, we utilize the following notation to describe the LPME framework for generic low and high dimensions, $d$ and $D$. Let $\left\{x_{it}\right\}_{i=1, t=1}^{I_t, T}$ represent the $I = \sum_{t=1}^{T}I_t$ observations in $\mathbb{R}^D$ (i.e., each $x_{it}\in\mathbb{R}^D$) for each image $i$, where $I_t$ denotes the number of observations available at each time point $t$, with $T$ total time points. As in the setting described above, at each time point $t$, these observations lie in the vicinity of a $d$-dimensional manifold and are corrupted by $D$-dimensional noise. In this setting, the manifold to estimate is represented by 
\begin{align}\label{eq: F form}
    \begin{aligned}
        F:\ \ & \mathbb{R}^d\times\mathbb{R} \rightarrow \mathbb{R}^D\times\mathbb{R}, \\
    & (t,\mathbf{r})\mapsto F(t,\mathbf{r})=:f_t(\mathbf{r}).
    \end{aligned}
\end{align}
where $t$ coincides with the time dimension. We define a longitudinal principal manifold as follows.
\begin{definition}
  \label{def:lpme} Given a collection $\mathbf{X} = \{ \mathbf{X}_t \}_{t=1}^T$ of data clouds observed at a series of time points, where $\mathbf{X}_t$ is the random  $D$-vector observed at time $t$, and tuning parameters $\lambda=\{\lambda_t\}_{t\in\mathbb{R}}$ and $\gamma$, we define the functional $\mathcal{K}_{\lambda, \gamma, \mathbb{P}}(F)$ as follows
\begin{align}\label{eq:newKappa}
  \begin{aligned}
      \mathcal{K}_{\lambda, \gamma, \mathbb{P}}(F) &:= \int_\mathbb{R} \mathbb{E}\left\|\mathbf{X}_t - f_t\left(\pi_{f_t}(\mathbf{X}_t)\right)\right\|_{\mathbb{R}^{D}}^2 \, dt + \int_\mathbb{R} \lambda_t \cdot\|\nabla^{\otimes 2}f_t\|_{L^2(\mathbb{R}^{d})}^2 \, dt + \gamma\cdot \int_{\mathbb{R}}\left\|\frac{\partial^2}{\partial t^2}F\right\|_{L^2(\mathbb{R}^d)}^2 \, dt \\
  &= \int_{\mathbb{R}}\mathcal{K}_{\lambda_t, \mathbb{P}}(f_t) \, dt + \gamma \cdot \int_{\mathbb{R}}\left\|\frac{\partial^2}{\partial t^2}F\right\|_{L^2(\mathbb{R}^d)}^2 \, dt,
  \end{aligned}
\end{align}
where $f_t(\mathbf{r})=F(t,\mathbf{r})$ is a continuous function of the form in equation~\eqref{eq: F form}, satisfying $\lim_{\|\mathbf{r}\|_{\mathbb{R}^{d}} \to \infty, t \to \infty}\|F(t,\mathbf{r})\|_{\mathbb{R}^{D}\times\mathbb{R}} = \infty$, and its coordinates are in $\nabla^{-\otimes 2}L^2(\mathbb{R}^{d}\times\mathbb{R}) = \left\{u \in \mathcal{D}'(\mathbb{R}^{d}): \|\nabla^{\otimes 2} u\|_{\mathbb{R}^{d \times d}} \in L^2(\mathbb{R}^{d}\times\mathbb{R})\right\}$. Then, the manifold $M_{F^{*}}^{d+1}:=\{F^*(t,\mathbf{r}): (t,\mathbf{r})\in \mathbb{R}^d\times\mathbb{R}\}$ is the longitudinal principal manifold of $\mathbf{X}$ if $F_{\lambda}^{*} = \argmin_{F}\mathcal{K}_{\lambda, \gamma, \mathbb{P}}(F)$.
\end{definition}

By minimizing the functional $\mathcal{K}_{\lambda, \gamma, \mathbb{P}}$ in (\ref{eq:newKappa}), our goal is to (i) minimize the distance between each data point $\mathbf{X}_t$ and its projection $f_t\left(\pi_{f_t}(\mathbf{X}_t)\right)$ on the fitted manifold at each time point $t$, (ii) penalize the roughness of the fitted manifold at each time point $t$ depending on the value of the tuning parameter $\lambda_t$, and (iii) impose smoothness of the function $F$ over time regularized by the tuning parameter $\gamma$. These goals correspond to minimizing the first, second, and third additives of the functional $\mathcal{K}_{\lambda, \gamma, \mathbb{P}}$ in (\ref{eq:newKappa}), respectively. Using separate smoothing parameters provides additional flexibility in situations where a manifold with a high level of spatial roughness shows minimal changes over time, or vice versa.







\subsection{The LPME Algorithm}

Given the framework proposed in Section \ref{section: The Longitudinal Principal Manifold Framework}, estimation of the longitudinal principal manifold entails minimizing the functional in (\ref{eq:newKappa}). Our proposed algorithm is based on a multi-stage approach, where we consider smoothing of the data clouds at each time point (spatial smoothing) and then smoothing of the obtained manifolds over time (temporal smoothing). Specifically, at each individual time point, we apply the PME algorithm (\cite{mengPrincipalManifoldEstimation2021}, Algorithm 2) to fit the data cloud at the time point and represent the fitted manifold using the spline form in equation~\eqref{eq: spline representation of the PME}. Hence, we have a collection of spline coefficients, i.e., the $s_{j,l}$ and $\alpha_{k,l}$ in equation~\eqref{eq: spline representation of the PME}, associated with each time point. Then, we smooth the time-dependent spline coefficients with respect to time. Details of this procedure are encapsulated in our proposed LPME algorithm, which consists of four steps: data reduction, initialization, fitting, and tuning. A visualization of these steps is shown in Figure \ref{fig:lpme_steps}. Details of this approach are described below and formally given in Algorithm \ref{alg:lpme}, and an implementation is available as an \texttt{R} package at \href{https://github.com/rjzielinski/pme}{\texttt{https://github.com/rjzielinski/pme}}.

%At each individual time point we assume that a manifold has been fit to the data cloud using a spline-based mapping. Hence, in the spatial smoothing phase we assume that we have a collection of spline coefficients that can be used to project from the low-dimensional parameterization to the high-dimensional space. For example, the PME algorithm can be used to obtain this parameterization. In the LPME algorithm, we then use these low-dimensional representations at each time point as the outcomes in a thin plate spline, smoothing over the spatial spline coefficients with respect to time. This yields a multi-stage model that, for a provided time point and set of tuning parameters, obtains a set of spline coefficients that are time-dependent, then returns the high-dimensional output of the spline function associated with the tuning parameter set. Our proposed LPME algorithm consists of four steps: data reduction, initialization, fitting, and tuning. A visualization of these steps is shown in Figure \ref{fig:lpme_steps}. Details of this approach are described below and formally given in Algorithm \ref{alg:lpme}, and an implementation is available as an \texttt{R} package at \href{https://github.com/rjzielinski/pme}{\texttt{https://github.com/rjzielinski/pme}}.

\begin{figure}[ht]
  \centering
  \subfloat[\centering Data]{\label{fig:lpme_step1} \includegraphics[width=8cm]{sim_case1_plot2}}%
  \hfill
  \subfloat[\centering Step 1: Sample Size Reduction]{\label{fig:lpme_step2} \includegraphics[width=8cm]{lpme_data_reduction}}
  \vfill
  \subfloat[\centering Step 2: Initialization]{\label{fig:lpme_step3} \includegraphics[width=8cm]{lpme_initialization}}
  \hfill
  \subfloat[\centering Steps 3 and 4: Fitting and Tuning]{\label{fig:lpme_step4} \includegraphics[width=8cm]{lpme_fitting}}
  \caption{LPME algorithm steps, demonstrated using longitudinal simulated data with $D=2$. Data from the functions $x_1 = r$ and $x_2 = \alpha \sin \left(\beta r + \frac{\pi}{2}\right)$ are generated at 5 time points. Random noise is added both in time and space as $\zeta \bf{g}(t) + \iota$, where $\bf{g}(t)$ represents a normally distributed spatial translation applied to all points at each time, $\zeta$ represents a normally distributed amplitude multiplier for this translation, and $\iota$ represents normally distributed noise applied to each point individually. In (a) we show the original data cloud, in (b) we show the sample size reduction step, where the red points indicate the representative data to be used for the next steps of the algorithm, (c) shows the fitted time specific curves, the blue surface in (d) shows the estimated smooth surface. }
  \label{fig:lpme_steps}
\end{figure}


\subsubsection{Data Reduction}

In most practical manifold learning tasks, data are often quite large. We begin by deriving a small number of feature points in $D$-dimensional space that capture the intrinsic structure of the data cloud. This approach is aimed at reducing the sample size of the original data, while maintaining the available information on the curvature of the underlying manifold of interest. Specifically, we consider $D$-dimensional observations $\left\{x_{it}\right\}_{i=1, t=1}^{I_t, T}$ where the sample size $I_t$ is large. To reduce the sample size $I_t$, we use the high-dimensional mixture density estimation (HDMDE) method (see \cite{mengPrincipalManifoldEstimation2021}, Algorithm 1) to estimate a reduced set of points in the $D$-dimensional space that represent the shape of the data cloud $\left\{x_{it}\right\}_{i=1, t=1}^{I_t, T}$. Briefly, this approach is based on clustering of the data at each original time point to obtain cluster centers, $\left\{\mu_{j, t}\right\}_{j=1, t=1}^{N_t, T}$, represented using red dots in Figure \ref{fig:lpme_step2}, along with their corresponding weights, $\{\hat{\theta}_{j, t}\}_{j=1, t=1}^{N_t, T}$, which characterize the shape of the original data. The number $N_t$ of clusters is estimated for each time point $t$ and is allowed to vary among time points to accommodate potential changes in the shape of the underlying structure. By implementing this approach, we reduce the size $\sum_{t=1}^T I_t$ of data $\left\{x_{it}\right\}_{i=1, t=1}^{I_t, T}$ to $\sum_{t=1}^T N_t$, where each $N_t$ tends to be much smaller than the corresponding $I_t$ (see the simulation study presented in Figure 3(c) of \cite{mengPrincipalManifoldEstimation2021}). The centers $\left\{\mu_{j, t}\right\}_{j=1, t=1}^{N_t, T}$ estimated in this step are then used to reach an initial low-dimensional parameterization.

\subsubsection{Initialization}

The next step in the LPME algorithm for the estimation of the longitudinal principal manifold is initialization. The PME algorithm uses Isomap (\cite{tenenbaumGlobalGeometricFramework2000}) to develop an initial parameterization, which the PME algorithm proceeds to iteratively improve. Reaching a successful fit in the LPME algorithm requires consistent parameterizations across time points. Here, consistency refers to the similarity of the parameterizations of similarly shaped point clouds. However, using Isomap to find parameterizations of similarly shaped manifolds (i.e., the surfaces of brain regions observed at consecutive time points) is likely to result in inconsistent parameterizations. For example, the following two drastically different expressions parameterize the same curve: $x_1 = r$, $x_2 = \sin(r + \frac{\pi}{2})$ and $x_1 = -r$, $x_2 = \sin(\frac{\pi}{2} - r)$. If Isomap is applied at each time point separately, it tends to yield inconsistent parameterizations between time points. Therefore, a different approach to generating an initial parameterization is needed for the LPME algorithm.

To obtain consistent parameterizations of the obtained cluster centers $\left\{\mu_{j, t}\right\}_{j=1}^{N_t}$ over time $t$, we propose using the PME algorithm to develop a parameterization of the centers $\left\{\mu_{j, t_1}\right\}_{j=1}^{N_{t_1}}$ taken from the first time point $t_1$, then using the projection index associated with this estimate to find initial parameterizations of the centers for the remaining time points. This approach is based on the assumption that the underlying manifolds of interest remains relatively stable over time. %The results given in Section \ref{s:simulations} indicate that this process ensures that the initial parameterization remains consistent over time under the assumption of relative stability of the shape of the structure.

\subsubsection{Fitting}

Once the initial parameterization for each cluster center $\mu_{j, t}$ is obtained, the model fitting process begins by using the cluster centers $\mu_{j, t}$, weights $\hat{\theta}_{j, t}$ and the parameterizations associated with each time point to initialize a PME run. Specifically, the cluster centers $\mu_{j, t}$ and weights $\hat{\theta}_{j, t}$ replace those obtained using the HDMDE algorithm, and the Isomap-derived parameters obtained in Steps 1 and 2 of Algorithm 2 in \cite{mengPrincipalManifoldEstimation2021}, respectively.
This results in an estimated principal manifold $\widehat{f_t}$ for each time point $t$ and an optimal tuning parameter $\lambda_t^*$ corresponding to the first two terms of the $\mathcal{K}_{\lambda, \gamma, \mathbb{P}}(F)$ defined in \eqref{eq:newKappa}, illustrated using multicolored lines in Figure \ref{fig:lpme_step3}. The performance of each fit $\widehat{f_t}$ is measured using $\tau_t = \frac{1}{I_t}\sum_{i=1}^{I_t}\| x_{it} - \widehat{f_t}(\pi_{\widehat{f_t}}(x_{it}))\|^{2}_{\mathbb{R}^D}$, an estimate of the mean squared distance between the observed data points and their projections to the manifold defined by $\widehat{f_t}$. 

Further smoothing of the time-specific manifold estimates $\widehat{f_t}$ with respect to time is needed to consider curvature in the time dimension, corresponding to the third term of $\mathcal{K}_{\lambda, \gamma, \mathbb{P}}(F)$. As described in Section \ref{s:PME}, the principal manifold $\widehat{f_t}$ at time $t$ has spline form (see equation~\eqref{eq: spline representation of the PME}) and is characterized by spline coefficients $\{s_{t, j, l}\}_{j=1, l=1}^{N, D}$ and $\{\alpha_{t, k, l}\}_{k=1, l=1}^{d+1, D}$. To smooth the functions $\widehat{f_t}$ over time $t$, we seek to estimate a spline function mapping from the time dimension to the coefficient space given a tuning parameter $\gamma$ (see the third term in equation~\eqref{eq:newKappa}). As detailed by \cite{greenSilverman1994}, smoothing splines are fitted at a collection of knots corresponding to the design points (in this case, the cluster center parameterizations $\{\pi_{\widehat{f_t}}(\mu_{j, t})\}_{j=1, t=1}^{N_t, T}$). Because time-specific principal manifold estimates were fit with different cluster centers $\mu_{j, t}$ for each time point $t$, the coefficients of fitted functions at different time points are not comparable. Hence, to obtain coefficients that can be compared across time points, we design a grid of knots $\left\{\mathbf{r}_i\right\}_{i=1}^{N} \subset \mathbb{R}^d$ ranging from the minimum to the maximum of the approximated parameters $\{\pi_{\widehat{f_t}}(\mu_{j, t})\}_{j=1}^{N_t}$ in each dimension of $\mathbb{R}^d$. Define
\begin{align*}
    N:=\max_{t=1,2,\ldots,T} N_t,\ \ \ \text{ and }\ \ \ Y_{i,t}=\widehat{f_t}(\mathbf{r}_i)\ \text{ for all }i=1,2,\ldots,N \text{ and }t=1,2,\ldots,T.
\end{align*}
Following the discussion in Chapter 7 of \cite{greenSilverman1994}, for each time point $t$, comparable spline coefficients $\mathbf{s}_t^*$ and $\mathbf{\alpha}_t^*$ can then be calculated by minimizing the following function.
\begin{equation}\label{eq:splineL}
    \mathcal{L}(\widehat{f_t}) = (\mathbf{Y}_t - \mathbf{E}\mathbf{s}_t - \mathbf{R}^\T\mathbf{\alpha}_t)^\T(\mathbf{Y}_t - \mathbf{E}\mathbf{s}_t - \mathbf{R}^\T\mathbf{\alpha}_t) + \lambda_t^*\mathbf{s}_t^\T \mathbf{E}\mathbf{s}_t,
\end{equation}
where $\mathbf{Y}_t$ is the $N \times D$ matrix consisting of elements $\left\{Y_{i,t}\right\}_{i=1}^{N}$ at each time point $t$, the matrix $\mathbf{E}=(E_{ij})_{1\le i,j\le N}$ is defined by $E_{ij} = \eta_{d}(\|\mathbf{r}_i - \mathbf{r}_j\|)$, where $\eta_{d}(r) = r^{4 - d}\log r$ if $d$ is even, and $\eta_d(r) = r^{4-d}$ if $d$ is odd, the functions $\phi_1,\ldots,\phi_{d+1}$ form a basis of the linear space of polynomials on $\mathbb{R}^d$ with degrees $\le1$; and the matrix $\mathbf{R}=(R_{ij})_{1\le i\le d+1,1\le j\le N}$ is defined by $R_{ij} = \phi_i(\mathbf{r}_j)$. By computing the derivative of $\mathcal{L}(\widehat{f_t})$ in (\ref{eq:splineL}) with respect to the coefficients, we conduct the minimization of $\mathcal{L}(\widehat{f_t})$ by solving the following system of equations.
\begin{equation}
  \left(
    \begin{array}{cc}
      \mathbf{E} + \lambda_t^* \mathbf{I} & \mathbf{R}^\T \\
      \mathbf{R} & \mathbf{0}
    \end{array}
  \right)\left(
    \begin{array}{c}
      \mathbf{s}_t \\
      \mathbf{\alpha}_t
    \end{array}
  \right) = \left(
    \begin{array}{c}
      \mathbf{Y}_t \\
      \mathbf{0}
    \end{array}
  \right). \label{eq:13}
\end{equation}
The solution to equation~\eqref{eq:13} , denoted by $\mathbf{\alpha}^*_t=(\alpha^*_{t,1},\ldots,\alpha^*_{t,d+1})^\T$ and $\mathbf{s}^*_t=(s^*_{t,1},\ldots,s^*_{t,N})^\T$, yields the function
\begin{equation}\nonumber
  f_{t, l}^*(r) = \sum_{j=1}^{N}s_{t, j, l}^* \cdot  \eta_{d}\left(\|r - r_j^*\|\right) + \sum_{k=1}^{d + 1}\alpha_{t, k, l}^* \cdot \phi_k(r).
\end{equation}

We denote the manifold coefficients at each time point $t$ as $\mathbf{b}_t = (\mathbf{s}_t^{*\T}, \mathbf{\alpha}_t^{*\T})^\T$. To limit the ability of a poorly-fitting PME estimate to greatly influence the results of the LPME model, we fit a weighted spline model to smooth over the manifold coefficients $\mathbf{b}_t$, where the weights for each time point $t$ are equal to
$$w_t = \frac{1}{\tau_t\sum_{i=1}^{T}\frac{1}{\tau_i}}.$$ Thus, the weights correspond to the normalized inverse errors of the PME estimates at each time point. With a given tuning parameter $\gamma$, we then fit a weighted cubic spline function using $\left\{t_i\right\}_{i=1}^T$ as predictors and $\left\{\mathbf{b}_t\right\}_{t=1}^T$ as the response values, with weights $\left\{w_t\right\}_{t=1}^T$ resulting in a function taking the form
\begin{equation}\nonumber
  g_{\gamma}(t) = \sum_{i=1}^{T}\delta_i \,\|t - t_i\|^{3} + \sum_{j=1}^{2}\nu_j\,\phi(t)_j,
\end{equation}
where by defining the $T \times T$ matrix $\mathbf{A}$ as  $\mathbf{A}_{ij} = \|t_i - t_j\|^{3}$ with $i,j = 1,\ldots, T$, the matrix $\mathbf{T}$ as  $\mathbf{T}_{ij} = \phi_i(t_j)$, and $\mathbf{W} = \operatorname{diag}(w_1, \dots, w_T)$, we obtain the coefficients $\mathbf{\delta}=(\delta_1,\ldots,\delta_T)^\T$ and $\mathbf{\nu}=(\nu_1,\nu_2)^\T$ as the solutions to
\begin{equation}
  \left(
  \begin{array}{ccc}
    2\mathbf{A}\mathbf{W}\mathbf{A} + 2\gamma\mathbf{A} & 2\mathbf{A}\mathbf{W}\mathbf{T} & \mathbf{T} \\
    2\mathbf{T}^\T\mathbf{W}\mathbf{A} & 2\mathbf{T}^\T\mathbf{W}\mathbf{T} & \mathbf{0} \\
    \mathbf{T}^\T & \mathbf{0} & \mathbf{0}
  \end{array}
  \right)\left(
  \begin{array}{c}
    \mathbf{\delta} \\
    \mathbf{\nu} \\
    \mathbf{m}
  \end{array}
  \right) = \left(
  \begin{array}{c}
    2\mathbf{A}\mathbf{W}\mathbf{B} \\
    2\mathbf{T}^\T\mathbf{W}\mathbf{B} \\
    \mathbf{0}
  \end{array}
  \right), \label{eq:16}
\end{equation}
where $\mathbf{B}=(\mathbf{b}_1,\ldots, \mathbf{b}_T)^\T$.

For a given $\gamma$, we denote the manifold coefficients at time $t$ estimated by function $g(\cdot)$ as $\mathbf{B}_{\gamma}(t) = \left(\mathbf{s}_{\gamma}(t)^\T, \mathbf{\alpha}_{\gamma}(t)^\T\right)^\T$. Hence, given $\gamma$, the estimated embedding function at time $t$ and $d$-dimensional parameterization $\mathbf{r}$ is
\begin{equation}
  F_{\gamma}(t, \mathbf{r}) = \sum_{j=1}^{N}\mathbf{s}_{\gamma}(t)_j \eta_{d}\left(\|\mathbf{r} - \mathbf{r}_j^*\|\right) + \sum_{k=1}^{d+1}\mathbf{\alpha}_{\gamma}(t)_k \phi_k(\mathbf{r}). \label{eq:17}
\end{equation}

Depending on the locations of spline knots  across time points, this function may take the form of a case in the varying-coefficient model framework discussed in \cite{hastieVaryingCoefficientModels1993}. However, instead of attempting to estimate each coefficient function individually, we consider all coefficients together as the output of a single smoothing spline function. The initial use of smoothing splines to estimate principal manifolds complicates the direct use of estimation approaches developed for this model class in our use case.

\subsubsection{Tuning}

The optimal tuning parameter $\gamma^*$ is identified using leave-one-out cross validation. In this process, the coefficient smoothing function is computed while excluding all data and coefficients with time $t$ from consideration, for $t = 1, \dots, T$. We denote this function by $g_{\gamma}^{(t)}$. We can then assess the performance of the LPME model using tuning value $\gamma$ by averaging over estimates of MSD calculated using the original observations associated with the left-out time $t$ as follows.
\begin{equation}
  \operatorname{MSD}(\gamma) = \frac{1}{T} \sum_{t=1}^{T}\frac{1}{I_t}\sum_{i=1}^{I_t}\|x_{i, t} - F_{\gamma}^{(t)}(t, \pi_{f_{\gamma}^{(t)}}(x_{i, t}))\|^2. \label{eq:18}
\end{equation}
The optimal tuning value $\gamma^*$ is chosen as the value of $\gamma$ that minimizes $MSD(\gamma)$. Leave-one-out cross validation was chosen for this application due to the relatively limited number of time points available for individuals in the ADNI dataset. In other circumstances with greater data availability, alternative cross validation methods, such as $k$-fold cross validation, could easily be used in place of leave-one-out cross validation to better meet computational demands.

In applying the proposed algorithms in extensive simulations, we did not encounter any issues with convergence of the algorithms. Further work must be done to demonstrate that the proposed estimator does indeed minimize the objective function, however, \cite{mengPrincipalManifoldEstimation2021} demonstrated that the function that minimizes equation \eqref{eq:pme_kappa} is of smoothing spline form, while smoothing splines can be derived as a Bayes estimator in a penalized regression setting (\cite{wahba1990}). These results motivate our expectation that the estimator from the LPME algorithm will minimize equation \eqref{eq:newKappa}.



\RestyleAlgo{ruled}
\LinesNumbered

\SetKwComment{Comment}{/* }{ */}

\RestyleAlgo{ruled}
\LinesNumbered



\begin{algorithm}
\caption{Longitudinal Principal Manifold Estimation (LPME)}\label{alg:lpme}
  \KwData{Data points $\left\{X_{i, t}\right\}_{i=1, t=1}^{I_t, T}$, positive integer $d$, positive integer $N_0 < \min{I_t} - 1$, $\alpha$, $\epsilon$, $\epsilon^* \in (0, 1)$, candidate tuning parameters $\left\{\lambda_k\right\}_{k=1}^K$, $\left\{\gamma_l\right\}_{l=1}^L$, $itr \geq 1$, which is the maximum number of iterations allowed.}
\KwResult{Analytic formula of $\hat{f}^*: \mathbb{R}^{d + 1} \to \mathbb{R}^{D + 1}$, optimal tuning parameter $\gamma^*$.}
\For{$t = 1, 2, \dots, T$} {
  Apply HDMDE algorithm with input $\left(\left\{X_{i, t}\right\}_{i = 1}^{I_t}, N_0, \epsilon, \alpha\right)$ and obtain $N_t$, $\left\{\mu_{j, t}\right\}_{j = 1}^{N_t}$, and $\left\{\theta_{j, t}\right\}_{j = 1}^{N_t}$\;
}

  Apply PME to parameterize $\left\{\mu_{j, 1}\right\}_{j = 1}^{N_1}$ by the $d$-dimensional parameters $\left\{r_{j, t}\right\}_{j = 1}^{N_1}$ and use $\pi_{f_1(0)}$ to parameterize $\left\{\mu_{j, t}\right\}_{j=1, t = 1}^{N_t, T}$ by $\left\{r_{j, t}\right\}_{j=1, t=2}^{N_t, T}$. Formally set $\pi_{f_1(0)}(\mu_{j, 1}) \gets r_{j, 1}$ for $j = 1, 2, \dots, N_1$, and $\pi_{f_{t}(0)}(\mu_{j, t}) \gets \pi_{f_1(0)}(\mu_{j, t})$ for $j = 1, 2, \dots, N_t$, $t = 2, \dots, T$\;

\For{t = 1, 2, \dots, T} {
  Apply modified PME algorithm with $\pi_{f_t(0)}(\mu_{j, N_t, t}) \gets \left\{r_{j, t}\right\}_{j = 1}^{N_t}$ and obtain $f_t$, $\lambda_t^*$, and $\tau_t$\;
  $\left\{r_{j, t}\right\} \gets \pi_{f_t}(\mu_{j, t})$ for $j = 1, \dots, N_t$ and $Y_{j, t} \gets f_t(r_{j, t})$ for $j = 1, \dots, N_t$\;
}

  Let $N = \max(N_t)$, $\left\{r_i^*\right\}_{i=1}^{N}$ be a grid spanning the range of estimated values of $\left\{r_{i, j}\right\}_{i=1, t=1}^{N_t, T}$\;

\For{t = 1, 2, \dots, T} {
  Set $Y_{i, t} = f_t(r_{i}^*)$ for $i = 1, \dots, N$\;
  Compute $f_t^*(r)$ by solving \eqref{eq:13}\;
  Set $w_t = \frac{1}{\tau_t \sum_{i=1}^{T}\frac{1}{\tau_i}}$\;
}

  Define $\mathbf{\omega}$ by setting $\mathbf{\omega}_t = \left[\mathbf{s}_t, \mathbf{\alpha}_t\right]$\;
  \For{l = 1, 2, \dots, L} {
    Compute $g_{\gamma_l}(t)$ by solving \eqref{eq:16}\;
    \For{t = 1, 2, \dots, T} {
      Compute $g_{\gamma_l}^{(t)}$ by solving \eqref{eq:16}\;
    }
    Estimate $MSD(\gamma_l)$ using \eqref{eq:18}\;
  }
  $\gamma^* = \arg\min_{\gamma}MSD(\gamma)$\;
  $f^*(t, \mathbf{r}) = f_{\gamma^*}(t, \mathbf{r})$, where the form of $f^*(t, \mathbf{r})$ is given in \eqref{eq:17}
\end{algorithm}


\subsection{Self-intersecting Manifold Estimation}\label{ss:selfInt}

Notably, many manifold learning methods are developed under the assumption that the underlying manifold is not self-intersecting. This is not the case in our motivating problem, where we are interested in modeling the surface of a brain region that is a closed 2-dimensional surface embedded in 3-dimensional space, e.g. see the images of the hippocampus and thalamus in Figure \ref{fig:adni_result}. This may often be the case in other applications where manifold learning is implemented. One such simple example is modeling digits where the number 8 for instance is a self-intersecting manifold.

The self-consistency condition used to define principal curves in \cite{hastiePrincipalCurves1989} theoretically precludes the estimation of curves that are self-intersecting. However, Hastie and Stuetzle demonstrated that this can be done empirically through the use of periodic smoothers. \cite{banfieldIceFloeIdentification1992} generalized the algorithm proposed in \cite{hastiePrincipalCurves1989}, averaging over projection residuals rather than data points, to fit closed curves. These approaches do not adapt easily to the higher dimensional penalized regression setting we consider here.

A commonly used alternative approach for handling closed manifolds is partitioning the high-dimensional data cloud into parts that are not self-intersecting, performing manifold fitting for each partition, and then combining the results preferably by using a technique that results in a smooth manifold in the low-dimensional space (e.g., \cite{mengPrincipalManifoldEstimation2021}). To avoid this process, we propose a data augmentation approach taking advantage of polar or spherical coordinates, depending on the value of $D$. This approach is motivated by the concept of ``lift" in algebraic topology (\cite{hatcher2002algebraic}, Section 1.1). As a simple example, the unit circle can be considered a self-intersecting manifold with $d = 1$ when viewed in two dimensions. However, by adding a third dimension equal to the angle of each observation from the origin, the manifold can be viewed as given in the space with $d = 1$ with the data cloud of $D = 3$, while avoiding any self-intersections. This enables the PME algorithm, and thus the LPME algorithm, to be fit under these circumstances. This process is illustrated graphically in Figure \ref{fig:unit_circle_augmentation}.

\begin{figure}[ht]
  \centering
  \subfloat[\centering Unaugmented Unit Circle]{{\includegraphics[width=9cm]{unit_circle_D2}}}%
  \hfill
  \subfloat[\centering Augmented Unit Circle]{{\includegraphics[width=9cm]{unit_circle_D3}}}
  \caption{Illustration of our proposed data augmentation approach using the Unit Circle. The left panel (a) shows the depiction of the circle with $d=1$ and $D=2$, while the right panel (b) shows how our proposed use of considering this same manifold in $D=3$ by using the polar coordinates results in a manifold that is not self-intersecting.}
  \label{fig:unit_circle_augmentation}
\end{figure}

Because the added dimensions duplicate information contained within the unaugmented observations, these dimensions can be discarded to reach estimates in the original $D$-dimensional space. This approach to data augmentation offers an alternative to the method used in \cite{mengPrincipalManifoldEstimation2021} to fit the PME algorithm to self-intersecting manifolds, where the full dataset was manually partitioned to avoid intersections within each partition. Reaching a combined estimate after fitting PME to each partition individually requires ``gluing'' the estimated manifolds on each partition together, which may introduce error to the combined manifold estimate. This source of error is avoided using our proposed data augmentation approach. Additionally, scaling the added dimensions by a constant value enables the modification of the level of curvature present in the manifold. This may encourage improved performance when estimating manifolds that have too much curvature for PME to reach an appropriate fit under normal circumstances. However, an in depth comparison of these approaches is beyond the scope of this article and is left for future work.





\section{Simulations}\label{s:simulations}

To assess the performance of the LPME algorithm (Algorithm \ref{alg:lpme}), we use simulation studies to compare how closely (in terms of reduction of MSE) the LPME-estimated underlying functions approximate the true data generating functions in comparison with that of three alternative approaches. We consider settings in which $d = 1$ and $D = 2$, $d = 1$ and $D = 3$, as well as $d = 2$ and $D = 3$, with several manifolds being used to generate datasets in each setting. For each manifold, we consider differing values for the time duration, interval between observations, noise levels within and between time points, and the type and magnitude of structural changes in the underlying manifold over time. The embedding functions used to define each manifold are given in Table \ref{table:simulation_embeddings}. In order to put these results in context of previous published work in this area, we use true underlying functions that are similar to those used in previous literature and build on these existing results. Based on our observations of systematic noise sources in imaging data, we add various types of terms to these functions at each visit. In these functions, the value of $\zeta$ and the functional form of $g(\cdot)$ are varied between visits and represent structural changes in the underlying manifold over time. The function $g(\cdot)$ may show change with respect to time that is constant, linear, quadratic, or sinusoidal, with $\zeta$ serving as a scalar multiplier. The values of $\iota$ represent within-image noise in the high-dimensional space, and are randomly sampled from a normal distribution with mean zero and variance that is varied between visits. Meanwhile, $\alpha$ and $\beta$ are vectors of length $d$ that are drawn from a normal distribution with mean 1 and variance that is varied between visits. These values describe random fluctuations in the manifold between time points, reflecting noise introduced by different imaging sessions.

\begin{table}[ht]
\small
  \centering
  \begin{tabular}{|c c c c c|}
    \hline
    Case & $d$ & $D$ & $f_t(\mathbf{r})$ & Domain \\
    \hline
    1 & 1 & 2 & $\left(r, \ \alpha \text{sin} \ (\beta r + \frac{\pi}{2})\right) + \zeta\mathbf{g}(t) + \iota$ & $-3 \leq r \leq 3$ \\
    2 & 1 & 2 & $\left(r, \ \alpha\text{sin} \ (\beta r)\right) + \zeta\mathbf{g}(t) + \iota$ & $-3\pi \leq r \leq 3\pi$ \\
    3 & 1 & 2 & $\left(\alpha_1 \text{cos} \ (\beta_1 r), \ \alpha_2\text{sin} \ (\beta_2 r)\right) + \zeta\mathbf{g}(t) + \iota$ & $-\frac{4\pi}{5} \leq r \leq \frac{\pi}{2}$ \\
    4 & 1 & 3 & $\left(r, (\alpha_1r + \beta_1)^2, \ (\alpha_2r + \beta_2)^3\right) + \zeta\mathbf{g}(t) + \iota$ & $-1 \leq r \leq 1$ \\
    5 & 1 & 3 & $\left(r, \ \alpha_1\text{cos} \ (\beta_1 r), \ \alpha_2\text{sin} \ (\beta_2 r) \right) + \zeta\mathbf{g}(t) + \iota$ & $0 \leq r \leq 3\pi$ \\
    6 & 2 & 3 & $\left(\beta_1r_1, \ \beta_2r_2, \ \alpha_1(\alpha_2\|\beta\mathbf{r}\|^2)\right) + \zeta\mathbf{g}(t) + \iota$ & $-1 \leq r_1, r_2 \leq 1$\\
    7 & 2 & 3 & $\left(\alpha_1\beta_1r_1\text{cos} \ (\alpha_1r_1), \ \alpha_2\beta_2r_1\text{sin} \ (\alpha_2r_1), \ r_2\right) + \zeta\mathbf{g}(t) + \iota$ & $0 \leq r_1 \leq 3\pi$; \ $-1 \leq r_2 \leq 1$\\
    8 & 2 & 3 & $\left(\alpha_1\text{sin}(\beta_1r_1)\text{cos}(\beta_2r_2), \ \alpha_1\text{sin}(\beta_1r_1)\text{sin}(\beta_2r_2), \ \alpha_1\text{cos}(\beta_1r_1)\right) + \zeta\mathbf{g}(t) + \iota$ & $0 \leq r_1 \leq \pi$; \ $0 \leq r_2 \leq 2\pi$\\
    \hline
  \end{tabular}
  \caption{Embedding functions used for simulation studies. Function $\bf{g}(t)$ denotes structural change in the underlying manifold over time, while $\zeta$ represents the scale of this change. Parameters $\alpha$ and $\beta$ are normally distributed with mean 1, and represent random changes in the manifold between time points, while $\iota$ represents within-image noise.}
  \label{table:simulation_embeddings}
\end{table}

In the situations where $d = 1$, LPME is compared to the PME method naively run at each time point without smoothing over time, as well as the principal curve algorithm described in \cite{hastiePrincipalCurves1989}, also run independently at each time point. The principal curve algorithm is implemented using the \texttt{principal\_curve()} function in the \texttt{princurve} package (\cite{Cannoodt2018princurve}), developed using \texttt{R} (\cite{rSoftware2023}). In this function, each of the three smoothing options, \texttt{smooth\_spline}, \texttt{lowess}, and \texttt{periodic\_lowess}, are tested, with the option resulting in the lowest error from the true values being chosen. Following \cite{mengPrincipalManifoldEstimation2021}, the inputs for the PME algorithm are set to $N_0 = 20 \times D$, $\alpha = 0.05$, and $\epsilon = 0.001$, with $\lambda_g = \exp(g)$ for $g = -15, \dots, 5$. In cases where $d = 2$, LPME is again compared to the naive PME approach described above, and the principal surface estimation algorithm developed by \cite{yueParameterizationWhiteMatter2016}.

A factorial design is used to run the simulation studies, with the factor levels set as follows: 1) $\alpha, \beta, \zeta \in \left\{0, 0.05, 0.1, 0.25, 0.5, 1.0\right\}$; 2) study duration: $\left\{1, 2, 5\right\}$; 3) interval between images: $\left\{0.1, 0.25, 0.5\right\}$; 4) longitudinal change model: Constant, Linear, Quadratic, Sinusoidal. The sample size for each time point is set to 1,000 observations. Each combination of factor levels is run once, resulting in each embedding map being simulated a total of 7,776 times. All code required to reproduce the simulation results and associated Figures is provided at \href{https://github.com/rjzielinski/lpme-project}{\texttt{https://github.com/rjzielinski/lpme-project}}.

Visualizations of the results from some example simulated cases, truncated for concision, are shown in Figures \ref{fig:sim_case1} and \ref{fig:sim_case7}. Informally, the performance of each estimation method can be observed by considering the proximity of each method's estimated manifold to the true manifold (shown in red) over time. Thus, in Figure \ref{fig:sim_case1}, we see that the LPME-estimated manifold bears the closest resemblance to the true manifold underlying the data, while the manifolds estimated by PME and the principal curve method show greater responses to temporary random fluctuations in the data at each time point.



\begin{table}[h]
  \centering
  \begin{tabular}{|c c c c c|}
    \hline
    Case & Data & LPME & PME & PC/PS \\
    \hline
    1 & 0.146 (0.233) & {\bf 0.074 (0.122)} & 0.131 (0.258) & 0.118 (0.206) \\
    2 & 0.467 (0.665) & {\bf 0.248 (0.516)} & 0.516 (0.750) & 0.564 (0.419) \\
    3 & 0.291 (0.601) & {\bf 0.239 (0.564)} & 0.317 (0.640) & 0.264 (0.584) \\
    4 & 4.26 (21.2) & {\bf 3.29 (13.0)} & 4.23 (21.1) & 4.22 (21.2) \\
    5 & 0.895 (1.33) & {\bf 0.584 (1.08)} & 0.894 (1.36) & 0.821 (1.22) \\
    6 & 0.284 (1.06) & {\bf 0.273 (0.891)} & 0.316 (1.04) & 0.557 (0.387) \\
    7 & 0.145 (0.552) & 2.96 (4.65) & 6.92 (1.17) & {\bf 1.58 (0.514)} \\
    8 & 0.110 (0.325) & {\bf 0.074 (0.208)} & 0.115 (0.331) & 0.172 (0.234) \\
    \hline
  \end{tabular}
  \caption{Mean Squared Distance comparison of simulated data, longitudinal principal manifold estimation (LPME)-, principal manifold estimation (PME)-, and principal curve / surface-based estimates to true values, Median (IQR). The lowest algorithm-specific median (IQR) are highlighted in bold.}
  \label{table:simulation_results_median}
\end{table}

\begin{figure}[h]
  \centering
  \subfloat[\centering Simulation Case 1]{{\includegraphics[height=8cm]{sim_case1}} \label{fig:sim_case1}}%
  \vfill
  \subfloat[\centering Simulation Case 7]{{\includegraphics[height=8cm]{sim_case7}} \label{fig:sim_case7}}
  
  \caption{Scatterplots of two example simulated data clouds (gray points) with $d=1$ and $d=2$ overlaid with points on the true embedding manifold (red), the estimated LMPE function (blue), PME (green), and principal curve or surface (purple).}
  \label{fig:sim_results}
\end{figure}

The median and interquartile range of the mean squared distance from the true underlying manifold values for the estimates of each approach, as well as for the data itself, are shown in Table \ref{table:simulation_results_median}. Analogous mean and standard deviation summaries of the results are given in the Appendix. Because the PME, principal curve, and principal surface methods each attempt to estimate the manifold in question at each individual time point without allowing other time points to inform these estimates, they should result in similar deviations from the true underlying manifold, with differences in error resulting primarily from differing performances in fitting to the observed data. Meanwhile, because the LPME approach accounts for all time points simultaneously, this approach results in lower mean squared distance values.

The result summaries indicate that in most cases, LPME provides a substantial improvement in performance over the PME and principal curve approaches when estimating the underlying manifold. As seen clearly in Figure \ref{fig:sim_case1}, while the structure of the simulated data changes noticeably between time points, the LPME estimates, shown in blue, remain relatively stable over time. This contrasts to the estimates found by the PME and principal curve approaches, which are highly sensitive to the systematic added noise in the data observed at each given time point, as expected. This ultimately results in the LPME estimates remaining closer to the true underlying manifold, shown in red. Similar results hold in most of the other simulation cases.



\section{Longitudinal Segmentation of MRI images in Alzheimer's Disease Studies}\label{s:application}

As discussed previously, meaningful between-image errors in estimates of subcortical structures are introduced during the process of segmenting MRI images. This section demonstrates how the LPME algorithm may be used to mitigate the effects of such noise on further analysis using these estimates. To achieve this, we used MRI data collected through the ADNI study, a longitudinal observational study with the goal of identifying imaging biomarkers to assess the progression of AD.

While the original ADNI 1 cohort included 200 cognitively healthy elderly individuals, 400 with mild cognitive impairment, and 200 with AD, this analysis focuses on 463 participants, of whom 130 were cognitively normal, 223 displayed symptoms of mild cognitive impairment, and 101 were diagnosed with AD at the baseline study visit (\cite{jack2008adni}). Diagnostic information was unavailable for 9 of the 463 participants for whom imaging data was available. Of the 130 cognitively normal participants, 34 were subsequently diagnosed with mild cognitive impairment or dementia during the course of the study. Follow-up duration for these participants ranged from 0 months to approximately 52 months, with imaging scheduled to be conducted at six- or 12-month intervals depending on the stage of the study. To encourage greater stability of longitudinal estimates, only participants with at least 24 months of follow-up were considered. Our final analysis set included 236 participants. Of these remaining participants, 88 were cognitively normal, 107 had mild cognitive impairment, and 41 were diagnosed with AD at their baseline visits. We focused on two brain regions of interest: the hippocampus and the thalamus. The hippocampus is of interest due to the changes experienced by those with AD, while the shape of the thalamus more closely aligns with the spherical structure used in simulation case 8.

Images were processed using FSL (\cite{jenkinsonFSL2012}) via the \texttt{fslr} package (\cite{muschelliFslrConnectingFSL2015}) in \texttt{R}, with FSL's FLIRT method linearly registering the images to MNI space, and the FIRST method being used for image segmentation. Following image segmentation, the surfaces of each region were identified by finding the extreme voxels in each dimension with nonzero intensity readings. The estimated surface positions were then standardized to a maximum distance of one from the origin in each dimension of Euclidean space and centered around the origin. Finally, the data were augmented with spherical coordinates in a manner similar to that described in Section \ref{s:LPME} to avoid the need to fit to a self-intersecting manifold.

Results of fitting the PME and LPME algorithms on the surface of the left hippocampus and left thalamus of a single participant are shown in Figure \ref{fig:adni_result}. The previously described data augmentation approach was used to enable the fitting of closed surfaces. Visual inspection of the hippocampus estimates indicates that while there are slight differences between the shape shown in the data and the shape estimated by the LPME algorithm, the surface estimated by LPME appears to fit reasonably to the data. There are two main sources of discrepancies between the data and the LPME estimate. First, at time points where the observed surface changes orientation, as is visible between the first two time points, the LPME estimates maintain a consistent orientation, reflecting the goal of encouraging stability in the structural estimates between time points. The second difference between the LPME estimates and the observed data is seen at the sharper corners of the hippocampus, where the LPME estimates do not fully capture the severity of curves in the observed data.

Figure \ref{fig:lhipp_cross_sections} depicts cross sections of the observed hippocampus surface and the estimated surface obtained using LPME and PME at each individual time point. These cross sections illustrate the differences between the estimates reached by the PME and LPME algorithms, with the LPME-based estimate being generally less responsive to changes in the shape and orientation of the hippocampus between time points. This Figure reiterates that both the PME and LPME algorithms are unable to capture the distinctive sharp curve in the structure at the upper-left corner of each plot. Additionally, we see that, along the upper-right edge of the surface, there is a spot where the estimated manifold intersects with itself. This is present for both the PME and LPME estimates, but appears to be more prevalent when using LPME. Notably, it also appears that the boundaries of the LPME-estimated surface tend to fall inside the boundaries of the PME-estimated surface, particularly at locations with the highest levels of curvature.

The hippocampus is a region with a relatively irregular shape, which may account for the differences in performance between the LPME algorithm when applied to simulated data and when applied to the hippocampus data. To understand how the approach performs when used with a more regularly shaped brain region, we also applied the PME and LPME algorithms to the surface of the left thalamus of the same set of individuals described previously. Estimates of the surface of the thalamus were obtained from the MRI images using the same preprocessing and segmentation steps detailed above for the hippocampus data.

Considering the same individual shown in Figures \ref{fig:adni_result} and \ref{fig:lhipp_cross_sections}, cross sectional results of fitting the PME and LPME algorithms to the surface of the left thalamus are shown in Figure \ref{fig:lthal_cross_sections}. While the segmentation estimates of the thalamus in Figure \ref{fig:adni_result} do not show deviations that are clearly visible for the hippocampus, the cross section plots shown in Figure \ref{fig:lthal_cross_sections} indicate that both the PME and LPME algorithms show a closer fit to the more regularly shaped thalamus surface than was achieved for the hippocampus.

To understand how the subcortical structure estimates obtained by the LPME and PME algorithms may impact the corresponding estimates of the volumes of these structures, the volume of each structure was approximated from the raw data and the estimates obtained by the PME and LPME algorithms at each time point. The volume estimation approach used here relies on counting the voxels contained within the boundary defined by the PME- or LPME-estimated embedding map, with each voxel having a known volume.  The code used to implement this approach is available at \href{https://github.com/rjzielinski/lpme-project}{\texttt{https://github.com/rjzielinski/lpme-project}}. To align processing steps as closely as possible, the volume estimates shown for the raw data use the same volume estimation method used to find LPME- and PME-based approximations, rather than the volume estimates produced by FSL's FIRST method during segmentation.

The volume estimates for randomly selected individuals can be found in Figures \ref{fig:lhipp_volume_comparison} and \ref{fig:lthal_volume_comparison}, corresponding to estimates for the left hippocampus and left thalamus, respectively. Here, we see that volume estimates differ substantially depending on whether PME, LPME, or the raw data were used, with the LPME estimate resulting in the lowest volume computations. In Figure \ref{fig:lhipp_volume_comparison}, we see that the LPME-based volume estimates for the hippocampus are consistently lower than those obtained from the raw data and the PME estimates. All three sources show substantial variability between study visits, particularly for subjects 011\_S\_0003 and 036\_S\_1240. Considering the thalamus estimates in Figure \ref{fig:lthal_volume_comparison}, we again see that meaningful variability in the volume estimates using the raw data and PME estimates as sources exist between time points. However, the LPME estimates demonstrate the ability to smooth over this variability, resulting in volume estimates that are not as sensitive to differences between time points. We also see that when basing volume estimates on the more reliable LPME fit to the thalamus data, LPME yields volume estimates similar in magnitude to those obtained from the raw data and PME estimates.

A summary of the between-visit variability in the volume estimates of the subcortical structures considered here is shown in Table \ref{table:adni_volume_sds}. In addition to reporting the standard deviations of the volume estimates, we also report regression-adjusted estimates of variability to account for cases in which the structure cannot be assumed to be constant over time, as is the case for those with AD. The regression-adjusted summaries estimate the standard deviation of the residuals of a simple linear regression of volume on time. Meanwhile, in cases without regression adjustment, we estimated the standard deviation of the volume estimates for each individual and each estimate source (i.e. raw data, LPME, and PME). We then took the mean of these values for each estimate source. Table \ref{table:adni_volume_sds} shows that PME yields the smoothest volume estimates for both hippocampuses. This conclusion aligns with the visual evidence of inconsistency in the LPME estimates caused by the irregular shape of the hippocampus. However, when applied to the thalamus, LPME shows notable improvements in the smoothness of the associated volume estimates.

\begin{figure}[h]
  \centering
  %\includegraphics[width=\textwidth]{adni_plots/adni_cross_section}
  \subfloat[\centering Left Hippocampus Cross Section]{{\includegraphics[height=8cm]{adni_plots/adni_lhipp_cross_section}} \label{fig:lhipp_cross_sections}}%
  \vfill
  \subfloat[\centering Left Thalamus Cross Section]{{\includegraphics[height=8cm]{adni_plots/adni_lthal_cross_section}} \label{fig:lthal_cross_sections}}
  
  \caption{Left Hippocampus and Left Thalamus Cross Sections. LPME-approximated embedding function, shown in blue, routinely falls inside the boundaries of the observed hippocampus surface. In the fifth hippocampus observation with $t=6.55$, the LPME-estimated surface has a gap. Such gaps in the surface estimates frequently correspond with inaccurate volume estimates. The cross sectional plots indicate that the PME and LPME algorithms fit much more closely to the regularly shaped thalamus surface.}
  \label{fig:adni_cross_sections}
\end{figure}

\begin{figure}[h]
\centering
\subfloat{
\label{fig:adni_lhipp_result}
\subfloat[\centering Data]{
\includegraphics[height=10cm]{adni_plots/adni_lhipp_data_plot}
}
\hfill
\subfloat[\centering LPME]{
\includegraphics[height=10cm]{adni_plots/adni_lhipp_lpme_isomap_plot}
}
\hfill
\subfloat[\centering PME]{
\includegraphics[height=10cm]{adni_plots/adni_lhipp_pme_plot}
}
}
\hfill
\subfloat{
\label{fig:adni_lthal_result}
\subfloat[\centering Data]{
\includegraphics[height=10cm]{adni_plots/adni_lthal_data_plot}
}
\hfill
\subfloat[\centering LPME]{
\includegraphics[height=10cm]{adni_plots/adni_lthal_lpme_isomap_plot}
}
\hfill
\subfloat[\centering PME]{
\includegraphics[height=10cm]{adni_plots/adni_lthal_pme_plot}
}
}
\caption{Left Hippocampus (left), and Left Thalamus (right), Cognitive Healthy ADNI Participant. The raw surface data, displayed in red, show slight changes in orientation that are absent in the LPME estimates, shown in blue. The estimates obtained by the PME and LPME algorithms appear unable to accurately capture the true shape of the hippocampus at points with high levels of curvature.}
\label{fig:adni_result}
\end{figure}


%\begin{figure}[h]
%\centering
%\subfloat{
%\label{fig:adni_lhipp_result}
%\begin{subfigure}
%\includegraphics{adni_plots/adni_lhipp_data_plot}
%\caption{Data}
%\label{fig:adni_lhipp_result_data}
%\end{subfigure}
%}

% \hfill
%\subfloat{
%\label{fig:adni_lthal_result}
%\begin{subfigure}
%\includegraphics{adni_plots/adni_lthal_data_plot}
%\caption{Data}
%\label{fig:adni_lthal_result_data}
%\end{subfigure}
%}

%\caption{Left Hippocampus (left), and Left Thalamus (right), Cognitive Healthy ADNI Participant. The raw surface data, displayed in red, show slight changes in orientation that are absent in the LPME estimates, shown in blue. The estimates obtained by the PME and LPME algorithms appear unable to accurately capture the true shape of the hippocampus at points with high levels of curvature.}
%\label{fig:adni_result}

%\end{figure}



\begin{figure}[h]
    \centering
    \subfloat[\centering Left Hippocampus Volume Estimates]{
      \includegraphics[height=8cm]{adni_plots/adni_lhipp_volume_comp}
      \label{fig:lhipp_volume_comparison}
    }
    \vfill
    \subfloat[\centering Left Thalamus Volume Estimates]{
      \includegraphics[height=8cm]{adni_plots/adni_lthal_volume_comp}
      \label{fig:lthal_volume_comparison}
    }
    \caption{Left hippocampus and left thalamus volume estimates for three ADNI participants. Volume estimates are obtained by summing the volumes of voxels contained within the structure as described by the segmented data (red, circles), LPME (blue, squares), and PME (green, triangles). When PME and LPME are applied to the irregular shape of the hippocampus, there is a clear ordering of the volume estimates. Gaps in the estimated surface induce underestimates of the volume compared to the volume values estimated from the data. These gaps may also cause inconsistency in the LPME volume estimates, resulting in higher aggregate variability than for the data or PME estimates. When applied to the thalamus, the PME-based volume estimates demonstrate similar time point-to-time point changes in the volumes estimated from the segmented data, reflecting the close fit of the PME algorithm to the regularly-shaped thalamus data. The LPME estimates appear to successfully smooth over regions with large variations in subsequent volume measures from the data and PME estimates, as seen for participant 011\_S\_0003.}
\end{figure}


\begin{table}[ht]
  \centering
  \begin{tabular}{|c c c c c|}
    \hline
    Structure & Regression-Adjusted & Data & LPME & PME  \\
    \hline
    Left Hippocampus & No & 129 & 132 & \textbf{106} \\
    Right Hippocampus & No & 137 & 136 & \textbf{110} \\
    Left Hippocampus & Yes & 100 & 108 & \textbf{87.1} \\
    Right Hippocampus & Yes & 108 & 121 & \textbf{85.9} \\
    Left Thalamus & No & 57.5 & \textbf{31.3} & 42.0 \\
    Right Thalamus & No & 108 & \textbf{66.4} & 85.3 \\
    Left Thalamus & Yes & 45.8 & \textbf{20.6} & 35.4 \\
    Right Thalamus & Yes & 89.3 & \textbf{50.1} & 73.1 \\
    \hline
  \end{tabular}
  \caption{Mean of the empirical standard deviation of volume estimates computed for each ADNI participant. Entries that were not regression-adjusted include computations for raw volume estimates, while regression-adjusted entries compute the standard deviations of linear regression models regressing the volume estimates on time. }
  \label{table:adni_volume_sds}
\end{table}



\section{Discussion}\label{s:discussion}

In this article, we propose the LPME framework for modeling longitudinal changes in a low-dimensional manifold underlying high-dimensional data. To the best of our knowledge, this is the first time a nonlinear manifold learning method has been adapted to a general-purpose longitudinal setting. We also suggest a data augmentation approach that circumvents challenges in fitting principal manifold-based methods to self-intersecting manifolds in select settings. In simulated datasets where spurious changes in the structure to be estimated are introduced between time points, the LPME algorithm shows improved performance, in terms of minimizing the MSD, in recovering the underlying manifold when compared to a naive approach of applying either the PME or principal curve/surface algorithms at each time point. This improvement is observed across several manifold types.

Similar to other applications of statistical smoothing techniques, it is difficult to compare performance of models in real data experiments. Specifically, the MSD provides a summary of the distance of the data points and their projections onto the estimated manifolds for each model. This means that a model that performs well using this metric may not actually be the best fit for the underlying manifold, e.g. in the case of overfitting. This largely explains why PME does not usually show better performance than the PC/PS methods using this metric - both approaches are estimating a curve or surface using the data available at a given time point, which may be affected by systematic noise that increases the distance to the true manifold. When comparing the fit of the PME and principal surface and principal curve methods in the simulated data, PME tends to show improvement in the median MSD, with elevated mean MSD levels compared to principal curves.

While the LPME algorithm demonstrated strong performance in a number of settings, it is important to note the assumptions under which the method performs well. First, because Isomap and PME are used for initialization and time-specific fitting, it is necessary that the assumptions required by these algorithms hold for the LPME algorithm to perform well. In practice this limits the success of LPME to situations where Isomap performs well, which excludes nonconvex manifolds, manifolds that have holes, or manifolds that have too high a level of curvature. In such cases, we recommend the use of alternative methods, such as Locally Linear Embedding (\cite{roweisNonlinearDimensionalityReduction2000}) or Laplacian Eigenmaps (\cite{belkin2003laplacian}), to generate the initial parameterization to alleviate this concern. Opportunities also exist to improve the tuning of the initialization method, which may result in better performance, especially when considering data with different levels of noise. A second assumption is that the LPME method requires relative stability in the overall shape of the underlying manifold structure. Thus, while this algorithm is useful for separating meaningful changes in a manifold over time from inter-observation error, it may not perform well in situations where the error between observations is too substantial, similar to any smoothing method. How restrictive this requirement is depends largely on the area of application. When used to estimate subcortical surfaces in a single individual as suggested here, this tends not to be overly restrictive, as changes in the shape and size of a subcortical structure, while clinically meaningful, tend to be relatively small in magnitude compared to their defining features.

An interesting future research direction is extension of the LPME method for modeling  shapes such as the hippocampus surfaces obtained from the ADNI dataset. It is clear that neither the PME nor the LPME estimates were able to fully capture the distinctive curved structure of the hippocampus. There are two potential sources of this difficulty, each suggesting different steps that may be taken to address this issue. The manifold learning method used in the initialization step of the PME algorithm, or the parameters of that method could be adjusted to better estimate the sharp curvature of the hippocampus. Also, while a simple grid search is used to select the optimal value of the smoothing parameter in both the PME and LPME algorithms, it is possible that an improved search method would yield results that are more sensitive to the nuances of the structure being estimated. 

The second potential interesting direction of research in the future is more in-depth investigation of the data augmentation approach  described in Section \ref{s:LPME}, that allows us to fit some self-intersecting manifolds. The proposed augmentation with polar or spherical coordinates, allowed the PME and LPME algorithms to estimate self-intersecting manifolds that would not previously have been estimable by these methods. However, while this approach addressed a problem posed by structures with roughly circular or spherical shapes, such as the thalamus, its success may not extend to settings with more complex data. In the case of the hippocampus, potential intersections in the spherical coordinates may result in the overlaps in certain regions of the estimated manifolds. Similar types of issues may arise in settings with other complex structures, like a figure 8 shape. The development of a procedure to handle these considerations would expand the potential use cases for this approach, thus allowing principal manifold-based methods to be used in a wider range of situations.

Estimation of volumes of the resulting manifolds is essential, as the volumes are often used as outcomes in modeling disease processes over time in clinical trials and observational studies. In this manuscript, the volumes were obtained by simply computing the number of within-region voxels resulting from each fitting procedure. If the resulting manifold has gaps, however, this procedure may not be possible to implement. \cite{mengPrincipalManifoldEstimation2021} introduced an interior identification approach that effectively makes use of the explicit representation of the embedding map obtained through the PME and LPME fitting processes. Future work may adapt this procedure to settings with gaps in the estimated manifold.

While the approach described in this article is limited to the estimation of manifolds from observations taken from a single structure over time, extension of this method to manifold estimation for groups of similar structures will be a valuable contribution to the literature allowing population-level comparisons of time-dependent trajectories of shapes. Reaching estimates associated with groups of structures raises the possibility of statistical comparisons between these groups, which, among others, could prove beneficial in a number of clinical trial settings. In population-level modeling of the data, it will be important to also consider registration of the participant-level MRI data to a common template space, raising important considerations for alignment of the shapes without losing important information.

Finally, Gaussian process regression (GPR) may be applied to smooth the time direction. The smoothness can be represented by the covariance kernel used in the GPR, e.g. by applying the Gaussian or Matérn kernels (\cite{li2023inference}). Future work may consider implementation of a similar approach in the framework we proposed in this article. 


\section*{Acknowledgements}

Research presented in this manuscript was funded by National Institute On Aging of the National Institutes of Health (NIH) under Award Number R01AG075511. The ideas presented in this paper are the responsibility of the authors and may not necessarily represent the official views of the NIH.




\newpage

%\nocite{*}
%\bibliographystyle{plain}
%\bibliography{references}
%\printbibliography

\newpage

\appendix

\section*{Appendix}

\begin{table}[h]
  \centering
  \begin{tabular}{|c c c c c|}
    \hline
    Case & Data & LPME & PME & PC/PS \\
    \hline
    1 & 0.223 (0.256) & {\bf 0.125 (0.161)} & 0.268 (0.850) & 0.189 (0.245) \\
    2 & 0.514 (0.408) & {\bf 0.384 (0.648)} & 0.843 (1.93) & 0.600 (0.296) \\
    3 & 0.446 (0.445) & {\bf 0.401 (0.446)} & 0.507 (0.594) & 0.412 (0.423) \\
    4 & 30.7 (88.1) & {\bf 27.7 (260)} & 30.7 (88.2) & 30.6 (88.1) \\
    5 & 0.980 (0.771) & {\bf 0.791 (0.845)} & 1.04 (1.07) & 0.934 (0.713) \\
    6 & 1.43 (6.04) & 1.21 (5.66) & 1.47 (6.06) & {\bf 1.01 (2.11)} \\
    7 & 0.580 (0.839) & 4.07 (3.30) & 7.37 (1.14) & {\bf 1.95 (0.800)} \\
    8 & 0.226 (0.275) & {\bf 0.136 (0.169)} & 0.242 (0.311) & 0.274 (0.243) \\
    \hline
  \end{tabular}
  \caption{MSD comparison to true values, Mean (SD). For each case, the lowest algorithm-specific mean (SD) are highlighted in bold. }
  \label{table:simulation_results_mean}
\end{table}

\bibliographystyle{agsm}
\bibliography{references}

\end{document}
