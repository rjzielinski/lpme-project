%\documentclass[12pt]{amsart}
\documentclass[11pt,reqno]{article}
%\usepackage{cases}


\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
%\RequirePackage[numbers]{natbib}
\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
\RequirePackage{graphicx}%% uncomment this for including figures

\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{graphicx}
\usepackage[perpage,symbol*]{footmisc}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}  
\usepackage{tikz}

\usepackage{algorithm2e}

\usepackage{natbib}
\usepackage{authblk}

\usepackage[english]{babel}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}


\renewcommand{\baselinestretch}{1.0}
\setlength{\oddsidemargin}{-0.5cm}
\renewcommand{\topmargin}{-2cm}
\renewcommand{\oddsidemargin}{0mm}
\renewcommand{\evensidemargin}{0mm}
\renewcommand{\textwidth}{180mm}
\renewcommand{\textheight}{240mm}

\begin{document}

\title{Longitudinal Manifold Estimation}
\author[1]{Robert Zielinski and Ani Eloyan}


\maketitle

\section{Introduction}

Neuroimaging plays a critical role in the diagnosis and monitoring of a number of common neurodegenerative conditions, such as Alzheimer's disease and Parkinson's disease (Knopman et al (2021), Poewe et al (2017)). Frequently, interest centers around changes in one or more substructures. For example, it is common to observe atrophy in the hippocampus of those with Alzheimer's disease or cognitive impairment from other causes. Image segmentation allows for the extraction and analysis of these subcortical structures.

Traditionally, manual segmentation of images by a trained radiologist has been considered the most accurate approach, and is the standard. However, this approach is highly time and resource intensive. In studies analyzing a large number of images, this may be cost prohibitive. Additionally, manual segmentation suffers from meaningful inter-rater and intra-rater variability. Automated segmentation approaches, such as FSL-FIRST and FreeSurfer, have been introduced to address these concerns. While automating the segmentation process drastically reduces costs, it potentially introduces additional inaccuracies. 

To demonstrate the potential extent of the inaccuracies introduced by segmentation, using FSL-FIRST to segment the hippocampus in images of one healthy individual over the course of four year in the ADNI dataset, the volume of the left and right hippocampi decreased by 3.6\% and 5.4\%, respectively, while each hippocampus was estimated to have increased in volume in subsequent images twice. In a larger comparison of the accuracy of FSL-FIRST and FreeSurfer, Mulder et al. (2014) found that 6.9\% of segmentations by FIRST failed visual inspection, as did 7.5\% of segmentations for FreeSurfer. After removing these failed segmentations, FIRST and FreeSurfer produced segmentations with variability similar to and slightly lower than manual segmentation, respectively. If failed segmentations were not removed from analysis, reflecting a more realistic situation when working with large numbers of images, variability as measured by the Limits of Agreement were much higher for FIRST and FreeSurfer than for manual segmentation. Mulder et al. (2014) also found slightly higher rates of segmentation failure among individuals with Alzheimer's disease for both automated segmentation approaches.

Ultimately, high levels of variability are present at the subcortical level regardless of the segmentation approach used, but may be particularly influential when using automated segmentation methods. To address these elevated noise levels, we propose a manifold learning-based method to develop smooth estimates of the surfaces of subcortical structures over time. This approach, which extends the principal manifold estimation algorithm introduced by Meng and Eloyan (2021) for a longitudinal setting, includes a regularization parameter to limit the effects of between time-point variability.

\section{Introduction - Old}

Manifold learning describes a set of statistical approaches to modeling high-dimensional data that are assumed to lie along a low-dimensional manifold. These methods have become increasingly popular for medical imaging applications, where assuming an underlying manifold structure is helpful for modeling certain anatomical shapes. In these settings, interacting with a structure in a low-dimensional form may be more intuitive than in a higher-dimensional space. For example, Yue et al. (2016) seeks to compute a parametric representation of the corpus callosum. A number of methods have been developed for manifold learning, including Isomap (Tenenbaum et al. 2000), locally linear embedding (Roweis and Saul, 2000), and Laplacian eigenmaps (Belkin and Niyogi, 2003). Meng and Eloyan (2021) proposed a principal manifold framework that resolved several limitations of previously developed extensions of the principal curves approach introduced by Hastie and Stuetzle (1989).

In many medical imaging situations, understanding how an anatomical structure changes over time could provide meaningful information into the function of that structure, or medical conditions that affect that function. In situations where the structure is better represented by a low-dimensional manifold, interest carries over to understanding how the underlying manifold changes longitudinally as well. 

While the methods given above provide a variety of approaches to retrieving a low-dimensional manifold from high-dimensional data at a single time point, they are ill-equipped to reach meaningful estimates across multiple time points. This is because these methods consider time as an additional dimension in the data, meaning they include time in the dimension-reduction process. This treatment of the time dimension causes time to be incorporated into the low-dimensional parameterization, preventing meaningful interpretation of this dimension. Here, we propose an alternative approach to estimating manifolds over time.

Rather than include time as another dimension in a manifold learning method, we use a process that maintains the interpretability of the time dimension. Specifically, we use the method proposed in Meng and Eloyan (2021) to estimate an appropriate principal manifold at each given time point. We then smooth over these approximated manifolds in the time dimension, yielding an estimate showing longitudinal changes in the underlying manifold.

Some work has been conducted on longitudinal dimension reduction methods, but this research has primarily focused on linear methods, such as the longitudinal principal component analysis approach proposed by Kinson et al. (2019). To our knowledge, the approach to longitudinal smoothing in the manifold learning setting taken in this paper is novel.

The remainder of this article is laid out as follows: In Section 2, we introduce the principal manifold estimation method proposed in Meng and Eloyan (2021). Section 3 discusses how this approach can be adapted for longitudinal settings, and shows the algorithm being used to accomplish this. Section 4 demonstrates the performance of this approach on simulated data. In Section 5, we apply this method to longitudinal data of the hippocampus in individuals with Alzheimer's disease. The article finishes with a discussion of the method in Section 6.

\section{Principal Manifolds and Estimation Algorithm}

Because the algorithm proposed in this article relies on the same definitions and notation as those introduced in Meng and Eloyan (2021), we provide it here. To generalize the principal curve framework proposed in Hastie and Stuetzle (1989) to intrinsic dimensions $d > 1$, Meng and Eloyan introduce the function space 
\[%
  C_{\infty}(\mathbf{R}^d \to \mathbf{R}^D) = \left\{f \in C()\right\}
.\]%

\section{Longitudinal Principal Manifold Estimation Algorithm}

Let $\{x_{it}\}_{i=1, t=1}^{I, T}$ be data points in $D$-dimensional space.

Outline:
\begin{enumerate}
    \item Introduce principal surfaces / principal manifolds, PME algorithm, and the loss function it minimizes
    \item Show updated loss function for LPME algorithm
    \item LPME algorithm
\end{enumerate}

\section{Simulations}

\section{Application}

\section{Discussion}

\RestyleAlgo{ruled}
\LinesNumbered

\SetKwComment{Comment}{/* }{ */}

\begin{algorithm}
\caption{Longitudinal Principal Manifold Estimation}\label{alg:two}
\KwData{Data points $\{y_{i, t}\}_{i, t}^{I, T}$, positive integer $d$, positive integer $N_0 < I - 1$, $\alpha$, $\epsilon$, $\epsilon^* \in (0, 1)$, candidate tuning parameters $\{\lambda_k\}_{k=1}^K$, $\{\gamma_l\}_{l=1}^L$, $itr \geq 1$, which is the maximum number of iterations allowed.}
\KwResult{Analytic formula of $g^*: \mathbf{R}^{d+1} \to \mathbf{R}^{D+1}$, optimal tuning parameter $\gamma^*$.}
\For{$t = 1, 2, ... T$}{
    Apply PME algorithm with input $(\{y_{i, t}\}_{i=1}^I, N_0, \alpha, \epsilon, \epsilon^*, \{\lambda_k\}_{k=1}^K)$ and obtain $\{f_t^*\}_{t=1}^T$ and $\{\lambda_t^*\}_{t=1}^T$\;
    Use $\{f_t^*\}_{t=1}^T$ to map a grid of $d$-dimensional parameters $\{r_j\}_{j=1}^N$ to $\{x_{j, t}\}_{j=1}^N$ in $\mathbf{R}^D$\;
}
Apply ISOMAP to parameterize the newly generated collection $\{x_{j, t}\}_{j, t}^{N, T}$ by the $d$-dimensional parameters $\{r_j\}_{j=1}^N$\;
Use parameters $\{r_j\}_{j=1}^N$ and time values $\{t_j\}_{j=1}^N$ to create new
$\pi_{g_{(1)}}(x_{j}) \gets r_j$ for $j=1, ..., N$\;
\For{$l = 1, 2, ..., L$}{
    $\gamma \gets \gamma_l$.\;
    Obtain $g_{(1)}$ by solving 
    $\left(\begin{array}{ccc} 
    2\mathbf{E}\mathbf{W}\mathbf{E} + 2\gamma\mathbf{E} & 2\mathbf{E}\mathbf{W}\mathbf{T} & \mathbf{T} \\
    2\mathbf{T}^T\mathbf{W}\mathbf{E} & 2\mathbf{T}^T\mathbf{W}\mathbf{T} & \mathbf{0} \\
    \mathbf{T}^T & \mathbf{0} & \mathbf{0} \end{array}\right)\left(\begin{array}{c} 
    s_l \\
    \alpha_l \\
    m_l 
    \end{array}\right) = \left(\begin{array}{c}
    2\mathbf{E}\mathbf{W}\mathbf{x} \\
    2\mathbf{T}^T\mathbf{W}\mathbf{x} \\
    \mathbf{0} 
    \end{array}\right)$\;
    $\mathcal{E} \gets 2\epsilon^*$\;
    $n \gets 1$\;
    $\mathcal{D}_{\hat{Q}, N}(g_{(1)}) \gets \sum_{j=1}^N\|x_j - g_{(1)}\|_{\mathbf{R}^{D}}^2$\;
    \While{$\mathcal{E} \geq \epsilon^*$, $n < itr$}{
        Compute $g_{(n + 1)}$ by solving $\left(\begin{array}{ccc} 
        2\mathbf{E}\mathbf{W}\mathbf{E} + 2\gamma\mathbf{E} & 2\mathbf{E}\mathbf{W}\mathbf{T} & \mathbf{T} \\
        2\mathbf{T}^T\mathbf{W}\mathbf{E} & 2\mathbf{T}^T\mathbf{W}\mathbf{T} & \mathbf{0} \\
        \mathbf{T}^T & \mathbf{0} & \mathbf{0} \end{array}\right)\left(\begin{array}{c} 
        s_l \\
        \alpha_l \\
        m_l 
        \end{array}\right) = \left(\begin{array}{c}
        2\mathbf{E}\mathbf{W}\mathbf{x} \\
        2\mathbf{T}^T\mathbf{W}\mathbf{x} \\
        \mathbf{0} 
        \end{array}\right)$\;
        $\mathcal{D}_{\hat{Q}_N}(g_{(n + 1)}) \gets \sum_{j=1}^N\|x_j - g_{(n + 1)}(\pi_{g_{(n + 1)}}(x_j))\|_{\mathbf{R}^D}^2$\;
        $\mathcal{E} \gets |\mathcal{D}_{\hat{Q}_N}(g_{(n + 1)}) - \mathcal{D}_{\hat{Q}_N}(g_{(n)})|/\mathcal{D}_{\hat{Q}_N}(g_{(n)})$\;
        $n \gets n + 1$\;
    }
    $\hat{g}_l \gets g_{(n)}$\;
}
$\kappa \gets \text{max}\{\|\pi_{\hat{g}_{l^*}}(x_j)\|_{\mathbf{R}^d}: j = 1, 2, \dots, I\}$, where $l^* = \arg \min_l\{\frac{1}{N}\sum_{j=1}^N\|x_j - g_{(l)}(\pi_{\hat{g}_{(l)}}(x_j)\|_{\mathbf{R}^D}^2: l = 1, 2, \dots, L\}$\;
$g^*(r) = g^{l^*}(\kappa r)$, where the analytic formula of $g^*$ is from $g_{(n + 1), l}(r) = \sum_{j=1}^N \eta_{4-d}(r - \pi_{g_{(n)}}(x_j)) + \sum_{k=1}^{d + 1} \alpha_{k, l} \times p_k(r)$, and $\gamma^* \gets \gamma_{l^*}$
\end{algorithm}

	\begin{equation}\label{PMSEF}
	\mathcal{K}_{\lambda,\mathbf{P}}(f,g)=\mathbf{E}\left\Vert X_t - f\left(\pi_g(X_t)\right)\right\Vert^2_{\mathbf{R}^D} + \lambda \sum_{t = 1}^T \left\Vert\nabla^{\otimes 2} f(t)\right\Vert_{L^2(\mathbf{R}^d)}^2 + \gamma \sum_{X} \left\Vert\nabla^{\otimes 2} f(X_t)\right\Vert_{L^2(\mathbf{R})}^2, 
	\end{equation}
	\begin{equation}\nonumber
	\mathcal{K}_{\lambda,\mathbf{P}}(f)=\mathcal{K}_{\lambda,\mathbf{P}}(f,f),
	\end{equation}
	where $\Vert\nabla^{\otimes 2}f \Vert_{L^2(\mathbf{R}^d)}^2 = \sum_{l=1}^D\int_{\mathbf{R}^d} \sum_{i,j=1}^d\vert\frac{\partial^2 f_l}{\partial t_i \partial t_j}(t)\vert^2dt$. A manifold $M_{f^*}^d$ determined by $f^*$ is called a principal manifold for $X$ (or $\mathbf{P}$) with the tuning parameter $\lambda$ if 
	\begin{align}\label{def: PM}
	f^*=\arg\min_{f\in\mathscr{F}(\mathbf{P})}\mathcal{K}_{\lambda,\mathbf{P}}(f),\ \ \mbox{ where }\mathscr{F}(\mathbf{P}):=\left\{f\in C_\infty\cap \nabla^{-\otimes 2}L^2( R^d \rightarrow R^D):\sup_{x\in supp(\mathbf{P})}\left\Vert\pi_f(x)\right\Vert_{R^d} = 1 \right\}.
	\end{align}

Questions:
\begin{enumerate}
  \item What is the best way to evaluate the performance of the estimates? We can use SSD for the embedded values, but how to assess the parameterized values?
  \item Should there be updates to what is defined as the "optimal" model in the PME algorithm? Currently it is possible for a model with non-minimal SSD to be chosen within the fitting of a single tuning value.
  \item Why would we see the SSD increase substantially in late optimization steps?
  \item Are there cases where the SSD and MSD differ substantially? Why would that be the case?
  \item Are there any techniques that help with large amounts of noise in the data? Data reduction should help with outliers, but likely not consistently high noise levels. Early indications that possibly allowing for higher tuning parameters would help?
  \item Why does PME algorithm struggle on Swiss Roll cross-section? It seems to work well up until spiral reaches second layer. The manifold should never intersect, but from the angle from the origin does overlap.
  \item Generally, what are the limitations on the estimated manifolds intersecting? How close can it come to intersecting without showing strange behavior?
  \item Is thin-plate spline computational strategy from Wahba the best approach, or could updated computational approach help (Wood, 2003)?
  \item How much error does the registration process introduce? Can the LPME algorithm successfully handle a certain number of bad registrations or extractions?
\end{enumerate}

\nocite{*}
\bibliographystyle{plain}
\bibliography{references}

\end{document}
